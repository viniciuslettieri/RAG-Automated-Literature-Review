{"title": "Fault detection and diagnosis of a two-column sour water treatment unit based on artificial inteligence algorithms", "authors": ["Nogueira, Júlia do Nascimento Pereira", "Souza Jr., Maurício Bezerra de orient.", "Melo Junior, Príamo Albuquerque coorient.", "Universidade Federal do Rio de Janeiro. Escola de Química"], "abstract": "Detecção e Diagnostico de Falhas (FDD) é uma area de Engenharia de Sistemas de Processos (PSE) de grande importância, principalmente com o aumento da automação de processos. É um dos campos da engenharia química considerados promissores para aplicação em Inteligência Artificial (AI), juntamente com design de materiais e operação de processos. Algumas técnicas de AI que podem ser implementadas são florestas aleatórias (RF), máquinas de vetores de suporte (SVM), k vizinhos mais próximos (KNN), redes neurais profundas (DNN) e mapas auto-organizáveis (SOM). Os sistemas de FDD podem ser úteis para supervisionar o comportamento das Unidades de Tratamento de Águas Ácidas (SWTU), pois são processos químicos que apresentam dificuldades operacionais quando ocorrem distúrbios. SWTU removem contaminantes de correntes de águas ácidas (SW) geradas através do processamento de petróleo, consistindo principalmente de pequenas quantidades de H2S e NH3. Elas são consideradas um dos principais resíduos aquosos de refinarias e não podem ser descartadas devido a regulamentações ambientais. No entanto, não existem estudos anteriores focados no desenvolvimento de sistemas FDD para SWTU e trabalhos sobre sua dinâmica são escassos.Assim, o presente trabalho se propõe a estudar o comportamento dinâmico simulado de uma SWTU e desenvolver um sistema de FDD aplicando técnicas de AI. A simulação foi realizada no Aspen Plus Dynamics® e testou a operação normal e seis falhas relevantes. O tratamento de dados, incluindo adição de ruído e FDD foram feitos usando Python. O FDD e realizado por meio de classificação de dados, e os resultados são avaliados principalmente por meio de matrizes de confusão e acurácia. Mesmo após a redução da dimensionalidade pela remoção das variáveis menos importantes, o FDD foi satisfatório com mais de 87,50% de acurácia em todas as técnicas de AI. RF e SVM com kernels lineares e gaussianos apresentaram os melhores resultados e tiveram os menores tempos computacionais. O nível do fundo da segunda coluna provou ser a variável mais relevante no estudo de FDD apresentado aqui.", "bibliography_pages": [152, 160], "keywords": ["Detecção e diagnóstico de falhas", "Inteligência artificial"], "urls": ["http://objdig.ufrj.br/61/dissert/918885.pdf", "http://lattes.cnpq.br/0923689760838522"], "pdf_url": "http://objdig.ufrj.br/61/dissert/918885.pdf", "id": "918885", "sentences": ["Lastly, I'd like to appreciate all the professors, engineers and colleagues that helped me to build the steps of this work, solving many doubts and passing on knowledge: Argimiro, Alexandre, Herbert, Paiva, Afr^ anio and many others. Also, the \fnancial support of the Brazilian foundation National Council for Sci- enti\fc and Technological Development (CNPq) is gratefully acknowledged. v \\I am among those who think that science has great beauty. A scientist in his laboratory is not only a technician: he is also a child placed before natural phenomena which impress him like a fairy tale.\" - Marie Curie vi Abstract of Master's thesis presented to EPQB/UFRJ as a partial ful\fllment of the requirements for the degree of Master of Science (M.Sc.) FAULT DETECTION AND DIAGNOSIS OF A TWO-COLUMN SOUR WATER TREATMENT UNIT BASED ON ARTIFICIAL INTELLIGENCE ALGORITHMS J\u0013 ulia do Nascimento Pereira Nogueira August/2021 Advisors: Maur\u0013 \u0010cio B. de Souza Jr.", "Pr\u0013 \u0010amo Albuquerque Melo Junior Graduate Program: Engineering of Chemical and Biochemical Processes Fault Detection and Diagnosis (FDD) is a Process Systems Engineering (PSE) area of great importance, especially with increased process automation. It is one of the chemical engineering \felds considered promising to Arti\fcial Intelligence (AI) application, along with material design and process operation. AI techniques that may be implemented are Random Forests (RF), Support Vector Machines (SVM), K-Nearest Neighbors (KNN), Deep Neural Networks (DNN) and Self-Organizing Maps (SOM). FDD systems can be useful to supervise Sour Water Treatment Units (SWTU) behavior, as they are chemical processes that present operational di\u000e- culties when disturbances occur. SWTU remove contaminants from sour water (SW) streams generated through petroleum processing, consisting mainly of small amounts of H2SandNH 3. They are considered one of the primary aqueous wastes of re\fneries and cannot be disposed of due to environmental regulations. However, no previous studies focused on the development of FDD systems for SWTU exist and works on its dynamics are scarce. Hence, the present work proposes to study the dynamic simulated behavior of an SWTU and develop a FDD system apply- ing AI techniques. The simulation was performed in Aspen Plus Dynamics®and ran to test normal operation and six relevant faults. Data treatment, including noise addition, and FDD were made using Python. FDD is performed through data classi\fcation, and results are evaluated mainly by means of accuracy and confusion matrices. Even after dimensionality reduction by less important variables removal, FDD was satisfactory with over 87.50% accuracy in all AI techniques. RF and SVM vii with linear and Gaussian kernels presented the best results and had the shortest computing times. The second column's sump level proved to be the most relevant variable in the FDD study presented here.", "viii Resumo da Disserta\u0018 c~ ao apresentada ao EPQB/UFRJ como parte dos requisitos necess\u0013 arios para a obten\u0018 c~ ao do grau de Mestre em Ci^ encias (M.Sc.) J\u0013 ulia do Nascimento Pereira Nogueira Agosto/2021 Advisors: Maur\u0013 \u0010cio B. de Souza Jr.", "Pr\u0013 \u0010amo Albuquerque Melo Junior Programa: Engenharia de Processos Qu\u0013 \u0010micos e Bioqu\u0013 \u0010micos Detec\u0018 c~ ao e Diagn\u0013 ostico de Falhas (FDD) \u0013 e uma \u0013 area de Engenharia de Sis- temas de Processos (PSE) de grande import^ ancia, principalmente com o aumento da automa\u0018 c~ ao de processos. \u0013E um dos campos da engenharia qu\u0013 \u0010mica consider- ados promissores para aplica\u0018 c~ ao em Intelig^ encia Arti\fcial (AI), juntamente com design de materiais e opera\u0018 c~ ao de processos. Algumas t\u0013 ecnicas de AI que podem ser implementadas s~ ao orestas aleat\u0013 orias (RF), m\u0013 aquinas de vetores de suporte (SVM), k vizinhos mais pr\u0013 oximos (KNN), redes neurais profundas (DNN) e mapas auto-organiz\u0013 aveis (SOM). Os sistemas de FDD podem ser \u0013 uteis para supervisionar o comportamento das Unidades de Tratamento de \u0013Aguas \u0013Acidas (SWTU), pois s~ ao processos qu\u0013 \u0010micos que apresentam di\fculdades operacionais quando ocorrem dist\u0013 urbios. SWTU removem contaminantes de correntes de \u0013 aguas \u0013 acidas (SW) ger- adas atrav\u0013 es do processamento de petr\u0013 oleo, consistindo principalmente de pequenas quantidades de H2SeNH 3. Elas s~ ao consideradas um dos principais res\u0013 \u0010duos aqu- osos de re\fnarias e n~ ao podem ser descartadas devido a regulamenta\u0018 c~ oes ambientais. No entanto, n~ ao existem estudos anteriores focados no desenvolvimento de sistemas FDD para SWTU e trabalhos sobre sua din^ amica s~ ao escassos. Assim, o presente trabalho se prop~ oe a estudar o comportamento din^ amico simulado de uma SWTU e desenvolver um sistema de FDD aplicando t\u0013 ecnicas de AI. A simula\u0018 c~ ao foi realizada noAspen Plus Dynamics®e testou a opera\u0018 c~ ao normal e seis falhas relevantes. O tratamento de dados, incluindo adi\u0018 c~ ao de ru\u0013 \u0010do e FDD foram feitos usando Python . O FDD \u0013 e realizado por meio de classi\fca\u0018 c~ ao de dados, e os resultados s~ ao avaliados principalmente por meio de matrizes de confus~ ao e acur\u0013 acia. Mesmo ap\u0013 os a redu\u0018 c~ ao da dimensionalidade pela remo\u0018 c~ ao das vari\u0013 aveis menos importantes, o FDD foi sat- isfat\u0013 orio com mais de 87,50% de acur\u0013 acia em todas as t\u0013 ecnicas de AI. RF e SVM com kernels lineares e gaussianos apresentaram os melhores resultados e tiveram os ix menores tempos computacionais. O n\u0013 \u0010vel do fundo da segunda coluna provou ser a vari\u0013 avel mais relevante no estudo de FDD apresentado aqui. x Contents List of Figures xv List of Tables xix List of Latin Variables and Symbols xx List of Greek Variables and Symbols xxii List of Machine Learning Hyperparameters xxiii List of Abbreviations xxv 2.4 Previous Works in Fault Detection and Diagnosis in Chemical Pro- xi 4.2.3 Previous Works in Sour Water Stripping Modelling and Sim- xii xiii Bibliography 152 Appendix A Static and Dynamic Simulations Schemes 161 Appendix B Developed Databases 164 xiv List of Figures", "4.20 Data processing", "xv 8", "2 Di\u000berent possible hyperplanes for linearly separable data with two 8", "7 Hyperparameters Candgamma e\u000bects in the decision boundary def- 8", "8 Parallel coordinate plot of linear kernel SVM hyperparameter opti- xvi 8", "9 Optimization history plot of linear kernel SVM hyperparameter op- 8", "11 Parallel coordinate plot of polynomial kernel SVM hyperparameter 8", "12 Optimization history plot of polynomial kernel SVM hyperparameter 8", "14 Parallel coordinate plot of Gaussian kernel SVM hyperparameter op- 8", "15 Optimization history plot of Gaussian kernel SVM hyperparameter 8", "17 Parallel coordinate plot of sigmoid kernel SVM hyperparameter opti- 8", "18 Optimization history plot of sigmoid kernel SVM hyperparameter op- 9", "6 Learning rate types: constant and adaptive, with and without mo- xvii xviii List of Tables xix List of Latin Variables and Symbols Qm(yi;c) Impurity of the Results in a Decision Tree, p", " 72 Ri Regions Generated in a Decision Tree, p", " 71 b jjwjjDistance Between an Hyperplane and the Origin in SVM, p", " ^cm Most Frequent Category in a Region of a Decision Tree, p", " 71 ^pmk Proportion of a Class in a Region in a Decision Tree, p", " 72 dij Distance Between Samples in KNN, p", " 85 A Right Singular Vectors in a Singular Value Decomposition, p", " I( ) Indicator Function, p", " 71 L Singular Values in a Singular Value Decomposition, p", " 61 M Number of Regions Generated in a Decision Tree, p", " 71 N(\u0016,\u001b) Normal Distribution of Mean \u0016and Standard Deviation \u001b, p", " Noise Vector of Noise Added to a Process Variable, p", " 55 S Covariance Matrix of the Dataset, p", " 60 Sigma Noise Amplitude, p", " 55 U Left Singular Vectors in a Singular Value Decomposition, p", " 61 V Neuron Output After Activation Function Application, p", " 117 X Normalized Dataset Matrix, p", " 60 xx Z Dataset Matrix, p", " 60 [H2S ]H2Sconcentration in a stream, p", " 27 [NH 3 ]NH 3concentration in a stream, p", " 27 a Vector of Constants, p", " 60 bi Bias in a Neural Network, p", " 116 r Parameter of The Minkowski Distance Equation, p", " 85 s Threshold of the Splitting Condition in a Decision Tree, p", " 72 sgn( ) Signal Function, p", " 94 tr( ) Trace of a Matrix, p", " 62 v Neuron Output Before Activation Function Application, p", " 117 var( ) Variance of an Expression, p", " 60 w Normal Vector to a Hyperplane in SVM, p", " 94 we Weights in a Neural Network, p", " 116 xxi List of Greek Variables and Symbols \b Transformation from the Original Space to the Feature Space In SVM, p", " 96 \u0015 Eigenvalue, p", " 60 \u0019 Proportion of Total Variance, p", " 62 \u0018i Slack Variables in Soft-Margin SVM, p", " 96 ti Integral Time, p", " 43 tdom Dominant Time Constant, p", " 45 xxii List of Machine Learning Hyperparameters clin Regularization Parameter for Linear Kernel, p", " 101 cpoly Regularization Parameter for Polynomial Kernel, p", " 101 crbf Regularization Parameter for Gaussian Kernel, p", " 101 csig Regularization Parameter for Sigmoid Kernel, p", " 101 cnnactivation Activation Functions of CNN Layers, p", " 130 cnnbatch size Batch Size of CNN, p", " 130 cnn\flters Number of Filters in Convolution Ouputs, p", " 130 cnnhidden size Number of Neurons in CNN Hidden Layers, p", " 130 cnnkernel size Length of the 1D Convolution Window, p", " 130 cnnoptimizer Optimizer Function in CNN, p", " 130 cnnpool size Size of the Pooling Window, p", " 130 coef Independent Term of the Kernel Functions, p", " 98 degree poly Degree of Polynomial Kernel Function, p", " 101 gamma poly Kernel Coe\u000ecient for Polynomial Kernel, p", " 101 gamma rbf Kernel Coe\u000ecient for Gaussian Kernel, p", " 101 gamma sig Kernel Coe\u000ecient for Sigmoid Kernel, p", " 101 knnalgorithm Applied Algorithm in KNN, p", " 86 knnnneighbors Number of Neighbors in KNN, p", " 86 mlpactivation Activation Functions of MLP Layers, p", " 128 xxiii mlpbatch size Batch Size of MLP, p", " 128 mlphidden size Number of Neurons in MLP Hidden Layers, p", " 128 mlpoptimizer Optimizer Function in MLP Training, p", " 128 rfmax depth Maximum Depth of the Trees in RF, p", " 74 rfminsamples leaf Minimum Number of Samples Required to Split a Node in RF, p", " 74 rfnestimators Number of Estimators in RF, p", " 74 som activation distance Map Activation Distance in SOM, p", " 133 som learning rate Learning Rate in SOM, p", " 133 som neighborhood function Neighborhood Function in SOM, p", " 133 som sigma Spread of Neighborhood Function in SOM, p", " 133 som size Size of the SOM, p", " 133 som topology Topology of SOM, p", " 133 xxiv List of Abbreviations ACG Acid Gas, p", " 23 ACGi Acid Gas Streams, p", " 32 AI Arti\fcial Intelligence, p", " 1 AMG Ammoniacal Gas, p", " 25 AMGi Ammoniacal Gas Streams, p", " 32 ARU Amine Regeneration Units, p", " 26 AUC Area Under the Curve, p", " 20 C1 Column 1, H2SStripping, p", " 31 C2DL C2's Drum Level Set Point, p", " 55 C2SL C2's Sump Level Set Point, p", " 48 C2 Column 2, NH 3Stripping, p", " 32 CART Classi\fcation and Regression Trees, p", " 71 CM Confusion Matrix, p", " 19 CNN Convolutional Neural Networks, p", " 12 CWWT Chevron Waste Water Treatment, p", " 25 CW Clean Water, p", " 26 CWi Clean Water Streams, p", " 32 DL Deep Learning, p", " 10 DNN Deep Neural Network, p", " 11 DPCA Dynamic Principal Component Analysis, p", " 57 xxv FDDC Fault Detection, Diagnosis and Correction, p", " 7 FDD Fault Detection and Diagnosis, p", " 1 FN False Negatives, p", " 19 FP False Positives, p", " 19 GPA Gas Processors Association, p", " 29 GPSWAT GPA Sour Water Equilibrium, p", " 29 IQR Interquatile Range, p", " 20 KNN K-Nearest Neighbors, p", " 1 LGP Lique\fed Petroleum Gas, p", " 26 MDA Mean Decrease in Accuracy, p", " 74 MDG Mean Decrease in Gini, p", " 74 MLP Multi-Layer Perceptrons, p", " 12 ML Machine Learning, p", " 9 NN Neural Networks, p", " 1 NW New Water, p", " 26 NWi New Water Streams, p", " 30 OOB Out-of-bag, p", " 72 OVA One-Versus-All Classi\fcation, p", " 100 OVO One-Versus-One Classi\fcation, p", " 100 PCA Principal Component Analysis, p", " 3 PC Principal Component, p", " 59 PSE Process Systems Engineering, p", " 1 PV Process Variable, p", " 5 PW Pretreated Water, p", " 25 PWi Pretreated Water Streams, p", " 32 xxvi Q1 Lower Quartile, p", " 20 Q3 Upper Quartile, p", " 20 RF Random Forest, p", " 1 ROC Receiver Operating Characteristic, p", " 20 ReLu Recti\fed Linear Unit, p", " 118 SLLN Strong Law of Large Numbers, p", " 74 SNR Signal-to-Noise Ratio, p", " 55 SOM Self-Organizing Maps, p", " 3 SP Set Point, p", " 5 SRU Sulfur Recovery Units, p", " 24 SVD Singular Value Decomposition, p", " 61 SVM Support Vector Machines, p", " 1 SW1F SW1 Mass Flow, p", " 30 SWTU Sour Water Treatment Unit, p", " 2 SW Sour Water, p", " 2 SWi Sour Water Streams, p", " 30 SeLu Scaled Exponential Linear Unit, p", " 118 TE Tennessee Eastman, p", " 9 TN True Negatives, p", " 19 TP True Positives, p", " 19 TW Treated Water, p", " 23 TWi Treated Water Streams, p", " 32 VI Variable Importance, p", " 74 WWT Waste Water Treatment, p", " 26 xxvii Chapter 1 Introduction", "1.1 Motivation", "Fault detection and diagnosis (FDD) is very relevant for process systems engi- neering (PSE), especially in the chemical industry. It helps to prevent accidents, minimizing health risks and economic loss. Avoiding faults is also important to keep the products, byproducts and e\u000fuents speci\fcations within the required range by the clients or environmental regulations (ISERMANN, 2006; SHU et al. , 2016; VENKATASUBRAMANIAN et al. , 2003).", "Data-driven methods are black-box approaches to the problem so that previous knowledge about balances, equations and kinetics is not necessary. They are based on real historical process or simulated data; both approaches present advantages and disadvantages (PARK et al. , 2020; ZHAO et al. , 2019). One of them is the presence of nonlinearities that might complicate the analysis. Real industrial data are often con\fdential, besides poorly recorded, described and organized. However, simulated data are also mismatched with the real process to some degree (LI et al. , The determination of which faults might happen and are relevant to the process' safety and e\u000eciency can be challenging. How many and which process variables are necessary to perform the analysis is also important, once an excess of features can lead to less e\u000ecient and more time consuming systems (FAN et al. , 2020; SHU In data-driven methods, a possible approach is applying arti\fcial intelligence (AI) algorithms, considered one promising solution to the FDD problem. Many AI algorithms apply binary or multiclass classi\fcation-based methods to solve fault de- tection and fault diagnosis, respectively. Some of these methods are random forests (RF), support vector machines (SVM), k-nearest neighbors (KNN) and various neu- ral networks (NN) types, all of them applied to some extent in FDD in literature (PARK et al. , 2020; VENKATASUBRAMANIAN, 2019; ZHAO et al. , 2019). AI techniques, such as RF, can also perform variable importance analysis, deter- mining which variables are more relevant to the problem. A dimensionality reduction can be bene\fcial to the FDD algorithms, reducing the computing time (FAN et al. , In the present work, FDD was applied to a sour water treatment unit (SWTU) with two columns. Sour water (SW) is an e\u000fuent generated during petroleum pro- cessing. The contact between water and hydrocarbons streams containing sulfur and nitrogen compounds makes them move from the organic to the aqueous phase asH2SandNH 3. SW is one of the major aqueous e\u000fuents in petroleum re\fner- ies and cannot be discarded without treatment due to their environmental impact (LEAVITT and ET AL, 2004; QUINLAN, 2003).", "SWTU intend to eliminate the contaminants in SW using a stripping column. Most of the NH 3absorbed by SW is discharged at the top of the column, along with H2S. Sulfur recovery units, which treat the discharged stream, present limitations regarding the concentration of NH 3in the gas. E\u000eciency loss, plugging and NOx formation are some of the consequences of NH 3high quantities in the stream, the last one being highly environmentally undesired (ADDINGTON et al. , 2013; QUINLAN and HATI, 2010).", "Therefore, when the amount of NH 3present in SW is high, plants are commonly built with two columns: the \frst discharges H2S-rich gas, and the second discharges NH 3-rich gas, which are treated separately. As Brazilian crude oils are especially high-nitrogen, which leads to higher absorption of NH 3in SW, the two-columns SWTU are frequently adopted in Brazilian re\fneries (BELATO et al. , 2002; QUIN- LAN and HATI, 2010).", "TheH2S-rich gas, also known as acid gas, should eliminate at least 90% of the H2Spresent in SW, according to Brazilian environmental laws. This is important once mostH2Sresidues that goes into the second column will be eliminated in the NH 3-gas, also known as ammoniacal gas. This stream's treatment generates SOx ifH2Sis discharged in it, which is also environmentally undesired (BRAZIL, 2007; QUINLAN and HATI, 2010).", "This chemical process may present some operational complexities in the presence of disturbances, such as the heat duty control in the \frst column. Both contami- nants' concentration increase in acid gas with an increment in heat duty. However, the objective is to have the highest possible H2Sconcentration with the lowest possi- bleNH 3concentration, which are con icting intentions. Besides, the \frst column's heat duty operational range is considered narrow, once very smooth changes can lead to sudden variations in the stream composition (HATCHER et al. , 2014). Therefore, an adequate process control and FDD system can be bene\fcial to SWTU operation, especially from an environmental point of view. As in any other chemical process, the safety and economic bene\fts also apply. Although various works studying SWTU have already been published, few of them are focused on two-column con\fgurations, and most intend to make design analysis, as well as cost and components' recuperations optimizations. Dynamic behavior analyses are scarce in the literature, and there are no works focused on fault studies in these units.", "Unlike all the publications discussed throughout Section 4.2.3, the present work focuses on creating static and dynamic simulations of a two-column SWTU, aiming the generation of a database representative of the process' behavior in multiple normal and abnormal situations. The FDD of the unit is performed using this database and with the application of a representative group of AI algorithms in a multiclass classi\fcation-based approach.", "1.2 Objectives", "The present work's main objective is to develop a FDD operational support system in a SWTU's dynamic simulated behavior, applying AI techniques. Speci\fc objectives necessary to achieve the proposed goal are listed as follows: Create a static simulation that represents the chemical process; Create a dynamic simulation, along with controllers' appropriate inclusion and tuning, representing the chemical process both under normal and abnormal operation to monitor the variables and understand their behavior; Create and treat a database based on the dynamic simulation composed of normal, or fault-free, operation and relevant faults in the process, equipment and sensors; Analyze the faulty scenarios and the process behavior when those faults occur; Apply and compare multiple AI techniques, such as RF, SVM, KNN and various NN types, to the database to detect and diagnose faults; Apply speci\fc AI techniques, such as RF and self-organizing maps (SOM), along with principal component analysis (PCA) performance, to determine the most relevant variables to perform FDD, aiming to reduce the database's dimensionality and, consequently, the computing time.", "1.3 Literature Contributions", "The following works were produced during the elaboration of this project: \\Fault Detection and Diagnosis in a Sour Water Treatment Unit with Random Forest\" - Submitted in Portuguese and accepted by the Chemical Engineering Brazilian Conference (COBEQ), to be presented in October/2021. \\Investigation of Faulty Dynamic Scenarios and Their Diagnosis in a Two- Column Sour Water Treatment Unit Using Arti\fcial Intelligence Algorithms\" - Submitted to Computers and Chemical Engineering, awaiting response from the journal.", "1.4 Organization", "This work is organized in a way to represent its natural ow. This Chapter introduces the motivation and the objectives to be ful\flled to complete it, closing with this section. Chapter 2 comprises a literature review about FDD in chemical processes, its importance and its applications with AI. General concepts of AI, such as taxonomy and types of learning are also discussed. The methodology description is presented in Chapter 3, including an introduction explaining the work's general steps, hardware and software description, and de\fnitions of the metrics used to evaluate the results.", "The case study in which FDD were performed is described in detail in Chapter 4. An introduction about SW and its treatment and a literature review on the topic open the Chapter, followed by the description of the methodology applied and results obtained in the multiple steps of the database generation: static simulation, dynamic simulation, fault simulation, and data processing. Chapter 5 presents a PCA of the database to evaluate its structure and more important variables before FDD implementation. In this Chapter, a dynamic analysis is also performed to determine the adequate delay to add to the database.", "Chapters 6 to 9 present parallel reports of independent AI technique applications to the same database generated in Chapter 4 and analyzed in Chapter 5. Each of those four Chapters contains an introduction to the topic, a literature review on the technique applied, the methodology on how it was made, and presentation, discussion, and comparison of results obtained. Those Chapters cover RF, KNN, SVM, and NN, respectively.", "A general discussion, with results comparisons is presented in Chapter 10. Con- clusions and future work suggestions are described in Chapter 11, which closes this work, followed by bibliography and the appendices.", "Chapter 2 Literature Review", "2.1 Introduction", "FDD systems are very relevant for the chemical industry, especially to maintain safety, environmental and competitive standards and reduce economic losses. A possible methodology is applying AI algorithms, considered a potential approach for practical applications.", "This Chapter comprises a literature review on FDD, followed by AI basic con- cepts. Relevant works published about applying AI to perform FDD in chemical processes are in the \fnal section of this Chapter.", "2.2 Fault Detection and Diagnosis", "2.2.1 Faults and Process Control", "As technology and engineering grow more complex and develop each time faster, chemical processes become more automated and integrated. Besides, environmental regulations on products' and e\u000fuents' speci\fcations are stricter in many countries, as well as safety standards, making the industrial plants more di\u000ecult to operate. Therefore, process control becomes essential to ensure that processes' e\u000eciency, safety and speci\fcations are ful\flled (ISERMANN, 2006; SEBORG et al. , 2011). To create an adequate control system, the variables must be correctly selected, and this depends on knowledge about the process of interest. As described in SE- BORG et al. (2011), the controlled variables are the ones that have a desired speci\fed value. Typically, they are stream's properties, such as mass ow, temperature, pres- sure or composition. This speci\fed value is de\fned as set point (SP) of the process variable (PV).", "The control is performed by adjusting a manipulated variable, commonly ow rates, responsible for maintaining the controlled variable the closest to its SP as possible. One of the most usual control strategies is feedback control, where the controlled variable value is used to adjust the manipulated variable, correcting the controlled variable and leading it closer to its set point (SEBORG et al. , 2011). The process can be disturbed, a\u000becting the controlled variables, typically due to changes in the feed streams, such as ow rate, temperature and composition.", "The feedback action does not rely on the disturbance source but depends on the deviation existence to happen (SEBORG et al. , 2011). If greater than what process control is able to handle, these disturbances can lead to a fault in operation. Each PV has an operational range, which assures the safety and e\u000eciency of the process. When a PV leaves this range and the controllable region, a fault occurs. Therefore, faults can be de\fned as abnormal events or symptoms in an industrial process (VENKATASUBRAMANIAN et al. , 2003).", "Faults can have di\u000berent sources, such as process disturbances, commented pre- viously, and structural failures, related to equipment malfunction. Besides, they might be caused by failures not only directly related to the process, but also in the actuators, sensors and controllers, responsible to keep all the PV in the controllable region (VENKATASUBRAMANIAN et al. , 2003). Figure 2.1 displays a simpli\fed scheme of the faults common origins.", "Figure 2.1: Simpli\fed scheme of faults' origins (based upon VENKATASUBRAMA- Studying and understanding abnormal events causes and solutions is important from a safety point of view and an economic perspective. Major accidents and tragedies are uncommon in chemical processes, but various minor occurrences lead to injuries in plants' operators, generating costs for the companies and the society. In VENKATASUBRAMANIAN et al. (2003), an estimative is provided that about 20 billion dollars are lost every year due to abnormal events incorrect management, only considering the US petrochemical industry.", "In SHU et al. (2016), two explosions in 2005 are described as examples of faults consequences, both with fatal victims. The British Petroleum Texas City re\fnery accident led to a loss of U $1.8 billion due to a series of operation mistakes and sensor failures. First, the high level alarm of a tower was ignored, then the level indicator showed a very low level, when in fact, the tower was \flled up. The compounds started to ow into the blowdown drum, and its high-level alarm did not go o\u000b. A vapor cloud formed outside the process was ignited by a car, provoking a huge explosion. The other accident happened in the Jilin chemical plant, and caused the pollution of the Songhua River with about a hundred tons of benzene and nitrobenzene. The absence of control interlocks and a series of operational mistakes created an explosive mixture, as a nitrobenzene stream was wrongly evaporated. High-temperature alarms went o\u000b for long times but were ignored by the operators, as they were mistaken by ood alarms that are considered normal in startup events.", "Hence, considering all the environmental, health, economic and technological factors commented in this section, the development of FDD systems is essential to the chemical processes. They will be further discussed in the next section.", "2.2.2 Detection, Diagnosis and Correction", "FDD systems have been being studied for over 50 years, but the interest in its application in chemical processes started in HIMMELBLAU (1978), a publication made almost 45 years ago focused on applying FDD using models. Despite the in- crease in process control automation and the long time since FDD was \frst proposed in chemical processes, most real plants are highly dependent on human interventions A more complete approach to the FDD problem is to include the correction step, de\fning the fault detection, diagnosis and correction (FDDC) system. The basic steps to perform FDDC are to understand which behaviors are normal and abnormal, detecting the faults and diagnosing their causes. The diagnosis is performed by isolating and analyzing the di\u000berent faults (SARTORI et al. , 2012). Afterwards, the necessary interventions and corrections are determined and the process is returned to a safe and desired operational behavior (SARTORI et al. , 2012). Figure 2.2 illustrates these steps.", "FDD or FDDC systems have typically three possible approaches: model-based, data-driven and knowledge-based. The \frst depends on information about the pro- cess model, its variables, balances and equations; meanwhile, data-driven methods rely on historical data, its quality and treatment. In knowledge-based approaches, the process data are combined with process behavior knowledge and rules (PARK Figure 2.2: Simpli\fed scheme FDDC steps (based upon SARTORI et al. (2012)). Even though many years of study have been dedicated to FDD systems with multiple approaches, and considering that annual publications on the topic are more than ten times greater than 25 years ago, there are few successful cases of industrial application (SHU et al. , 2016). Most of the publications use simulated process datasets, as real industrial data are often con\fdential, especially regarding to faults. Besides, the data are hard to collect once they are not always stored, or even described by the operators that detected the faults in real-time situations. In cases where the data are stored and organized, faults are uncommon, so datasets become unevenly distributed. With few fault examples, that might not even cover all the possible process' faults, learning enough about the faults to detect and diagnose them trustworthily becomes complicated (SHU et al. , 2016).", "Another challenge to be considered is real data's properties, such as the high number of features, nonlinearity and non-Gaussian distribution. The speci\fc dy- namics of each chemical process can lead to time-varying and multimode behaviors, making the analysis more complex (SHU et al. , 2016).", "The reliability of a FDD system depends on a series of questions that are often asked, but not always answered. Regarding the faults, it might be hard to know which faults should be diagnosed, or even if all of them have already been noticed. Also, if the system would be capable to diagnose a new fault without previous observations (SHU et al. , 2016).", "Concerning the PV, it is necessary to determine which ones are measured and if all of them are really necessary to perform the FDD. An excess of features can reduce the system's e\u000eciency and increase its time consumption (SHU et al. , 2016). One of the alternatives to avoid the di\u000eculties discussed above about real process data is to work with simulated process data. The closest the digital plant is to the original one, the more adherent the data will be. However, the creation and running of the simulations can be a complex and time-consuming task, and there is generally a mismatch between the real and simulated process data (LI et al. , 2020). Considering the problems of real data and the di\u000eculties in creating simulated ones, the majority of the works applies the Tennessee Eastman (TE) dataset to validate them. TE is a chemical process with a traditional dataset of process control and FDD, developed in DOWNS and VOGEL (1993). Once its PV and faults are already profoundly studied, many works have applied TE, or variations of it, or new simulated datasets based on the original process and faults descriptions (SHU et al. ,", "2.2.3 Arti\fcial Intelligence Application", "It is considered that AI is very promising in three chemical engineering \felds: material design, process operations and fault diagnosis. Some real-world applica- tions are already implemented, such as using machine learning (ML) techniques to monitor oil-well performance in British Petroleum. The objective is to maximize production while minimizing the downtime. The Uptake Company applies ML to predict failures in many equipment, successfully scheduling preventive maintenance (VENKATASUBRAMANIAN, 2019).", "AI application in FDD systems is considered to be knowledge-based or data- driven instead of the classic model-based approach. In an exclusively data-driven approach, there is no need to understand the phenomena, its equations, or the chem- ical reactions involved. This is an advantage over model-based approaches, making the analysis viable and its implementation easier, especially regarding complex pro- cess or equipment, or with little known kinetics (PARK et al. , 2020; ZHAO et al. , Data-driven methods can be applied with a classi\fcation approach, once numer- ical features are the inputs and categorical answers are the outputs. The inputs are commonly the PV, and the outputs are fault-free and the fault types observed in a speci\fc chemical process. Even when only the fault detection is being performed, it leads to binary classi\fcation, a true or false type of problem (ZHAO et al. , 2019). The present work is focused on FDD of a multiclass problem, detecting if and which fault happened. Figure 2.3 displays where this approach is located in the taxonomy of FDD methods.", "The next section of this Chapter comprises a brief literature review on AI and ML, with some basic de\fnitions. They are useful to understand the AI FDD sys- tems examples that will be presented in the \fnal section of this Chapter and the application of AI in the present work.", "Figure 2.3: Multiclass classi\fcation-based FDD taxonomy (based upon ZHAO et al.", "2.3 Arti\fcial Intelligence", "2.3.1 Basic De\fnitions", "AI is a \feld that mixes computer science, data mining, statistics, cybernetics and neuroscience. Those concepts began to converge over 70 years ago in an attempt to automate tasks originally performed by humans. Di\u000berent approaches are still being discussed and developed nowadays, with new published applications (CHOLLET, Although AI and ML are often used interchangeably, they are not synonyms, once ML is a subgroup of techniques considered as AI. Brain-inspired techniques, such as NN, are a subgroup of ML. Deep Learning (DL) are NN with deep architectures capable of performing more complex tasks, such as recognition (ALOM et al. , 2019). Figure 2.4 illustrates the AI taxonomy.", "AI algorithms mimic human abilities such as thinking, reasoning and learning. Some examples of AI applications are robotics, expert systems, fuzzy logic and ML. Techniques capable of learning through information available in datasets compose the ML subgroup, which experience a training step before application. In those cases, the task is trained and learned instead of explicitly programmed to be done. Some examples are RF, KNN, SVM and NN, all applied in the present work (CHOL- Figure 2.4: AI taxonomy (based upon ALOM et al. (2019); CHOLLET (2017)).", "2.3.2 Types of Learning", "ML can be performed in three di\u000berent ways: supervised, unsupervised and reinforcement learning. Supervised learning is task-driven, using datasets with the expected outcome, such as labels, as part of the training. Meanwhile, unsupervised learning is data-driven, which are unlabeled. Reinforcement learning works like a reward system so that the algorithms can learn from their mistakes (ALOM et al. , In general, supervised learning is more commonly applied (CHOLLET, 2017). This is also the case of the present work, where SOM was the only algorithm pre- senting unsupervised learning. The di\u000berence between those two types can be seen in Figure 2.5.", "In this example, supervised learning occurs using the labelled data, represented by the pink triangles and green circles, to learn how to distinguish them. The dashed line represents the information learned. Therefore, the data in the lower left side of the \fgure will be labeled as class A, and the data on the other side will be labeled as class B.", "However, unsupervised learning is performed by trying to group the samples, that is, creating clusters. The dashed circles represent the information learned, where two di\u000berent classes were detected. They can be referred to as classes A and B afterwards.", "Besides, the algorithms can perform deep or shallow learning. DL is normally relative to deep neural networks (DNN), with multiple layers between the input and output information (CHOLLET, 2017). In the present work, multi-layer perceptrons Figure 2.5: Supervised and unsupervised learning.", "(MLP) and convolutional neural networks (CNN) were applied.", "Shallow learning is typically corresponding to shallow NN, and also to most of the other ML techniques that are not brain-inspired (CHOLLET, 2017). In the present work, they are represented by SOM, which is a shallow NN, and also RF, KNN and SVM.", "Therefore, all the algorithms studied in this work are considered ML and, con- sequently, are AI techniques. Some of them have shallow learning, and others, deep learning. Most of them are supervised; meanwhile, SOM are unsupervised learners. The concepts and mathematical approaches to those algorithms will be discussed in their respective Chapters.", "As commented previously, learning is performed through a training step so that the algorithm can understand which task is expected of it. To accomplish this, a training dataset is necessary and is fed to a new and generic model. In a supervised classi\fcation example, the training dataset is composed of the input data, that is, the features, and also of the expected labels, or targets. During training, the model is adjusted to predict the correct labels of the training data. The objective is to minimize the error, in this case, to reduce the misclassi\fcations. After that, the model is ready to perform the desired task (CHOLLET, 2017).", "A testing dataset is then used to evaluate the model's quality, simulating a real application of it. In this case, the expected labels are unknown to the algorithm. The training and testing dataset must not have data in common, once the test should be performed with new data to the trained model (CHOLLET, 2017). A third dataset can be used, known as validation dataset. It is applied in some training methodologies before performing the test, to avoid over\ftting. When a model is so well-trained that it loses its ability to generalize the learned informa- tion, it is considered over\ft. This is undesirable, once the trained model will not perform the learned task correctly in unknown and new data, as it is too data-speci\fc The validation dataset is used to prevent the model from becoming too dependent on the training dataset, ensuring that the model is trained properly. The opposite scenario can also occur, as a poorly trained model is considered under\ft, and is not trained enough to perform the task correctly (CHOLLET, 2017). Figure 2.6 illustrates both situations, along with the expected \ftting. Figure 2.6: Expected \ftting, under\ftting and over\ftting.", "In the expected \ftting, it is possible to infer that the trained model, represented by the dashed line, is adequate to perform the classi\fcation. Even if it does not \ft all the samples with perfection, the trained model represents the behavior correctly. In under\ftting, the trained model does not represent the behavior, as many sam- ples are wrongly classi\fed. Hence, in over\ftting, the trained model is too adherent to the data, so much that it might not apply to other samples. Therefore, both scenarios are undesired and should be avoided.", "2.3.3 Hyperparameter Optimization", "Hyperparameters can be de\fned as intern variables of the model, such as the quantity and depth of trees in a RF, or even the size of a SOM. Although some of the hyperparameters are typically adjusted as the training occurs, others are \fxed and have to be chosen in the model's creation, especially regarding the model's topology. After training, the model is considered well \ftted. In terms of algorithms, this means that its adjustable hyperparameters have been correctly tuned. The literature about the speci\fc techniques and the libraries' documentation help determine which values or options to choose, but it can still be a challenging and long task, as it may require several training trials. Besides, it is necessary to record a detailed follow up of which options and combinations have already been tried.", "Some algorithms have been developed during the 2010's to automate the search for the optimum hyperparameters tuning, such as SMAC, Spearmint, Hyperopt, Vizier, and Autotune. These examples are considered to be \\de\fne-and-run\", or static algorithms, that depend on the user to explicitly de\fne the optimization strategy (AKIBA et al. , 2019).", "A new software developed in Python, known as Optuna, was \frst published in AKIBA et al. (2019) during the development of this work, and was implemented to perform the hyperparameter optimization. It is considered a \\de\fne-by-run\" algorithm, which constructs the search space dynamically, besides being easy to use and able to optimize di\u000berent ML techniques simultaneously.", "As Optuna is a new software, few works applying it have been published. How- ever, there are already works applying Optuna combined with the three libraries used in the present work: scikit-learn, Keras and MiniSom (AMAYA et al. , 2020; MONSHI et al. , 2021; PANYAKAEW et al. , 2020), but none of them in engineering \felds. Therefore, this work will be one of the \frst to combine those libraries and use this hyperparameter optimization tool in chemical engineering studies, such as", "2.4 Previous Works in Fault Detection and Di-", "agnosis in Chemical Processes with Arti\fcial Intelligence Regardless of applying real or simulated data in FDD systems, it is important to notice that supervised AI algorithms need labelled data to perform the classi\fcation- based task. Therefore, the process operators' or simulation experts' knowledge about the dynamic behavior of the process is essential to create those labels. The exact moment where a fault-free scenario becomes a faulty situation might be di\u000ecult to determine, as the dynamic behavior sometimes suggests the existence of a pre-fault region between them. Besides, the data should be treated and normalized before AI application.", "In PARK et al. (2020), a work published less than a year ago, an extensive review on FDD systems application in industrial processes is made, organized by the method approach. Many AI examples are commented on, but few of them are strictly related to chemical engineering processes.", "However, it is possible to observe the relevance of the AI algorithms applied in the present, such as RF, KNN, and NN various types, including convolutional ones. In FAN et al. (2020), one of the cited works in PARK et al. (2020), RF are applied to determine the most important variables in FDD performance, and the same will be done in the present work.", "During the execution of the present work, some publications were made applying AI in FDD systems in chemical processes. In ARUNTHAVANATHAN et al. (2020), unsupervised learning is associated with cognitive modelling, aiming to detect and diagnose unknown faults. The system can include this new label and train a dy- namic NN to classify this fault. The chemical process of interest is the TE, already commented on in this Chapter.", "In GE et al. (2021), a reactive distillation unit is studied once the integrated process is complex, and many abnormal events might happen. The formic acid production process was dynamically simulated and validated, and thirteen relevant faults were simulated using sixteen process variables. A CNN was applied to perform the FDD, achieving 91.31% of accuracy. This paper is one of the most similar to the present work, as the chemical process and its faults are simulated, the data are treated and organized in training and testing datasets, and then used for the NN learning to perform the classi\fcation of the expected faults. In GRANZOTTO (2020), FDD systems using SVM, NN and fuzzy logic, includ- ing a hybrid approach were developed. Two case studies were used: cyclopentanol production and a chemical plant with two reactors.", "Overall, the cited works assure AI algorithms' capabilities and high e\u000eciency to perform FDD in chemical processes. There are possibilities to study various algorithms and apply them to complex and relevant processes in existing industries. Therefore, it is a very promising \feld in chemical engineering, and is a \feld of study in the Graduate Program where the present work was developed (EPQB). Some works have already been published by the researchers of the departament that address the application of AI in chemical processes' FDD (DE SOUZA JR and ROJAS SOARES, 2018; PEREIRA PARENTE et al. , 2019; SOUZA JUNIOR et al. , Chapter 3 Methodology", "3.1 Introduction", "This work is composed of the static and dynamic simulations of the chemical pro- cess of interest, a two-column SWTU, and posterior data treatment and FDD. The dynamic simulation was run multiple times to test di\u000berent scenarios and behaviors of normal operation and relevant faults.", "Data processing includes saving the dynamic data, adding noise to the process variables, and setting the class of each moment of the simulation based on its be- havior. To represent process dynamics more accurately, forward modeling was used, including in each row of the dataset present and past inputs, at di\u000berent sampling times. Several runs are then concatenated to create training and test databases. Before AI applications, the databases are standardized to reduce features' in- uence due to their absolute value. FDD are performed through the classi\fcation of the data in fault-free or one of the process's simulated faults. The results are then evaluated mainly through accuracy and confusion matrices. These steps are summarized in Figure 3.1 and described in more detail in the following Chapters. Figure 3.1: Summary of methodology.", "3.2 Hardware and Software", "All the simulations, calculations, and codes were developed and run in a PC IntelR CoreT i5-3210M 2.50GHz, 4 GB RAM, Operational System Windows 10, 64 bits. The static simulation was performed using Aspen Plus®V10 and was upgraded to a dynamic simulation with Aspen Plus Dynamics®V10. Data processing and AI algorithms were developed in Python 3.7.5 in the inte- grated development environment Visual Studio Code 1.39.2. PCA, RF, SVM and KNN algorithms were built with the library scikit-learn 0.23.2, MLP and CNN were designed using Keras 2.3.1 with TensorFlow 2.0.0 backend, and SOM were designed with MiniSom 2.2.5. Optuna 2.4.0 was applied to optimize the topology and hyper- parameters of all techniques. The libraries NumPy 1.17.3, pandas 0.25.3, scipy 1.3.1, matplotlib 3.1.0, pickleshare 0.7.5, mlxtend 0.18.0, psynlig 0.2.1.dev0 and seaborn 0.9.0 were used in several codes for data manipulation, calculations, and graphic results.", "3.3 Arti\fcial Intelligence Metrics and Results", "Evaluation To evaluate the classi\fcation results of FDD performed in Chapters 6 to 9, SUTHAHARAN (2015) suggests and describes some qualitative measures, also de- tailed in scikit-learn library documentation. The \frst concept to be introduced is the confusion matrix. It is a way to compare the predicted class and the correct class of each test input. Generally, the predicted classes are represented as the columns and the true classes as the rows. There are four possible status: True Positives (TP): Correct classi\fcation, positive classi\fed as positive; True Negatives (TN): Correct classi\fcation, negative classi\fed as negative; False Positives (FP): Incorrect classi\fcation, negative classi\fed as positive; False Negatives (FN): Incorrect classi\fcation, positive classi\fed as negative. In a confusion matrix CM, for class i, the element CMi;irepresents the true positives; meanwhile, the rest of the main diagonal are true negatives. All the values in the i row are false negatives and in the icolumn are false positives. Considering class 2 as classiin a problem with four classes, Figure 3.2 shows the confusion matrix. Figure 3.2: Four class confusion matrix analyzing class 2.", "A useful metric to verify the classi\fcation as a whole and compare di\u000berent tech- niques results is accuracy. It is the ratio between the number of correct classi\fcations and the total number of samples, calculated as the sum of the main diagonal divided by the sum of the entire confusion matrix. In more simple terms, it is the general proportion of correct classi\fcation. Equation (3.1) expresses this calculation: Accuracy =TP+TN TP+TN+FP+FN(3.1) Classi\fcation e\u000eciency can also be analyzed in a more class-based way. The adequate metric to evaluate the sucess of the classi\fcation strongly depends on the studied problem, and also on the available dataset. It is suggested by CHOLLET (2017) that precision and recall, also called sensitivity, are adequate metrics for class-imbalanced problems, which is the case in this work. The same author states that metrics such as receiver operating characteristic (ROC) curve and the area under the curve (AUC) are more suitable for problems with well-balanced datasets, which is not the case. The class proportion will be discussed further in Chapter 4. Precision can be described as the ability to not classify a negative sample as pos- itive or how many of the positive classi\fcations are genuinely positive. It expresses the proportion between false and true positives (SUTHAHARAN, 2015), as shown in Equation 3.2:", "Precision =TP Recall expresses the relative amount of false negatives and true positives (SUTHAHARAN, 2015). It can also be described as the missed positive samples, as a proportion of actual positives rightly classi\fed as positives, expressed in Equation Recall =TP A metric that embodies precision and recall for each class is known as the F-beta score, the harmonic mean of those two performance metrics. Beta is the weight ratio between recall and precision and, if de\fned as 1, both values are equitably relevant, and the result can be called F1-score. Equation 3.4 shows the F1-score calculation: F1-score =2\u0001precision\u0001recall precision +recall(3.4) To evaluate the reproducibility of the metrics, training accuracy boxplots were made for each technique. Boxplots are graphic representations of data distribution, facilitating analysis. According to WICKHAM and STRYJEWSKI (2012), it was de\fned for the \frst time by SPEAR (1952) and improved by TUKEY and W. (1977), achieving a scheme very similar to the one applied nowadays. Boxplots are composed of the median, upper (Q3) and lower (Q1) quartiles, upper and lower extremes, and outliers (WICKHAM and STRYJEWSKI, 2012). The extremes are data up to 1.5 times the interquartile range (IQR) from upper and lower quartiles, and outliers are data more distant than the extremes. Due to the extremes' dependence on the data analyzed, the whiskers might not be symmetrical. Figure 3.3 illustrates this graphic and its structure.", "Figure 3.3: Boxplot structure.", "Learning curves were also used to analyze training e\u000eciency, using di\u000berent portions of the dataset of various sizes to evaluate training accuracy behavior as a function of the amount of training data. Those curves are also a way to perceive if there has been over\ftting in the training process. Boxplot and learning curve analysis were based on a Kaggle notebook available in AFRNIOMELO (2020). Another way to compare the applied techniques is the computing or running time, measured in all the AI algorithms. The smaller the time needed to achieve the results maintaining the quality, the better it should be to real and online applications. The computing time was measured in two di\u000berent ways in the present work. The \frst one is reduced computing time, aiming to compare only the di\u000berences between methods. This metric includes training and testing time, ignoring steps that would be the same for all methods, as data reading and the generation of graphic results. It also does not account for hyperparameter optimization, as it relies on the number of optimized variables and trials, making the comparison uneven. The other one is full computing time, which includes the entire script for each technique. Besides comparing classi\fcation e\u000eciency and running time between the applied algorithms, another interesting analysis to perform in this work is variable or feature importance. The objective is to determine which of the process variables measured and added to the database are more relevant to perform an e\u000ecient classi\fcation. Feature importance can be evaluated based on each component's most relevant variables in PCA, the variable importance examination performed by RF, and fea- ture maps available in SOM. Those will be performed, explained in detail, discussed in their respective Chapters, and compared further in this work. Therefore, the possibility of reducing database size and running time based on those analyses will also be debated on a suitable occasion.", "Chapter 4 Case Study", "4.1 Introduction", "SW is one of the most concerning wastewater in re\fneries, contaminated mainly byH2SandNH 3. Those compounds are removed from the stream by a stripping process in SWTU, composed of one or two columns, depending on the amount of NH 3in SW. The two-stage version of the process creates three streams: acid and ammoniacal gases to be treated separately, along with treated water, with the small- est possible amount of the contaminants. This process con\fguration is operationally more complex, especially regarding the \frst stripper's heat duty control. This Chapter addresses a literature review on the topic and explains how the treatment process's static and dynamic simulations were performed and con\fgured in the present work. The results are showed and discussed based on Brazilian envi- ronmental laws.", "It also consists of a description of the faults simulated, what phenomena they represent and how they were performed. The behavior of the process when distur- bances and faults are applied is discussed. The last section details how the data obtained was handled to become the \fnal dataset used in the next Chapters.", "4.2 Literature Review", "4.2.1 Sour Water", "SW is an e\u000fuent from petrochemical processes, originated by the contact be- tween water or steam with hydrocarbons streams containing sulfur and nitrogen residues. They move from the organic to the aqueous phase as H2SandNH 3 DespiteH2SandNH 3being the primary contaminants in SW, those e\u000fuents may also consist of CO 2, in smaller amounts, along with cyanides and phenols. SW composition depends on its origin. Typically, SW contains between 300 and 12,000 ppm ofH2Sand between 100 and 8,000 ppm of NH 3. Cyanides concentration often varies, and phenol tends to be only up to 200 ppm (ADDINGTON et al. , 2013).CO 2 tends to be only traces of the composition (WEILAND and HATCHER, 2012a). If the stream contains phenols and cyanides, it is considered a phenolic SW, commonly generated in the use of steam in distillation and in hydrocarbon par- tial pressure reduction in thermal or catalytic cracking. Nonphenolic SW is also known as HDS water; once they are generally a byproduct of re\fnery hydrotreating, also referred to as hydrodesulfurization (HDS) (QUINLAN, 2003; WEILAND and HATCHER, 2012a).", "Phenolic SW, or non-HDS water, might be corrosive, not recommended to be reused as wash water after treatment. Nonetheless, stripped nonphenolic water is adequate for this application (QUINLAN, 2003; WEILAND and HATCHER, 2012a). Overall, SW is one of the major aqueous e\u000fuents in petroleum re\fneries (LEAVITT and ET AL, 2004).", "In BARROS (2016), the author uses a re\fnery in Brazil, processing Brazilian crudes, as an example. Around 35% of all SW generated derives from hydrotreating; therefore, it is nonphenolic and contaminated only with H2SandNH 3. All the other sources produce phenolic SW with di\u000berent compositions, containing less H2Sand NH 3than the nonphenolic SW produced in the re\fnery.", "4.2.2 Sour Water Treatment Units", "To remove its contaminants, SW e\u000fuents are generally stripped in SWTU. The stripping process can be made using a reboiler or by direct steam injection, and it is common to use treated water (TW) to heat the SW column feed (QUINLAN, 2003). Acid gas (ACG) is discharged at the top with the contaminants. Figure 4.1 displays a simpli\fed scheme of SW stripping using a reboiler. The main components, as already commented, are H2S,NH 3,CO 2andHCN in water. The stripping process intends to remove the weak electrolytes from the aqueous phase. Reactions 4.1 to 4.8 are listed below as presented in LEE et al. (2002) and are used to describe the system:", "CO 2+ 2H2O$H3O++HCO\u0000 HCO\u0000 NH 3+H2O$NH+ NH 3+HCO\u0000 Figure 4.1: SW stripping simpli\fed scheme (based upon QUINLAN and HATI Nonphenolic SW systems are described by Reactions 4.1, 4.2, 4.3 and 4.6 if there are onlyH2SandNH 3in the system and also by Reactions 4.4, 4.5 and 4.7 if CO 2 is present. Reaction 4.8 describes cyanides dissociation.", "It is important to add that H2SandNH 3, when together in an aqueous solution, have nearly limitless solubility. This probably happens because ammonia is volatile and reactive, so it will continue to be absorbed if present in a gas phase with H2S due to becoming protonated as H2Sis simultaneously absorbed. Therefore, the solubility of those two components together is higher than the solubility of them separately (HATCHER and WEILAND, 2012).", "In re\fneries, ACG heads to Sulfur Recovery Units (SRU), which generates ele- mental sulfur and also SOx, that should be minimized (BASTOS et al. , 2005). A Claus SRU operates by burning a part of the H2Sreceived in ACG using air, gener- atingSO 2. The remaining H2Sin the system reacts with SO 2, producing elemental sulfur. It is expected that in the SRU around a third of the H2Sis combusted and NH 3is totally burned (QUINLAN and HATI, 2010). Water is a byproduct of the three reactions, as shown in Reactions 4.9 to 4.11:", "When the re\fneries process high-nitrogen crudes, SW absorbs a higher amount ofNH 3. This usually is the case in Brazilian crude oils, which are typically heavier than light Arabian crudes, containing one-third of the sulfur wt% and \fve times the nitrogen wt% (BELATO et al. , 2002).", "If higher amounts of NH 3are absorbed by SW, its concentration increases in ACG. SRU faces economic and technical di\u000eculties if ACG has high NH 3concen- trations, such as the need for a larger SRU equipment, due to performance decrease and incomplete NH 3processing, leading to higher NOxgeneration. There are also some operational complications, like catalyst deactivation and plugging. Plugging may occur when NH 3is not entirely consumed, and NH 3-H2Ssalts are formed in the equipment's cooler parts (ADDINGTON et al. , 2013; QUINLAN and HATI, One of the most indicated solutions is to strip the e\u000fuent in two stages, remov- ingH2Sin the \frst step and stripping NH 3afterwards. Those contaminants are concentrated in di\u000berent gas streams and treated separately (QUINLAN and HATI, 2010). The stripping operation releases CO 2, when present in SW, along with H2S andNH 3. In phenolic SW streams, strong or nonvolatile acids can be dealt with by inserting caustic soda in the second column; meanwhile, a phenol extraction step might be needed depending on this contaminant's concentration (KOHL and NIELSEN, 1997).", "SWTU with two columns to treat SW with ammonia was developed by Chevron and is known as Chevron Waste Water Treatment (CWWT). This process generates mainly three streams: acid gas (ACG), ammoniacal gas (AMG) and stripped water, or treated water (TW) (KOHL and NIELSEN, 1997). The aqueous stream between the columns is de\fned as pretreated water (PW). Figure 4.2 shows a simpli\fed scheme of this process.", "In this scheme, the H2Sstripper operates at around 9.5 bar; meanwhile, the NH 3stripper operates at a lower pressure, about 2.3 bar. When there is only one column in the SWTU, it generally operates at a similar pressure to the NH 3stripper in a two-stage process, around 2.3 bar (QUINLAN and HATI, 2010). As well as in one-stage SWTU, it is common to use PW and TW to heat SW and other intermediate streams of the process (KOHL and NIELSEN, 1997; QUINLAN Figure 4.2: SW two-column stripping simpli\fed scheme (based upon QUINLAN and and HATI, 2010). Energy integration may be presented in multiple con\fgurations, depending on what is practical, e\u000ecient and economically viable for each unit in particular. In this work, new water (NW) is de\fned as water input in the system without contaminants to dilute TW, generating clean water streams (CW), which might also be used in the energy integration of the process. As stated previously, multiple re\fnery units produce SW as byproducts, mainly of fuel gas, lique\fed petroleum gas (LGP), diesel, gasoline and coke production. The SWTU, along with SRU, is a part of the sulfur block, a group of operations and units in re\fneries that treat sulfur streams, aiming to generate elemental sulfur. Meanwhile SWTU strips SW, Amine Regeneration Units (ARU) treat rich amine and generate ACG as well, besides lean amine. ACG from both sources is treated in SRU (QUINLAN and HATI, 2010). Figure 4.3 illustrates the steps previously described and how SWTU is integrated to the sulfur block, in a two-stage stripping operation.", "The two-column SWTU, as described before, generates TW and AMG along with ACG. TW is reused in Waste Water Treatment (WWT) and some of the re\fnery units. There are no restrictions to the amount of H2Sin the stream, once normally it is mostly eliminated in the \frst column. Meanwhile, NH 3speci\fcations depend on the WWT limitations, usually less than 80 ppm (HATCHER and WEILAND, Figure 4.3: SWTU as a part of the sulfur block in re\fneries (based upon QUINLAN According to Brazilian environmental laws detailed in BRAZIL (2007), ACG should eliminate at least 90% of the H2Spresent in the SW to be treated. Therefore, PW should contain 10% of it, at most.", "AMG heads to an ammonia converter designed to destroy NH 3. This operation generates nitrogen and SOx, minimizing NOxemissions. It is also essential to mini- mizeSOxemissions, as commented previously, so most of H2Sshould be eliminated in ACG, before the second stripper column and AMG generation (QUINLAN and HATI, 2010). This stream's speci\fcations depend on the other streams and should have the least amount of H2Sand water possible, being rich in NH 3. The two-column SWTU design presents some operational complications, such as heat duty control in the H2Sstripper. In HATCHER et al. (2014), some simulations examples are discussed, and it is shown that changes in the heat duty of this column have contrary e\u000bects in [ H2S] and [NH 3] in the top product. The more steam is input to the reboiler, the higher is H2Srecovery in ACG, but NH 3recovery also increases, which is undesired. That makes increasing H2Sin ACG, while reducing NH 3concentrations to protect the SRU, con icting objectives.", "In addition to these observations, HATCHER et al. (2014) also show that mini- mal changes in the \frst column's heat duty may lead to signi\fcant increases in the presence of NH 3in ACG. For example, an increase of about 3% in the reboiler steam ow raisesNH 3recovery from below 0.00001% to almost 1%, for external re uxes percentages smaller than 2.5% (HATCHER et al. , 2014). That con\frms that the operational range of this variable is narrow.", "In such cases, the increase in heat duty also leads to greater water vaporization, rising ACG temperature. If the situation is not reversed, the phenomenon that may be called \\column boil up\" happens, reaching the column's \\operational limit point\". As the top and bottom temperatures approach each other, most of the water is vaporized. When this happens, NH 3is highly discharged in ACG; what should not happen, as already discussed in this section (KNUST, 2013; MORADO, 2019). Overall, SWTU with two columns are recommended when NH 3amounts in SW are higher than SRU can handle safely, discharging H2SandNH 3in separate top products with di\u000berent destinations in the re\fnery. However, this increases the op- erational di\u000eculty, mainly because of the high solubility that those compounds have when together in water and the sensitivity in the \frst column's heat duty control. To guarantee small amounts of NH 3in ACG and H2Sin AMG it is important to reduceSOxandNOxemissions.", "4.2.3 Previous Works in Sour Water Stripping Modelling", "and Simulation Many studies contemplating SWTU of di\u000berent con\fgurations have been made through the last thirty years, although only a few of them consider two-column designs. Besides, most of those works focus on designing the unit and using the static data for multiple evaluations, such as cost and components' recuperations optimizations. Therefore, very few works have been made focusing on dynamic behavior analysis, especially under abnormal events, such as in the present work. In WEILAND and HATCHER (2012a) and HATCHER and WEILAND (2012), the design of SWTU of one stripper only is explored using a mass transfer rate- based simulation model. Meanwhile, WEILAND and HATCHER (2012a) focused in determining tray e\u000eciencies and variables that a\u000bect them, in HATCHER and WEILAND (2012) that is also discussed, along with other analysis to examine the reliability of SWTU design using this kind of model.", "Mass transfer rate-based simulation is also used in HATCHER et al. (2014); however, it studies two-stage SWTU, CWWT alike. That work aims to analyze its performance and the production of separate and relatively pure streams of H2S andNH 3. In ADDINGTON et al. (2013), the same approach is applied, discussing in detail the di\u000berence between this type of model and an ideal stage approach. Both con\fgurations, one-stage and two-stage stripping, are simulated, discussed and compared.", "One of the closest approaches to the present work is found in LEE et al. (2002), using Aspen Plus®to simulate the static simulation and Aspen Plus Dynamics®to evaluate the dynamic behavior and control strategies of a two-stage SWTU stripped by steam. The same authors continued that work in LEE et al. (2004). Another work focused on process dynamics is VAIDA and CRISTEA (2016), where the authors used Hysys®to analyze the dynamic behavior of two di\u000berent con\fguration designs of a single-stripper SWTU.", "Speci\fcally about the Brazilian scenario, BARROS (2016) and KNUST (2013) studied two-stage SWTU using a static approach. The work developed by BARROS (2016) utilizes phenomenological and empirical approaches, aiming to create a soft sensor both to monitor and control H2Sremoval; meanwhile, KNUST (2013) used Hysys®to perform the process simulations, applying Response Surface Models to predict ACG composition.", "Recently, MORADO (2019) used Hysys®to create the static and dynamic sim- ulation of the \frst column of a two-stage SWTU. That work used Response Surface Models to develop models to the operational limits of the stripper in order to perform advanced control of the heat duty. That work emphasizes this control's importance and how sensitive the column is, as already described in this section. The present work concentrates on both static and dynamic simulations of a two- column SWTU using Aspen Plus®and Aspen Plus Dynamics®, just like in LEE et al. (2002) and LEE et al. (2004). However, the overall intent here is to generate a database that represents the process' behavior in multiple scenarios, which is di\u000berent from the works commented previously.", "4.3 Static Simulation", "4.3.1 Simulation Con\fguration", "The simulation represents a SWTU responsible for cleaning a water stream with small amounts of only H2SandNH 3, that is, nonphenolic SW. The GPSWAT (GPA Sour Water Equilibrium) property model was chosen to simulate the system, as de- \fned by ADDINGTON et al. (2013) as the result of several studies and research developed by the GPA (Gas Processors Association) in SW systems. Among many other research reports written throughout a decade, RR-118, also known as \\GP- SWAT GPS Sour Water Equilibria Correlation and Computer Program\" (WILSON and ENG, 1990), was completed as an outcome of the research. This model is available in Aspen Plus®V10. As stated in the software's docu- mentation, it is designed for gas treating and SW systems containing mainly water, NH 3,H2SandCO 2, dealing with the chemical reactions involved, such as Reactions 4.1 to 4.8. The equilibrium constants applied in the software were \ft to data and are temperature dependent.", "This method uses the GPSWAT property models for vapor and liquid fugacity coe\u000ecients of those components. The vapor phase nonideality is modeled by a virial equation, and its second and third coe\u000ecients are obtained through speci\fc calculations. More details about the equations that represent the model are available in Aspen Plus®V10 help section.", "Besides, the model is considered applicable from 20\u000eC to 315\u000eC and up to 138 bar, be\ftting with the values present in this work. This is a range extension of the original SWEQ model.", "The SWTU studied here has two columns, each one to remove one of the con- taminants. The \frst one discharges H2Sat the top as ACG, and the second one releasesNH 3also at the top as AMG. The following block diagram, shown in Figure 4.4, summarizes the main steps of the process. Each color represents a di\u000berent type or composition of the streams.", "Figure 4.4: Block diagram.", "As shown in Figure 4.4, the process presents two inlet streams { SW1 and NW1 { and \fve outlets streams { ACG2, AMG2, CW4, CW6 and CW9. SW1 is SW to be treated, and NW1 is a new water source without contaminants that dilutes the treated water stream after going through the two columns. Those streams and all the others presented in this work will have their mass ow described as a function of SW1, which is de\fned as SW1F. Table 4.1 shows the physical properties of the inlet streams.", "ACG2 and AMG2 are the acid gas and ammoniacal gas outlets, respectively. Table 4.1: Inlet streams properties.", "CW4, CW6 and CW9 are the clean water outlets of the same composition; the latter exits the process after preheating the SW inlet at the beginning of the process. The complete process consists of thirty-four material streams, three heat streams and twenty-three equipment: twelve valves, three heat exchangers, three splitters, two mixers, one pump, and two columns. The system was modeled as shown in Figure 4.5, also added in a larger version in the Appendix A. The same color scheme was adopted in Figures 4.4 and 4.5, except for the Heat Streams, which are not represented in the \frst one.", "Figure 4.5: Static simulation.", "A complete walkthrough of the process is stated ahead: the process begins in SW1 inlet stream, preheated in H1 and H2 through energy integration with CW7 and PW5. SW4 is split into SW5 and SW7 { the \frst one becomes SW6 after passing V2 and enters C1 at the top. The latter is preheated once more in H3 in integration with PW4 and becomes SW9 after passing V3 before heading to C1, with a lower entrance.", "C1 is heated by Q1, reboiler duty, and has two outlets: acid gas at the top as ACG1 and pretreated water at the bottom as PW1, which is split into PW2 and PW4. After being used to preheat SW, as stated above, PW4 becomes PW5 and then PW6, when it's mixed back with the rest of C1's bottom product. Despite the splitting, all of the pretreated water goes into C2.", "C2 condenser and reboiler duties are Q2 and Q3, respectively. Ammoniacal gas leaves through the top as AMG1 and treated water leaves through the column's bottom as TW1. This stream is mixed with NW2, becoming clean water (CW1). After going through a pump, clean water is split and leaves the process as CW4 and CW6, while CW7 is used to preheat SW, as commented previously. This stream becomes CW9 and leaves the process.", "The simulation was run in steps, adding equipment and energy integrations one at a time until it was complete, stable, and converging. It needed to be not only converging, but it should have coherent results; that is, ACG2 should be mostly composed of H2S, with little NH 3; meanwhile, AMG2 should discharge most of the NH 3that remained in the pretreated streams. It is also expected TW2 to contain most of the water initially present in SW1, meaning that only small percentages of it were lost in the gases discharged.", "The \frst pieces of equipment added to the process were the columns. To model both of them, the chosen operation was the \\RadFrac\" column. According to AL- MALAH (2017) and to the software's documentation, such choice is considered to perform much more rigorous modeling than other options available in the software, being adequate to absorption and stripping, and highly non-ideal liquid phases or with proceeding chemical reactions. \\RadFrac\" columns were also used in LEE et al. (2002) to simulate the static and dynamic behavior of a two-stage SWTU. The calculation type was set as Equilibrium for C1 and C2. The duties will be expressed as a function of the reboiler duty de\fned for C2: C2RD. Table 4.2 show the other parameters set in the column's speci\fcations.", "All the heaters are \\HeatX\", an energy integration heat exchanger available in the software. Model \fdelity was set as shortcut. Table 4.3 shows the heater's speci\fcations.", "Table 4.2: Columns speci\fcations.", "Table 4.3: Heaters speci\fcations.", "Mixers M1 and M2 were con\fgured to accept vapor and liquid as valid phases. Splitters' T1 and T2 split fractions were set as 10% to SW5 and 2.17% to PW2, respectively. T3 split the inlet stream in 81.69% to CW5 and 18.31% to CW7, so that CW3 and CW4 are zero. Consequently, valve V10 is closed and will be used in a safeguard control, only opening if necessary. Pump P1's pressure increase was de\fned as 28.1 bar, and its hydraulic static head was set as 5.8 m. Most valves were set to accept only liquid as a valid phase, except V4 and V7, which were set to be vapor only. In all of them, the outlet pressure was de\fned to create a small pressure drop. Those were the last equipment added to the static simulation, when it was already converging and stable, to prepare for the dynamic step of the work.", "4.3.2 Simulation Results", "Initially, the results will be evaluated by analyzing the columns' behavior and pro\fles and inlet and outlet streams properties, verifying separation e\u000eciency. After that, the process as a whole will be analyzed, comparing inlet and outlet streams, component recuperations, and checking if global mass balance, as well as per com- ponent, were correctly ful\flled. Figures 4.6 to 4.8 show the temperature, pressure, and mass fraction composition pro\fles in C1.", "Figure 4.6: C1 Temperature and pressure pro\fle.", "Figure 4.7: C1 liquid composition pro\fle.", "Figure 4.8: C1 vapor composition pro\fle.", "C1's temperature and pressure pro\fles are almost linear through all the stages. Liquid and vapor H2Sare reduced in every stage of the column until reaching the bottom. Since this column aims to discharge the highest possible quantity of this component at the top, that is expected and desired.", "Despite the variations, NH 3at the top is similar to those in the bottom in liquid streams. In vapor streams, this concentration reduces as it approaches the top outlet, which is also expected and wanted as this is an undesired component of ACG. The vapor stream behavior is be\ftting with the liquid one regarding NH 3. As liquid concentration at the top is similar to the one at the bottom, almost none is directed to the gas outlet.", "C1's reboiler duty, Q1, needed to perform the separation was 47.20% C2RD Gcal/h, and Table 4.4 displays the properties of C1's inlet and outlet material streams.", "Table 4.4: Inlet and outlet streams in C1.", "It is possible to observe that almost 95% of ACG is composed of H2S, with a small amount of water and very little NH 3. Water fraction increased from 99.28% in C1's inlet streams to 99.62% in PW1, and H2Sdecreased from 0.40% to 0.06% considering the same streams.", "CWWT is described in KOHL and NIELSEN (1997) as an ammonia-containing SW treatment process with an H2Sstripping column followed by an NH 3stripping column like the process studied in this work. This design aims to treat SW streams containing 0.3 to 6.0 wt% NH 3and 0.3 to 10.0 wt% H2S. The treated stream composition in the present work \fts in those ranges so that the expected results and outlet streams compositions are comparable. In a typical CWWT process, ACG leaves the top of the \frst column containing less than 100 ppm NH 3, which is consistent with this work's results. Figures 4.9 to 4.11 show the temperature, pressure, and mass fraction composition pro\fles in C2.", "Figure 4.9: C2 Temperature and pressure pro\fle.", "Figure 4.10: C2 liquid composition pro\fle.", "Figure 4.11: C2 vapor composition pro\fle.", "As well as in C1, C2's pressure pro\fle is also linear. This is expected in both columns, as it was speci\fed in the equipment con\fgurations. However, the temper- ature pro\fle has a considerable increase in the higher stages while presents small variations in the lowest ones. This is be\ftting with vapor and liquid streams behav- ior of bothH2SandNH 3, as they change little between stages 2 and 6 in comparison with the \frst stage variation. In both streams, the contaminants' concentration in- creases as they approach the top outlet, meaning most of NH 3and the remaining H2Sare discharged in AMG; meanwhile, treated water leaves C2 much purer. Reboiler and condenser duties, Q3 and Q2, needed in the second column were C2RD and 81.91% C2RD Gcal/h, respectively. Table 4.5 displays the properties of C2's inlet and outlet streams.", "Table 4.5: Inlet and outlet streams in C2.", "Treated water is composed of 99.97% of water, 0.03% of NH 3, and contains only traces ofH2S(18.7 ppm), which proves that the separation was e\u000ecient. Based on the same CWWT process described in KOHL and NIELSEN (1997), stripped water leaving the second column's bottom should contain less than 10 ppm H2Sand 50 ppmNH 3. This is almost be\ftting with the H2Sresults of the present work, and about six times the maximum NH 3recommended amount, even though it is a small concentration.", "To analyze the components' recuperations, to verify the process's separation e\u000eciency, the ratio between the component's mass ow in the outlet streams and in SW1 was calculated. Water recuperation was based on TW1 before the dilution, considering a control volume described in Figure 4.12. The objective is that the recuperation of H2Sin ACG1,NH 3in AMG1, and water in TW1 are the closest to 100% as possible. Those numbers are displayed in bold in Table 4.6. Figure 4.12: Control volume to calculate recuperation.", "Table 4.6: Components' recuperations.", "Table 4.6 implies that global mass balance and per component balances were accomplished in this control volume. Over 85% of the H2Sleaves the process in ACG, as desired, and most of the remaining amount is discharged in AMG; that is, less than 0.5% of its inlet is present in treated water. Although BRAZIL (2007) de\fnes that H2Srecuperation in ACG should be at least 90%, as stated previously, this simulation does not reach this limit, and this problem will be brought up again in the next section, with a proposed solution.", "Almost 91% of NH 3leaves the process in AMG, and very little of it is discharged in ACG, both also as wanted and expected. The rest of it remains in treated water. Considering the results of Table 4.5, although almost half of the AMG is composed of water and 5% of the ACG, as shown in Table 4.6, only 0.3% of the inlet process water was discharged in the gas streams. That means 99.7% of the water was recovered in the treated stream.", "As a diluted version of the treated streams, the clear water streams have a higher water concentration: from 99.97% to 99.98% and 0.02% of NH 3and even fewer traces of H2S(13.4 ppm). As explained earlier in this section, this stream is split into three ways before leaving the process. However, CW4 has no ow as V10 is closed, so CW6 and CW9 leave the process as described above and also in Table Table 4.7: Outlet streams of the process.", "Based on the results available, it is veri\fed that the static simulation runs as expected, has reliable and desired results in most of the streams, and is ready to be upgraded to a dynamic simulation to reproduce normal operation and selected faults.", "4.4 Dynamic Simulation", "The static simulation was exported as pressure driven from Aspen Plus®to Aspen Plus Dynamics®. Pressure driven simulations are considered more realistic and adherent to chemical processes' physical properties (LUYBEN, 2007). The dynamic simulation consists of the same material and heat streams, as well as equipment and con\fgurations of the static simulation. At \frst, the software automatically added some controllers when the simulation was converted from static to dynamic, to ensure stability. They were mainly level controllers of the tops and bottoms of the columns, as well as pressure controllers for acid and ammoniacal gases. Those controllers were the \frst to have their strategy adjusted and to be tuned.", "Aftwerwards, other controllers were added to guarantee process operability and stability. Additionally, the inclusion of more controllers also helps in the convergence of the dynamic simulation. There are fourteen controllers:", "Two of pressure in the columns' tops; Four of level in the columns' drum and sumps; Three of temperature, including one prior to the \frst column, one in its second stage and another in the second column's top; Four mass ow controllers, two before the \frst column and two after the second column; One control integrating mass ow and heat duty in the second column.", "After the insertion of controllers, the dynamic process was run as shown in Figure 4.13, also added in a larger version in Appendix A. As well as their in the static simulation, the streams color scheme remains the same. Table 4.8 shows the list of controllers in dynamic simulation, as well as manipulated and controlled variables. Figure 4.13: Dynamic simulation.", "Most valves' initial set point is around 50%, with opening range between 0% and 100%. There are only two exceptions, V10 and V5. The \frst one, as explained in the previous section, only opens if necessary, as if it were a drain for C2's sump. It opens as manipulated by C2S-LC2, the safeguard sump level control. V5's initial set point is around 10%, and it has a travel stop that limits its minimum opening to 2%, due to process requirements.", "The design of the control system was based on general rules from the literature (LUYBEN, 2013) and on industrial experts' advice. The controllers, all PI, were added one at a time to guarantee convergence, studying how the process reacts to the addition of each of them. They were tuned using the process reaction curves as explained in SEBORG et al. (2011). Assuming that the process can be considered as \frst-order or second-order, the gain and time constants can be extracted from the reaction curves. Table 4.9 shows the \fnal results after tuning and detuning of the controllers described above.", "Table 4.8: Controllers in dynamic simulation.", "Table 4.9: Controllers' tuning parameters.", "According to Table 4.9, the highest tivalues of this process, after controllers tuning, are between 20 and 40 minutes. As expected, those higher time constants are related to the columns since they have very slow dynamics and high time constants (SEBORG et al. , 2011); therefore, they are the operations that limit the process dynamics. Those time constants are so high that the controllers might be considered to be only proportional, without integral action in practice. As stated previously, one of the main goals of this process is to achieve at least 90% ofH2Srecuperation in ACG and, in the static simulation, this value is only a little over 85%. Hence, after tuning the controllers, the simulation was run with several small manual increases in C1S2-TC's set point, aiming to enhance H2Sin this stream. This was made repeatedly until detecting a new stable state, in which the simulation maintained its convergence and stability; meanwhile, H2Swas higher. This run was saved when the highest possible temperature of C1S2-TC was achieved; therefore, in the highest possible and stable H2Srecuperation in ACG. This new temperature is 123.4\u000eC and the new H2Srecuperation is a little over 89%, that is, very close to the desired limit. This simulation is de\fned as the base simulation of this work and was used to perform all the runs whose data were used to generate the database. Tables 4.10, 4.11, 4.12 and 4.13 represent the updated versions of Tables 4.4, 4.5, 4.7 and 4.6, respectively, considering the new base simulation. Table 4.10: Inlet and outlet streams in C1 { dynamic.", "Table 4.11: Inlet and outlet streams in C2 { dynamic.", "Table 4.12: Outlet streams of the process { dynamic.", "Table 4.13: Components' recuperations { dynamic.", "Comparing Tables 4.4 and 4.10, as expected, a smaller amount of H2Sis directed to pretreated water, exiting on ACG instead. Analyzing Tables 4.5 and 4.11, as lessH2Sis present in C2's inlet, there is a smaller amount of this component in AMG. Treated water's composition remained the same, which leads to clean water's composition also not changing, consequently, as noted by comparing Tables 4.7 and 4.12. Unfortunately, that means that the amount of NH 3in clear water, although very low, would still be above the limit commented in the previous section of this Chapter.", "Observing Tables 4.6 and 4.13, the most striking changes can be perceived in H2Srecuperation, as expected. It has increased in ACG and decreased in AMG, continuing the same in treated water. This is a positive change since less H2Sis directed to the ammonia converter and reduces SOxemission once it depends on the amount of H2Sthat inlets the second column (BRAZIL, 2007). After de\fning the base simulation, the next step is to run the normal operation and fault simulations. To create an accurate database, a suitable sampling time has to be chosen. According to SEBORG et al. (2011), determining an adequate sampling time is less like a science and more of an art, although some guidelines and equations have been proposed over the years. One of them was chosen to evaluate this work's sampling time, as de\fned as follows in Equation 4.1, in which tdomis the dominant time constant:", "Aspen Dynamics' default sampling time is 0.01 hour or 36 seconds, and an eval- uation was made to verify if this value is adequate, as it was practical. Using the range described above in Equation 4.1 and 4tas 36 seconds, tdomvalues vary between 12 minutes and 60 minutes.", "The dynamic simulation was run with the controllers considered as only pro- portional in automatic mode, or closed-loop, and all the others in manual mode, or open loop, to perform this analysis. After stabilizing, an increase of 0.3% was applied in SW1 mass ow, and the process reaction curve in SW1-FC was visually inspected. Figure 4.14 shows this plot along with two red dashed lines representing the range of 12 and 60 minutes after the 0.3% step was applied. Figure 4.14: Reaction curve in SW1-FC.", "As expected, the process variable oscillates in the \frst moments after the step, stabilizing soon after. It is possible to assume that the complete stabilization occurs inside the proposed range. Thus, a sampling time of 36 seconds is adequate for this process, and Aspen Dynamic's default value was chosen.", "4.5 Fault Simulation", "The base simulation was de\fned after adequate controllers tuning and multiple test runs, and it was used to generate fault-free or normal operation and faulty simulations data. The faults were chosen by indication of industrial experts. A brief description of the classes, including fault-free and simulated faults, is presented in Table 4.14, along with its limit values.", "Table 4.14: Classes descriptions and limits.", "Fault-free simulations are based on light disturbances to stimulate controllers' reactions and create a more diverse database. Those same disturbances were also applied before some fault applications, making a more reality adherent process be- havior.", "There are seven di\u000berent kinds of disturbances in this work, and they can be de\fned as small increases and decreases in certain variables, listed as follows: SW1 mass ow, [ H2S] in SW1 mass ow, [ NH 3] in SW1 mass ow, SW1 temperature, SW5 mass ow, CW5 mass ow and AMG1 temperature.", "The \frst three displayed faults in Table 4.14 are related to the process' feed, stream SW1, considering signi\fcant variances in its mass ow or composition, to check how the system reacts if signi\fcant changes happen in the inlet stream. Those situations would simulate some ooding phenomena that occur in real columns.", "Faults 4 and 5 are associated with sensor problems on the top of both columns, as the controllers are receiving deceiving pressure values. In a real industrial process, that o\u000bset could be caused by clogging.", "Although the last tested fault is also related to SW1, the objective is to eval- uate how the process reacts to the \\column boil up\", an overheating phenomenon described earlier in this Chapter. This is one of the most important faults eval- uated, as it represents a greater deviation from normal operating conditions. As C1's heat duty operational range is typically narrow, small changes could increase gas acid's temperature, reducing separation e\u000eciency. Increasing the process's inlet temperature is a way to simulate those changes in the column's heat duty. The limits shown in Table 4.14 were de\fned based on multiple trial simulations, evaluating its convergence and the generation of a fault region. For class determina- tion, in this work, the fault region is de\fned as reaching the limit where one or more controllers stop being able to reverse the deviation of one or more variables from the set point. That is when the simulation leaves the controllable region. In more practical de\fnitions, the fault region generates every time a valve opens or closes completely or when the drum or sumps of columns fully dry. According to that de\fnition, every faulty simulation has part of its data considered fault-free before the fault region appears.", "All the simulations run to generate the database were set with programmable tasks based on scripts to alter the variables automatically, always in an S-ramp shape, using the function \\SRAMP\" available in the software. Regardless of whether disturbances or faults are being programmed, controlled variables are accessed through \\set point remote\", and the respective controllers have to be set to \\cascade mode\". The non-controlled variables are accessed directly.", "Fault-free and faults 1, 2, 3, and 6 simulations did not need any change in the base simulation to run, while faults 4 and 5 simulations required an operator inclusion. The chosen operator was \\SUM\", which performs the sum between two values: the pressure signal, originally sent to the controller, and the negative \fxed bias value. The result of this operation is sent to AMG1-PC or ACG1-PC instead, for faults 4 and 5, respectively. The function \\SRAMP\" was applied to decrease the bias values to perform both referred faults.", "Each of the six faults and fault-free also had several runs, including a fouling e\u000bect, as if a heat exchanger had already a certain service period. To represent that, H3's overall heat transfer coe\u000ecient, or U-value, was reduced between 3% and 3.5% at the very beginning of the simulation. Several tests showed that this range already stimulated di\u000berent reactions in the process's behavior than without the fouling e\u000bect, generating fault regions even when only disturbances were applied. That is the only variable alteration done in a single step and not performed with \\SRAMP\".", "At the end of every run, the task script and output information are saved in text \fles, and the dynamic data are saved in spreadsheets. The fault regions are noted with timestamps for further addition of classes to the database. The generation of a fault region requires only one variable to leave the controllable region, but other variables may also leave, depending on the simulation. Therefore, the noted timestamp is the one when the \frst variable reaches the controllable limit. Table 4.15 displays the variables responsible for de\fning the beginning of the fault region of each fault and if this changes with the addition of fouling or not. Table 4.15: Variables generating the fault regions.", "It is possible to observe in Table 4.15 that the variables responsible for delimiting faults 1, 2, 4, 5 and 6 simulations' fault regions do not depend on the addition of the fouling e\u000bect in H3. In all fault 3 simulations, both C2SL and SW8T reach their controllable limits, but the one that achieves it \frst depends on the fouling addition. The fault-free simulations, as stated before, are composed of only small distur- bances in some process variables and perform without a fault region when there is no fouling e\u000bect. In the other cases, some parts of the simulation are in fault re- gions, and this was always observed in SW8T. Those timestamps were all considered part of the fault 1 class, once the general variables' behavior would be closer to this category.", "Although some variables may be the same to create the fault region in di\u000berent faults, the process's general dynamic behaviors are very di\u000berent, which makes them distinguishable by the AI algorithms applied afterwards.", "To exemplify some of the behaviors described in Table 4.15, Figures 4.15 to 4.18 show the pro\fles of SW8T, ACG1P, C2SL and SW5F in fault-free as well as in faulty simulations with and without the fouling e\u000bect addition. To each controller, the same scale was applied to all the corresponding plots to allow easier comparison. Figure 4.17 scale is expressed as a function of C2SL, the set point of this controller, and Figure 4.18 scale is written, as well as the other mass ows in this work, as a function of SW1F. This last controller's set point is 10% SW1F. Figure 4.15: SW8T dynamic behavior in a) fault-free, b) fault 1 without fouling and c) fault 1 with fouling simulation.", "Figure 4.16: ACG1P dynamic behavior in a) fault-free, b) fault 2 without fouling and c) fault 2 with fouling simulation.", "Figure 4.17: C2SL dynamic behavior in a) fault-free, b) fault 3 without fouling and c) fault 3 with fouling simulation.", "Figure 4.18: SW5F dynamic behavior in a) fault-free, b) fault 5 without fouling and c) fault 5 with fouling simulation.", "It is possible to observe that in SW8-TC the valve closes up to its lower limit, that is, 2% opened, and in all the other three examples, the valves open entirely, up to 100%. In SW8-TC, V5's closing leads to a stream temperature lower than the set point, which is expected once the controller's action is direct, as shown in Table Table 4.9 also shows that C2S-LC's and SW5-FC's actions are reverse and ACG1- PC's action is direct, what justi\fes di\u000berent behaviors despite the complete opening of the valves in fault regions. The level in C2's sump reduces until it dries, as shown in Figure 4.17, in many simulations. SW5's mass ow is also declined below the set point, while ACG1's pressure is increased.", "A general analysis of Figures 4.15 to 4.18 indicates that the fouling e\u000bect helps accelerating the fault region's generation. That is expected once fouling is a devia- tion from the ideal behavior of the process.", "In Fault 6 simulations, the e\u000bect observed in SW8-TC is the opposite of the one displayed in Figure 4.15. V5 opens completely and, from this moment on, the temperature of the stream keeps rising. Observing the dynamic data, it was also possible to infer that the attempt to simulate the \\column boil up\" phenomenon as Fault 6 was not completely successful once the software limited it. Figure 4.19 showsH2SandNH 3recuperations in ACG1 in a Fault 6 simulation, without the fouling e\u000bect.", "Figure 4.19: Fault 6 dynamic behavior of H2SandNH 3recuperations. An increase of about 25\u000eC in SW1 temperature led to around 6\u000eC to 10\u000eC of ris- ing in ACG temperature. According to Figure 4.19, H2Srecuperation dropped from about 90% to 70%; meanwhile, NH 3recuperation almost doubled. Although this is expected in this type of situation, the column did not reach its operational point limit, where the streams almost entirely vaporize. However, this fault was main- tained in the database and in future evaluations, as these recuperation values would already represent a fault, despite not being so representative of the phenomenon itself.", "4.6 Data Processing", "Once all raw data are already collected and saved, several steps of data processing begin. The following owchart in Figure 4.20 expresses those steps.", "Figure 4.20: Data processing owchart.", "The \frst operation is to add noise to the simulated data, aiming to make them more representative to \feld-collected data. The strategy was based on FISHER ROSEMOUNT SYSTEMS (2010) that suggests generating zero-mean random noise with maximum amplitude between 0.2% and 0.5% of the standardized range of the process data. Instead of using the range of the data, which may vary too much in certain fault regions, the maximum noise amplitude was based on each variable's set point. Equation 4.2 shows the calculation of the noise vector as a zero-mean normal distribution with a maximum amplitude of Sigma times the set point of each process variable.", "Noisei=N(0;Sigma\u0003SPi) ( 4.2) After several trials, even 0.2% of the set point was found to be too high for the noise's amplitude, as it started to mischaracterize the process's data. Therefore, the amplitude was \fxed as 0.1% of the set point. This analysis was made by visual inspection of the plots showing each controller's raw and noisy data. This same script also saves process variables after noise addition in a text \fle. As an example, Figure 4.21 shows the same controller's dynamics in the same simulation, but with a di\u000berent amplitude of noise added. For easier comparison, the same scale was used. The drum level in C2D-LC is de\fned as a function of C2DL, this controller's set point, and the mass ow returning to the re ux drum is de\fned as a function of SW1F, previously described.", "Using a higher Sigma value leads to a mischaracterization of the data as it is harder to perceive the peaks formed on the C2's re ux drum level. This might mask the process's dynamic behavior so that the dataset would lose information that might be important for the FDD study.", "Another evaluation of the adequate amount of noise to be added was made based on the signal-to-noise ratio (SNR). As stated by POE and MOKHATAB (2016), it is recommended that any change in a process variable value be at least ten times larger than the noise generated, meaning that SNR should be no less than 10 in dynamic data, and that was veri\fed to remain valid.", "Figure 4.21: C2DL dynamic behavior with a) Sigma = 0.003 and b) Sigma = 0.001 amplitude of noise addition.", "The next step is to add the classes to each row of the data, using the base simulations observed behavior. This was made using spreadsheets and text \fles, naming fault-free as class 0, and each fault with its already designated number as stated in Table 4.14.", "Those data are input in a script responsible for adding delay to the data, based on forwarding modeling. This procedure aims to represent the process's forwards dynamic, using present and past inputs to predict the outputs (RAMLI, 2018). Typically, in this method, past outputs are also used as inputs to help output predictions. This work uses a classi\fcation strategy instead of a prediction one, so the outputs are classes instead of process variables. Therefore, only past and present inputs are feed data, while the classes remain as outputs. The amount of time delays necessary to represent process' dynamics accurately depends on the data behavior and will be discussed further in Chapter 5. Dynamic principal component analysis (DPCA) will be used to determine, based on static and dynamic relations between variables, how many time delays will be applied to the developed database (KU et al. , 1995).", "Once all data had already been treated, the next step is to transform them into two databases, train and test, by concatenating the data from multiple simulation runs. The \frst principle is to avoid leaking information between the databases, pos- sibly bias to the AI techniques. Thus, the databases were composed by independent simulation runs so that all the data from each run go to the same database. The second principle was that all the classes must be present in both databases, including simulations with and without the fouling e\u000bect for each class, so that training is e\u000ecient to evaluate the test dataset. The third principle is class distribu- tion throughout the databases. As the classi\fcation performed in this work is based on faults in a chemical process, it is expected that most of the data are related to normal operation. The process should perform as expected most of the time so that a smaller amount embodies the faulty data.", "In non-stationary systems, as real industrial data, the distributions might be shifted from the training data distribution, making predictions and classi\fcations more complicated as accuracy tends to decrease in those cases (MORENO-TORRES et al. , 2012; OVADIA et al. , 2019; QUI ~NONERO-CANDELA et al. , 2009). Histor- ical data available might not have the same behavior as new data collected online. Most AI techniques presume that train and test data domains match or that match- ing is not crucial for the evaluation (QUI ~NONERO-CANDELA et al. , 2009). Prior probability shift is related to modi\fcations in the distribution of the output class variable (MORENO-TORRES et al. , 2012). That is the e\u000bect to be avoided to assure the validation of the performed classi\fcation. Therefore, the databases developed in this work are intended to have similar class distributions, although not the same, and mostly composed of fault-free data.", "As both databases are composed of the fourteen already described controlled process variables in Table 4.15, one of the last steps is to remove the column with C2SL2 data, as it presents the same raw data of C2SL. This would give the same variable, C2's sump level, double weight as any other variable. The \fnal databases inputs are based on the 13 process variables and one output { the class of the data. Those process variables are: SW1F, SW5F, CW5F, CW8F, SW8T, C1S2T, AMG1T, C1SL, C2DL, C2SL, ACG1P, AMG1P, C2HDI.", "With the databases already built, the data were standardized to make the AI techniques training process more manageable and less biased, once di\u000berent ranges make the data much more heterogeneous (CHOLLET, 2017). Those databases are then the inputs to the various AI algorithms.", "Chapter 5 Principal Component Analysis", "5.1 Introduction", "PCA is a method that reduces the dimensions of a dataset, creating linear combi- nations of the original variables, called principal components. The goal is to facilitate data management and interpretation, meanwhile not losing important information. A dynamic application of PCA is useful to determine if time delay additions are necessary for a database and how many should be included to express a phenomena' dynamic behavior correctly.", "In this Chapter, after a literature review about PCA and its dynamic application, the case study section shows the results obtained from the database analysis. The amount of time delays chosen to be added is discussed, along with the variance explained by the principal components and the database's behavior as a function of those components. The most important variables based on this analysis are selected, discussed and will be compared in the next Chapters.", "5.2 Literature Review", "5.2.1 Basic Concepts", "PCA is a mathematical technique that decreases the number of variables, or fea- tures, in a dataset. The main objective is to reduce its dimensions while maximizing the retained information. New variables are created, or principal components (PCs), that describe the problem as linear combinations of the original features. This fa- cilitates statical analysis and feature interpretations, especially in large datasets (JOLLIFFE and CADIMA, 2016).", "The concepts involving PCA were \frst de\fned by PEARSON (1901) over 120 years ago and were afterwards developed by HOTELLING (1933) almost 90 years ago. Only some decades after those works were published, it was possible to ap- ply PCA to datasets once the technological developments and software made that possible (JOLLIFFE, 2005).", "5.2.2 Basic Method", "The method is showed ahead, as described in JOLLIFFE and CADIMA (2016). Considering a dataset as an nxpmatrixZ, withnobservations and pvariables, the goal is to perform a linear combination of the pcolumns of this matrix, retaining maximum variance. Usually, the dataset is normalized by column to reduce the in uence of units in the analysis, replacing the matrix Zfor a matrix Xof the same size.", "The linear combinations can be calculated by Equation 5.1, where xjare the corresponding vectors of the columns of Xandais a vector of constants: pX Equation 5.2 describes the variance of any linear combination, where Sis the covariance matrix of the dataset:", "The objective is to \fnd the vector athat maximizes var2(Xa), considering it unit-norm vector, so a0a= 1. This is the same as maximizing a0Sa{\u0015(a0a{1), where \u0015is a Lagrange Multiplier. Equating the derivative of this expression in relation to the vectorato the null vector, it is possible to write Equation 5.3: Whereais an eigenvector and \u0015is an eigenvalue of S. The variances of the linear combinations are the \u0015values, once Equation 5.4 is valid: Shaspreal eigenvalues, \u0015k(k= 1;:::;p ), and the eigenvectors can be an or- thonormal set, as a0 kak0= 1, ifk=k0and zero if k6=k0. The eigenvectors of Sare the solutions for obtaining up to plinear combinations, as described in Equation Xak=pX Those linear combinations successively maximize the variance once they are un- correlated to the linear combinations previously calculated, since the covariance between two linear combinations, XakandXak0is obtained by Equation 5.6, if a0 k0Sak=\u0015ka0 Xakare the linear combinations de\fned as the principal components of the dataset, and their elements are de\fned as PC scores, meanwhile the elements of ak, the eigenvectors, are de\fned as PC loadings.", "PCs might be de\fned based on the centred variables x\u0003 j, with mean \u0016 xjand genereic element x\u0003 ij=xij\u0000\u0016xjof the feature j, without changing the Smatrix.", "X\u0003is de\fned as the nxpmatrix that has the centred variables x\u0003 jin the columns. Equation 5.7 connects the singular value decomposition (SVD) of X\u0003 with the eigendecomposition of S:", "Considering Uas annxrmatrix and Aas anpxrmatrix, both with or- thonormal columns, and Las anrxrdiagonal matrix, it is possible to de\fne any Ymatrix of dimension nxpand rankras in Equation 5.8, if r\u0014minfn;pg: The columns of AandUare called, respectively, the right and left singular vec- tors ofY. They are also the eigenvectors of the matrices Y0YandYY0, respectively, of dimensions pxpandnxn, associated with its non-null eigenvalues. The singular values of Yare the elements of the diagonal of matrix L. They are the non-negative square roots of the non-null eigenvalues of matrices Y0YandYY0. Those elements de\fne the order of UandAcolumns' once is assumed that they are in decreasing order.", "De\fningY=X\u0003, the matrix product by Ais given as follows by Equation 5.9: MatrixAare the vectors akof PC loadings and ULare the PCs of X\u0003. The variances of the PCs can be calculated by the squares of the singular values of X\u0003 divided by ( n\u00001).", "Combining the de\fnitions of Equations 5.7 and 5.8, it is possible to obtain the spectral decomposition, or eigendecomposition, of ( n\u00001)S, whereL2is the diagonal matrix with the eigenvalues of ( n\u00001)S. This is shown in Equation 5.10: Therefore, an SVD of X\u0003is equivalent to the PCA.", "Considering a matrix Yqof sizenxpand rankq < r composed of the elements inYthat minimize the sum of squared di\u000berences, Equation 5.8 applies. Lqis the qxqdiagonal matrix composed of the \frst highest qdiagonal elements of L.Uq andAqare theqcorresponding columns of UandA, therefore, their sizes are nxq andpxq.", "This creates a q-dimensional subspace of Rpwithnpoints, composed of the rows ofX\u0003 q. That subspace is composed by the n-point approximation that minimizes the sum of squared distances between corresponding points and can be de\fned as a principal subspace formed by the \frst qPCs. Therefore, the overall strategy is a dimension reduction from poriginal features to qPCs and can be utilized to visualize the dataset by creating 2-D or 3-D plots, applying the two or three most relevant principal components as axes.", "The trace of the Smatrix is the sum of variances of the poriginal variables and also of all the pPCs. Therefore, the quality of a principal component can be evaluated as the proportion of the total variance that it can describe. Equation 5.11 describes this calculation, where tr(S) is the trace of S.", "\u0019j=\u0015jPp A group of NPCs, generally the \frst q, can be expressed as a percentage of the total variance, as the expression shown in Equation 5.12:", "X j2N\u0015jPp Usually, the percentage of the total variance is previously de\fned, typically 70% or more. However, only the PCs relevant to creating plots are applied, regardless of the total variance they represent.", "As described in the mathematical steps above, PCA not only is a method capable of reducing the dimensions of a dataset, but it is also very useful to visualize the data and understand how the original features are relevant to describe the the studied system variance. However, this is a linear approach and may not represent the data's dynamic behaviour if they are time series due to possible dependencies between observations and variables (JOLLIFFE, 2005). Some strategies have been developed to deal with this issue, as described in the next section.", "5.2.3 Dynamic Application", "DPCA was proposed by KU et al. (1995), in which the analysis is performed in an already normalized database containing data from at least two di\u000berent timestamps, that is, including to the data matrix, X = [X(t)], the data of X(t-1) so that X = [X(t), X(t-1)]. Figure 5.1 summarizes this operation for a one-time delay, in which PVs are the process variables and t is any timestamp but the last one. Figure 5.1: One-time delay addition.", "That technique is ideal to evaluate the present work once chemical processes are dynamic and their behavior in time matters, as already discussed in the previous Chapter, not only regarding forward modeling but also about prior probability shift in non-stationary systems.", "DPCA is essential to perceive how many time delays are necessary to describe the system's dynamics without losing information about the process's behavior. Applying PCA in databases with no delay, that is, a static database, and also with di\u000berent time delays, it is possible to calculate the static and dynamic relations between the problem variables. The number of necessary time delays indicates the dynamics' order and, according to KU et al. (1995), is generally 1 or 2. The static relations can be obtained through PCA application in a database without delay, or X = [X(t)], by subtracting the calculated number of principal components from the total number of variables. When a one-time delay database is analyzed, like the one shown in Figure 5.1, it is expected that the static relations are doubled. For example, if PV1(t) and PV3(t) are related, then PV1(t-1) and PV3(t- 1) will be connected as well so that the same relation will be accounted twice. Therefore, to calculate the \frst-order dynamic relations in a one-time delay dataset, not only the number of principal components is subtracted from the number of variables, but also two times the number of static relations previously calculated. The result represents the relations between variables in di\u000berent timestamps, for instance, PV1(t) and PV4(t-1).", "The next step is to calculate the principal components necessary for a two-time delay database, as in X = [X(t), X(t-1), X(t-2)]. Analogously to what was just explained, the static relations will be tripled, and the \frst-order dynamic relations will be doubled. Following the same examples, not only PV1(t) and PV3(t) will be related, but also PV1(t-1) and PV3(t-1), as well as PV1(t-2) and PV3(t-2). In the dynamic relations' case, if PV1(t) and PV4(t-1) are related, PV1(t-1) and PV4(t-2) will also be related.", "Subtracting the number of principal components, the triple of the static relations, and the double of the \frst-order dynamic relations from the number of variables, the resulting number are the relations between variables that are two timestamps away, as if PV2(t) would be related to PV5(t-2). If that number is zero, no second-order dynamic relations were detected by the PCA. This second delay inclusion is not necessary; hence it is not adding new information to the database. In that case, a one-time delay is enough and should be applied. Figure 5.2 summarizes the steps described above and how those calculations are made.", "Figure 5.2: Static and dynamic relations calculated by DPCA (based upon SALVA-", "5.3 Case Study", "To determine how many time delays should be used to generate the database, the DPCA was applied to evaluate the static and dynamic relations between the process variables, as described in Figure 5.2. One-time and two-time delays databases were generated, as well as a static one, without delay. Those three databases were input independently to a PCA algorithm to calculate the number of principal components necessary to explain 95% of the problem's variance. Table 5.1 displays the results of the DPCA analysis and the number of static and dynamic relations calculated. Table 5.1: Static and Dynamic relations results.", "It was possible to perceive two static relations and seven \frst-order dynamic relations between the variables. As expected, at least a one-time delay is necessary to describe the process's behavior once it is dynamic. However, no second-order dynamic relations were detected; hence, it is unnecessary to add the two-time delay data. Therefore, the one-time delay database was chosen for all further analysis and technique applications. After delay addition, the training and test datasets sizes and distributions were adjusted and are displayed in Table 5.2. Table 5.2: Datasets sizes and distribution.", "It can be inferred by Table 5.2 that the total amount of data includes 60,044 observations divided into 62.49% to perform the training and 37.51% to test the model. Those datasets have both 27 columns, containing the 13 original variables in two di\u000berent timestamps and the class of the data.", "The datasets have similar distributions, both composed by more than 50% of normal operation. Faults 1 and 2 are the most frequent, composing about 23% of each subset's data. The other faults have similar distributions, each composing between 2% and 6% of each dataset, representing together between 11% and 18% of the total.", "As already debated in the previous Chapter, the datasets creation in this work is intended to have alike distributions, but not exactly the same, as this e\u000bect would not be realistic. They are also expected to contemplate mostly fault-free situa- tions. This should avoid the prior probability shift problem described previously. Observing Table 5.2, it is possible to assume that the created datasets ful\fll those requirements and can be used to represent relatively realistic situations. Both stan- dardized databases are available in the repository linked on Appendix B. The variance explained by the principal components was calculated to evaluate those data and is shown in Figure 5.3. As shown in Table 5.1, \ffteen principal components were calculated for the one-time delay database.", "Figure 5.3: Variance explained by the principal components.", "As expected, the variance explained by the principal components diminishes, varying from 20% to 1.5%, adding up to a total of 95.3%. There is a major drop between the three most important principal components, and their total explained variance is 43.3%.", "To visualize the database, Figure 5.4 shows 2-D and 3-D plots using the three most important principal components as the axis.", "Figure 5.4: Database PCA plots in a) 2-D and b) 3-D.", "The graphics indicate that the fault-free data are a central cluster, centered in (0, 0) in 2-D and (0, 0, 0) in 3-D. This is expected once they are the default data, not deviating from the normal conditions and variables' standard values. As the data were normalized before the analysis, they will be closer to zero if there are only light disturbances in the process SW that the variables do not have large variations. The six types of faulty data are visually clustered separately, so it is possible to di\u000berentiate the classes. Each group extends from the center of the space towards a particular direction, and that is due to the variables that compose each principal component deviating from its SWTU point value in the simulations. To analyze variables' importance, Figure 5.5 shows the absolute loading coe\u000e- cients values of the variables composing each of the three most important principal components, used to display the database in Figure 5.4.", "Each of the variables that compose the principal components reported in Figure 5.5 is accompanied by its delay. Therefore, not accounting the repetitions between timestamps or di\u000berent principal components, the most important variables to de- scribe the database are C2SL, SW1F, SW8T, AMG1T, C1S2T, C2DL, C2HDI and C2DL.", "Those variables are distributed throughout the process, as SW1F and SW8T are before the \frst column, C1S2T is in the \frst column, almost all the other variables are measured in the second column, and AMG1T is right after this equipment. The most signi\fcant variables will be compared with the ones determined by other methods, like RF and SOM, since the original dataset and features will be applied in the following Chapters. All the results presented in this section took around 30 seconds to be obtained, including loading the databases with di\u000berent time delays and the graphics generation.", "Figure 5.5: Loading coe\u000ecients for the three most important principal components. Chapter 6 Random Forest", "6.1 Introduction", "RF is an AI technique introduced in the literature 20 years ago. It can be de\fned as a set of decision trees and is used for prediction and classi\fcation. One of its applications is dataset optimization through variable importance calculation. The least important features may be removed from the original data, enhancing e\u000eciency.", "This Chapter addresses a literature review about RF, their origin, basic de\fni- tions and advantages, followed by a discussion on the case study con\fgurations and results obtained. They are displayed in three sections, starting with hyperparameter optimization and continuing to learning and classi\fcation evaluation. The last topic of the section discusses variable importance and compares the one determined by this algorithm with the one described in the previous Chapter.", "6.2 Literature Review", "6.2.1 Basic Concepts", "RF is a ML algorithm presented by BREIMAN (2001) 20 years ago, composed of a group of estimators, also called decision trees (SPEISER et al. , 2019). A sample subset is drawn with replacement to generate the trees. This means that a bootstrap sample is used, which is a bagging approach that may select multiple or none times a particular sample (BELGIU and DR \u0015AGUT \u0018 , 2016).", "The same author who introduced the RF concept proposed the concepts of de- cision trees and bagging (BREIMAN et al. , 1984; BREIMAN, 1996) a few years before. They will be brie y described ahead for a better understanding of RF.", "6.2.2 Decision Trees", "Classi\fcation and regression trees (CART) are algorithms that use the dataset features to separate the samples. The decision trees grow through successive binary splits, creating nodes, ending up in leaves when there is no more splitting. There is a hierarchy in those splits, which can be measured through entropy or impurity. CART can be applied in classi\fcation and regression, performing quantitative and qualitative predictions (BREIMAN et al. , 1984).", "Figure 6.1 illustrates an example of a decision tree applied to the classi\fcation of a dataset of four classes and three features. The black and green circles represent the nodes and leaves, respectively.", "Figure 6.1: Classi\fcation decision tree example.", "A simpli\fed description of CART mathematical steps as de\fned in MORAIS (2017), based on BREIMAN (2001), is presented ahead. The classi\fcation decision tree can be de\fned as Equation 6.1:", "^y=f(v) =MX Whereyis the predicted class, v= (v1;:::;vp) are thepvariables, and the M regions generated by splitting are R1;R2;:::;Rm. The indicator function is I() and returns only 0 or 1 depending on the ful\flment of conditions being false ortrue, respectively. The most frequent category kin a region mis de\fned as ^ cm, shown in Equation 6.2:", "^cm= arg max kX The dataset is used to determine the variable jin charge of the splitting condition and the threshold s. This generates two regions, shown in Equation 6.3, where i indicates the partition level, and by solving Equation 6.4, jandsare determined: Ri 1(j;s) =fvjvj\u0014sgand Ri min j;s[min c1X vi2Ri c2X vi2Ri Qm(yi;c) is impurity, or uniformity, of the results and can be calculated in di\u000ber- ent ways. One of them is the Gini impurity, which can be de\fned as the normalized total depletion provoked by a variable in a particular class, as shown in Equation 6.5. Considering a dataset of Kcategories, the proportion of a class kinRmis ^pmk, and is used to calculate the probability of each class, between 0 and 1, as in Equation 6.6:", "Qm(yi;c) =KX ^pmk=1 nmX vi2Rp Along with those equations, a stopping criterion is necessary to avoid over\ftting", "6.2.3 Bagging", "Bagging comes from bootstrap aggregating and combines both methods. Aggre- gation integrates multiple predictors, like CART, for instance. In bootstrap sam- pling, the chosen samples are drawn with reposition. That is, as the samples are not removed after selected, they can be chosen various times or none (MORAIS, 2017). It is possible to use only part of the training set to train the estimators and use the rest of it to perform an internal cross-validation, known as out-of-bag (OOB) samples. Typically, the OOB samples are a third of the total, contemplating those not chosen in the bootstrap sampling (BELGIU and DR \u0015AGUT \u0018 , 2016).", "6.2.4 Random Forest", "As de\fned in this section's \frst topic, RF uses di\u000berent bootstrap samples to build each tree, that is, di\u000berent subsets of the dataset. In BREIMAN (2001), it is de\fned that, after training, each tree has a vote, and the class is decided through the forest voting winner.", "Considering, as an example, a RF classi\fer with four trees and a dataset with multiple classes, this system is described in Figure 6.2. As well as in Figure 6.1, the black and green circles represent the nodes and leaves, respectively. The blue circles and arrows represent the travelled path of the samples into classi\fcation. Figure 6.2: RF classi\fer example with four trees and a dataset with multiple classes. However, scikit-learn library documentation explains that an adaptation is made so that each tree has a probabilistic prediction instead of a single vote. The average of those tree's probabilistic predictions determines the predicted class. This adapta- tion intends to reduce the estimator's variance and over\ftting, as some errors might cancel each other.", "The sampling in the present work was made applying the library functions default mode. That is, bootstrap sampling is performed, but the OOB samples are not separated for executing the internal cross-validation.", "Nevertheless, the model is expected to not heavily over\ft once the number of trees might be huge without an issue (BELGIU and DR \u0015AGUT \u0018 , 2016). This is mathematically proven in BREIMAN (2001), using the strong law of large numbers (SLLN), showing that as the number of trees rises, the generalization error reaches a limit.", "RF typically presents higher accuracy than a single decision tree once variance is reduced. They also commonly have one of the highest accuracies when compared to other classi\fcation strategies. Another advantage of this method is the capability to deal with datasets with many features, mainly due to variable importance analysis (SPEISER et al. , 2019).", "6.2.5 Variable Importance", "Variable importance (VI) is a RF functionality able to improving model e\u000eciency by selecting the most relevant variables to classi\fcation performance. The dataset of interest may be reduced and optimized, removing the least important features, simplifying the model (BELGIU and DR \u0015AGUT \u0018 , 2016; SPEISER et al. , 2019). There are two common ways to calculate VI, Mean Decrease in Gini (MDG) and Mean Decrease in Accuracy (MDA) (BELGIU and DR \u0015AGUT \u0018 , 2016). Despite the last being more frequent, the \frst is the default method applied by the scikit-learn functions. MDG analyses the reduction in Gini impurity, already de\fned in this Chapter. More signi\fcant VI is related to higher impurity.", "6.3 Case Study", "6.3.1 Hyperparameter Optimization", "As described in Chapter 3, RF was applied integrated to a hyperparameter op- timizer, Optuna. This is also valid for all the other AI techniques discussed in the following Chapters.", "In RF, the objective function was de\fned to maximize the mean of the accu- racy scores returned by cross-validation, using the default value of 5 folds, which is adequate to the amount of data available. The training data is automatically divided in 5 subsets of the same size or similar sizes if the number of observations is not divisible by the number of folds. Subsequently, their accuracy is evaluated separately by the forest, returning the mean value. The chosen hyperparameters to be optimized were the number of estimators ( rfnestimators ), or trees in the forest, the maximum depth of the trees ( rfmax depth ) and the minimum number of samples necessary to split a node ( rfmin samples leaf).", "The default rfmax depth is none, which means that the nodes will keep expand- ing until all the node samples have the same label, what is usually called pure leaves, or until all the leaves have fewer samples than the rfmin samples leaf. Therefore, one hyperparameter might be a function of the other. The rfmin samples leafde- fault value is 2, and the default rfnestimators is a 100, as described in the library documentation.", "The lower and upper bounds that de\fned the optimizer's search region were chosen after a couple of trials on di\u000berent search ranges. The rfnestimators range was reduced until becoming the lower possible, between only 1 tree and the default value, 100 trees. The accuracy and quality of the results remained practically the same throughout this reduction. Other chosen lower and upper bounds were between 5 and 30 for rfmax depth and 1 and 10 for rfmin samples leaf. The number of trials chosen for each technique depended on the amount of hyperparameters to be optimized; the more hyperparameters, the more trials were made. In RF, those 3 hyperparameters were optimized using 40 trials. Figures 6.3 to 6.5 show the hyperparameter optimization results. The slice plot illustrates how each hyperparameter individually a\u000bects the objective value. The parallel coordinate plot represents how the hyperparameters' association a\u000bects the objective value and the optimization history exhibits how the objective value varied throughout the trials.", "Figure 6.3: Slice plot of RF hyperparameter optimization.", "Figures 6.3 and 6.4 show that higher values of rfnestimators and rfmax depth increase the objective value; meanwhile, rfmin samples leafhas better results closer to the lower bound. Once larger forests with bigger trees should be more capable of dealing with the data, this is expected. However, this e\u000bect is not unlimited, as it may lead to over\ftting the data. The trees' size is more prone to cause an issue than the number of trees, as explained in the previous section, due to the SLLN. However, Figure 6.5 shows that all the trials reached over 91.50% of accuracy. The optimized hyperparameters in RF were 97 rfnestimators , 30 rfmax depth and 3 rfmin samples leaf. Although the rfmax depth optimized value reached the upper bound, this avoids the tree to be deeper than necessary and over\ftting, as already explained. The objective value related to those hyperparameters, that is, the mean of the accuracy scores was 96.13%.", "Figure 6.4: Parallel coordinate plot of RF hyperparameter optimization. Figure 6.5: Optimization history plot of RF hyperparameter optimization.", "6.3.2 Classi\fcation Results", "After optimization, the model was re-evaluated through the cross validation scores to analyze the results' reproducibility. The accuracy scores were displayed in a boxplot, Figure 6.6, to facilitate interpretation. The mean and standard deviations Figure 6.6: Boxplot of RF accuracy training scores.", "The small standard deviation value and Figure 6.6 illustrate that the results can be considered reproducible, varying only between 95.87% and 96.27%. This lowest value was considered an outlier once all the other four accuracy scores were much closer between themselves.", "As described in Chapter 3, the learning curve was also used to evaluate the training. Di\u000berently of the cross-validation analysis, where the dataset is divided in subsets of the same size, in this function, the training data is split into subsets of di\u000berent sizes, cumulatively. Then, cross-validation is applied in each of those subsets, as shown in Figure 6.7.", "If one of the curves reaches 100% accuracy, the model is de\fnitely over\ftted to the training dataset. This is hugely undesired, as the model is too adherent to the training set, and will have di\u000eculties evaluating a di\u000berent set of data. The curves displayed in Figure 6.7 do not reach 100% accuracy, so the model is expected to be well adjusted.", "Figure 6.7: Learning curve of RF accuracy training scores.", "Once the model is trained, adjusted and analyzed, the test was performed to verify the classi\fcation quality. That is, if the model is capable of predicting the fault-free and faulty situations. Table 6.1 displays the classi\fcation report obtained. Table 6.1: Classi\fcation report of the RF test data analysis. In faulty classes, precision is higher or very similar to recall; meanwhile, the opposite happened to fault-free data. This means, analyzing the de\fnition of those metrics, that in faulty classes the misclassi\fed data are mostly false negatives instead of false positives. Therefore, the mistakes involve missing the correct faults and not over perceiving them.", "This is undesirable, as in a real industrial plant it is much more dangerous not to notice a fault that could compromise the process, than having a false alarm. False negatives are more concerning than false positives in faulty classes for safety and economic reasons.", "Considering that the opposite e\u000bect may be observed in fault-free, it can be assumed that the missing data in the faulty classi\fcation were considered by the RF mostly as fault-free. The di\u000eculty faced by the algorithm seems to be related to fault detection and not to fault diagnosis. When the fault is detected, it is classi\fed most of the time correctly. This can be veri\fed by an inspection of the confusion matrix, displayed in Figure 6.8.", "Figure 6.8: Confusion matrix of the RF test data analysis.", "The confused data between the faulty classes were smaller than 1%, which con- \frms the presented hypothesis. Considering those observations and an analysis of the \frst column of the confusion matrix, it is possible to infer that the confusion might be related to a pre-fault region.", "In each simulation there is a transition moment, very di\u000ecult to detect and delimit, leading the process from normal operation to a fault. The transition behav- ior probably is very confusing to the AI algorithms, and those data end up being classi\fed as fault-free. This class is most likely the most heterogeneous one, as it embraces all the disturbances and probably also the pre-fault regions. The overall classi\fcation accuracy was 93.44%; that is, the percentage of the data that were classi\fed correctly by the RF. Therefore, it can be assumed that the classi\fcation was performed e\u000eciently, despite this problem.", "6.3.3 Variable Importance", "The RF algorithm is naturally capable of returning the importance of each vari- able to the executed classi\fcation. Figure 6.9 displays the VI in the test dataset classi\fcation.", "Figure 6.9: VI of the RF test data analysis.", "In most of the variables, the importance is very similar between the same variable of two di\u000berent sampling times. This is expected, as they should represent the same kind of information in the dataset.", "The most important variables are ACG1 pressure, SW5 mass ow, C2 sump level, SW8 temperature and SW1 mass ow, each with over 6% of importance.", "Considering that most of the faults were applied in SW1 and the fouling e\u000bect is related to SW8, those variables should be important in the classi\fcation. SW5 is a stream, as well as SW1 and SW8, before the columns treatments; meanwhile, ACG1 is related to the \frst column, and C2 sump is associated with the second column. The importance is spread throughout the process and the variable types, as this group includes mass ow, temperature, pressure and level controllers.", "To continue the analysis, the same optimized hyperparameters were used to create a new RF, to be trained and tested using a reduced dataset based on the most important variables. After some tests, it was established that all the variables with importance lower than 2% would not be included in the creation of the reduced database. The remaining variables had accumulated importance of 88.85%. VI was analyzed again after the dataset reduction and is displayed in Figure 6.10. Figure 6.10: VI of the RF reduced test data analysis.", "The ranking of the importance remained very similar to the one displayed in Figure 6.9, as expected. They varied from 3% to 14% each. The most important variables are ACG1P, SW5F, C2SL, SW8T, SW1F and AMG1P. Comparing this list to the one presented in the end of Chapter 5, as determined by PCA, the variables in common are C2SL, SW8T and SW1F. They are considered the most important until this moment and will also be compared to the ones determined by other algorithms in future Chapters.", "The classi\fcation report and confusion matrix were also calculated, considering the reduced database. They are displayed in Table 6.2 and Figure 6.11. The classi\fcations' precision remained the same or increased; meanwhile, most of the recall did not change. This indicates a reduction in false positives in most classes. The most signi\fcant di\u000berence between Tables 6.1 and 6.2 is fault 2 recall, as it increased signi\fcantly. That e\u000bect can also be observed comparing the confusion matrices, as the confusion between fault 2 and fault-free diminished. This led to higher overall accuracy, reaching 95.21%, and indicates that the re- moval of most of the variables did not decrease the classi\fcation quality, on the contrary. The subtracted variables were creating some kind of noise that disturbed the perception of the process behavior. Therefore, the variable reduction was bene- \fcial to RF application.", "Table 6.2: Classi\fcation report of the RF reduced test data analysis. Figure 6.11: Confusion matrix of the RF reduced test data analysis. As already described in Chapter 3, the full computing time accounts for the entire script's processing, including loading the data, optimizing the hyperparameters, training and testing the technique, and generating graphic results. In this case, it also includes training and testing the RF again using only the reduced dataset based on VI. The full computing time was around 1 hour and the reduced computing time, considering only training and testing time, was about 40 seconds. Chapter 7 K-Nearest Neighbors", "7.1 Introduction", "KNN is an AI method used mainly for classi\fcation. It is a lazy1learning technique, labeling new data based on the memorized ones from training. The number of neighbors, the distance measure and the search algorithm are the most important variables in the method. It was published 30 years ago and is still relevant. This Chapter addresses a brief literature review about the method, including its origin and de\fnitions, along with the results obtained. They are divided between the hyperparameter optimization and the classi\fcation results, compared with the RF outcomes.", "7.2 Literature Review", "7.2.1 Basic Concepts", "The KNN is a ML algorithm known to be a lazy learner, as its training mainly consists of recording the dataset (TAUNK et al. , 2019). It was presented by AHA et al. (1991) 30 years ago in a paper about instance-based learning. This learning type is based on the comparison between new data and the memorized ones, not performing generalizations (TAUNK et al. , 2019).", "Although this method may be applied for prediction and classi\fcation, the latter is more frequent. Therefore, the technique involves determining the class of unknown data based on the classes and features of the nearest data recorded in training (TAUNK et al. , 2019).", "1Lazy and eager are the typical kinds of supervised learning. The \frst needs to keep the training samples stored; meanwhile the second only needs them to build a hypothesys, and then discards the data (FERRERO, 2009).", "The class of the studied data is decided as the most frequent among those neigh- bors. To perform this analysis, the knumber of nearest neighbors has to be chosen. This is one of the most relevant hyperparameters of the KNN technique (TAUNK Figure 7.1 illustrates the importance of k's value in classi\fcation performance. For example, considering a two-class dataset, an unlabeled new sample will be clas- si\fed using KNN with di\u000berent kvalues. The data are displayed in 2-D plots with two random features as the axis to better visualization.", "Figure 7.1: KNN algorithm example a) before application, and using b) k= 1, c) k= 4 and d) k= 9 (based upon FERRERO (2009); TAUNK et al. (2019)). Observing Figure 7.1, it is possible to perceive the in uence of the number of neighbors used to perform the classi\fcation. In this example, if k= 1, the unlabeled data will be classi\fed as a class A sample. However, if this number increases and k= 4 ork= 9, the unknown sample will be labeled as class B, once the majority of theknearest neighbors are class B.", "Another important con\fguration is how the distance between the samples is determined, as this will de\fne which samples are close or far from each other. Therefore, which samples are the nearest neighbors depend on how this distance is calculated. This will be further discussed in the next section.", "7.2.2 Basic Method", "As described in FERRERO (2009), various proximity measures have been pro- posed in literature throughout the years. They measure the dissimilarity between samples, resulting in the distance between them.", "The conditions that turn a dissimilarity measure into a distance measure are described in EVERITT et al. (2011) and FERRERO (2009). They are the positivity, identity, symmetry and triangular inequality, as described by Equations 7.1 to 7.4: dij= 0; if and only if x i=xj (7.2) dij=dji (7.3) Wheredijis the distance between two samples xiandxj, andi,jandqare random samples' indexes.", "Analyzing the conditions, distance measures are always nonnegative, and two samples have a unique distance that is the same regardless of the referential being samplexiorxj. The triangle inequality is a classic Euclidian geometry proposal. According to EVERITT et al. (2011), the most common distance measure ap- plied between the samples is the Euclidian distance. This expression is a speci\fc case of the Minkowski distance, given by Equation 7.5:", "dij= pX k=1jxik\u0000xjkjr!1 r Wherepis the number of features, and kis any variable of the dataset. The parameterrcan assume numbers equal to or higher than 1, and the e\u000bect of it is shown in Figure 7.2.", "Figure 7.2: E\u000bect of rin Minkowski distance (based upon FERRERO (2009)). The Manhattan distance, also called City Block, and the Euclidian distance can be de\fned as the Minkowski distance when r= 1 andr= 2, respectively. They are shown in Equations 7.6 and 7.7:", "dij=pX k=1jxik\u0000xjkj (7.6) dij=vuutpX Scikit-learn's library default metric, according to the documentation, is the Minkowski distance with r= 2. Therefore, it is the Euclidian distance. This was the measure applied in the present work.", "Besides the number of neighbors and the distance measure, the search algorithm applied in the distance calculation performance is an important hyperparameter of KNN. As the datasets become larger, obtaining the distances between multiple samples becomes more computationally costly. This will be discussed in the next section, as it depends on the options available in the library functions.", "7.3 Case Study", "7.3.1 Hyperparameter Optimization", "As well as in the previous Chapter, Optuna was used to optimize KNN hyper- parameters. The objective function was de\fned in the same way, maximizing the mean of the accuracy scores obtained by cross-validation with 5 folds. The opti- mized hyperparameters were the applied search algorithm ( knn algorithm ) and the nearest neighbours' number ( knn nneighbors ).", "The default knn nneighbors is 5, and it was searched between 1 and 10 neighbors. The default knn algorithm is auto, which attempts to choose the best algorithm between the three available options: balltree,kdtreeand brute , all tested in the performed optimization.", "According to the library documentation, brute -force ( brute ) veri\fes the distances between every pair of samples in the dataset. As the number of data increases, brute -force becomes ine\u000ecient and impractical. The k-dimensional tree structure algorithm ( kdtree) was developed to deal with this problem, creating tree-based structures to group the data during the analysis. Therefore, not every single distance needs to be measured, as data from the same group are all close or distant to another group of data.", "However, as also described in the scikit-learn library documentation, kdtreeloses e\u000eciency when there are more than 20 features in the dataset. Therefore, the ball tree structure algorithm ( balltree) was created. This method also groups the data, but creating hyper-spheres instead of using Cartesian axes, like kdtree. Therefore, balltreeis considered more computationally demanding, although very e\u000ecient for datasets with numerous variables.", "To perform optimization, 30 trials were chosen, as this is the number of possible combinations between the three algorithm options and the number of neighbors. Figures 7.3 to 7.5 show the hyperparameter optimization results. As already de- scribed in Chapter 6, the slice plot illustrates the e\u000bect of each hyperparameter individually; meanwhile, the parallel coordinate plot displays the e\u000bect of the asso- ciation between them. The optimization history shows the objective value variation during the trials.", "Figure 7.3: Slice plot of KNN hyperparameter optimization.", "Figure 7.4: Parallel coordinate plot of KNN hyperparameter optimization. Figure 7.5: Optimization history plot of KNN hyperparameter optimization. The optimization graphics indicate that all the trials achieved over 91% of ac- curacy. The three tested knn algorithm can achieve an objective value higher than 93%, if the knn nneighbors is equal or bigger than 7. The best trial had 93.29% of accuracy, used 9 neighbors and the ball tree algorithm.", "7.3.2 Classi\fcation Results", "The boxplot analysis is applied to verify results' reproducibility in the model re- evaluation, shown in Figure 7.6. The cross-validation's mean and standard deviation accuracy scores were 93.29% and 0.21%.", "Figure 7.6: Boxplot of KNN accuracy training scores.", "The results were very reproducible, once they only varied between 92.98% and 93.55% and the standard deviation was slight. In comparison with the RF results, they were a little less accurate but also quite reproducible. The learning curve was performed in the same way as described in the previous Chapter to analyze the training. The dataset is divided in subsets of cumulative sizes, and cross-validation is applied to them. This is shown in Figure 7.7. Figure 7.7: Learning curve of KNN accuracy training scores.", "Both curves have similar accuracies, reaching a maximum of 94.25% in training and 93.40% in cross-validation when all the samples of the training set are used. The model is probably not over\ftted to the training data.", "It is also possible to notice that the shades around the curves are much more visible than in Figure 6.7, the RF learning curve plot. The shades represent the variation, that is, the standard deviation of the analysis. They are more signi\fcant than in RF application.", "The test data are now analyzed after hyperparameter optimization, model train- ing and evaluation. The classi\fcation report is displayed in Table 7.1 to verify the classi\fcation performance.", "Table 7.1: Classi\fcation report of the KNN test data analysis. Similarly, as observed in RF, the precision was smaller than the recall in fault- free data, and the contrary happened in the faulty data. Once again, this concludes that the fault classes have false negatives; meanwhile, fault-free data present false positives.", "The misclassi\fcation is caused by missing the faulty data and making mistakes in fault detection instead of fault diagnosis. The confusion matrix, shown in Figure 7.8, is helpful to continue this analysis.", "The confusion between the faulty classes was again smaller than 1%. The \frst column of the matrix indicates that most of the misclassi\fed data were faulty data confused with normal operation behavior, especially in fault 2. This reinforced the conclusion discussed in the previous Chapter about the existence of a pre-fault region that hampers the classi\fcation.", "The test dataset classi\fcation accuracy was 91.83%, a little smaller than the one observed in RF test analysis. This technique was also capable of performing an e\u000ecient classi\fcation.", "As commented in Chapter 6 and explained in Chapter 3, the full computing time accounts for the entire script's processing; meanwhile, the reduced computing time considers only training and testing. The full computing time was 1.2 hours and the reduced computing time was almost 2 minutes. Therefore, KNN was slower than Figure 7.8: Confusion matrix of the KNN test data analysis.", "Chapter 8 Support Vector Machines", "8.1 Introduction", "SVM is an AI method created to perform the classi\fcation of linearly separable datasets, published for the \frst time 16 years ago. The algorithm's objective is to create an optimal hyperplane with maximized margins to separate the classes. An extra dimension addition may be necessary if the data are not linearly separable in the original space. This technique is known as the kernel trick. In multi-class datasets, the classi\fcation can be performed by creating hyperplanes in one-versus- one or one-versus-all approaches.", "This Chapter provides a short literature review about the technique, with the- oretical, mathematical, and practical explanations on the algorithm's application. The results are divided between the hyperparameter optimization and the classi\f- cation results, compared with those presented in previous Chapters. In this case, the hyperparameter optimization is split into the application of four di\u000berent kernel functions and compared among themselves.", "8.2 Literature Review", "8.2.1 Basic Concepts", "SVM is an AI technique that can be used for prediction and classi\fcation, devel- oped with the main purpose of classifying groups of linearly separable data (IVAN- CIUC, 2007). It was published initially in 1995, but 4 years later, a more complete version of the original book, with three new Chapters about SVM, was released as The author had previously developed the concepts of statistical learning in VAP- NIK and CHERVONENKIS (1971), on which the ideas of SVM are based. This method intends to enhance the learning generalization, providing high accuracy even in groups of data not used in training (LORENA and CARVALHO, 2007). The classi\fcation performed by SVM is made through the calculation of an optimal hyperplane that separates the data, that is, a surface with p\u00001 dimensions in apdimensional space. Depending on the data's disposition in the dimensions, it may not be possible to e\u000eciently separate them with a hyperplane. In these cases, it is necessary to include an extra dimension to rearrange the data, turning them linearly separable (IVANCIUC, 2007; RHYS, 2020).", "The following sections will discuss the application of SVM in linearly separa- ble data, non-linearly separable data and in multi-class datasets, respectively. A mathematical approach will also be included.", "8.2.2 Linearly Separable Data", "As an example of the shape of linearly separable data, Figure 8.1 shows a group of data with two features and two classes. In this case, the hyperplane will be a straight line, as there are only two dimensions. The solid line indicates the hyperplane, and the dashed lines represent the margins. This symbology will be applied in the other \fgures of this literature review.", "Figure 8.1: Linearly separable data with two dimensions and two classes (based upon LORENA and CARVALHO (2007); RHYS (2020)).", "Many possible hyperplanes can correctly separate a particular group of data. Figure 8.2 shows some examples, using the same data displayed in Figure 8.1. Figure 8.2: Di\u000berent possible hyperplanes for linearly separable data with two di- mensions and two classes (based upon RHYS (2020)).", "To choose the optimal hyperplane to perform the classi\fcation, the algorithm maximizes the margin around itself (RHYS, 2020). The mathematical steps and de\fnitions to accomplish the choice of the hyperplane will be described as follows, according to LORENA and CARVALHO (2007). A hyperplane can be de\fned as Equation 8.1:", "WhereXrepresents, as usual, the normalized dataset matrix with nsamplesxi of classes\u00001 or +1, and wis the normal vector to the hyperplane. Therefore, w\u0001x is the dot product between those vectors. The distance between the hyperplane and (0, 0), in a two-dimensional case, is de\fned asb jjwjj. This hyperplane divides the space in two regions: above it and below it. A signal function is used to perform classi\fcation, as in Equation 8.2:", "An in\fnite amount of hyperplanes can be calculated through those equations. The canonical hyperplane is de\fned once wandbmake the nearest examples to the hyperplane satisfying Equation 8.3. This generates Equations 8.4, which can be summed up in Equation 8.5:", "Wherekiare the possible classes \u00001 and +1. The hyperplanes H1andH2are de\fned in Equations 8.6 and displayed in Figure 8.3:", "Figure 8.3: Hyperplanes H1andH2and the distance between them (based upon LORENA and CARVALHO (2007)).", "The datax1andx2are in the hyperplanes H1andH2, respectively, as shown in Figure 8.3. The distance dbetween the hyperplanes is calculated by the projection ofx1\u0000x2inw's direction, as shown in Equation 8.7:", "w jjwjj:(x1\u0000x2) jjx1\u0000x2jj!", "Those data that are in the margin, like x1andx2compose the support vectors, naming the technique as they are supporting the hyperplane localization (RHYS, 2020). The subtraction of the equations in Figure 8.3 leads to Equation 8.8. The substitution in Equation 8.7 results in Equation 8.9.", "d= jjwjjjjx1\u0000x2jj!", "And the norm of the vector is expressed as distance din Equation 8.10: Distancedis twice the margin of the optimal hyperplane. Therefore, to maximize the margin, it is necessary to minimize jjwjj. Usually, this is made as shown by Equation 8.11, with the constrain shown in Equation 8.5.", "min w;b1 This constraint prevents samples of the training data from being located between the margins, which is typically called a hard-margin SVM.", "The soft-margin SVM application de\fnes a di\u000berent constraint, in which a few data can be inside the margins. This is shown in Equation 8.12, where \u0018irepresent the slack variables:", "A new minimization can be de\fned in Equation 8.13, considering that when \u0018i\u00151 there is a misclassi\fcation, which should be minimized: min", "2jjwjj2+C nX", "WhereCis a regularization hyperparameter, also known as cost. It a\u000bects if the margin is harder or softer (LORENA and CARVALHO, 2007; RHYS, 2020). This will be further discussed in the next section when the hyperparameter optimization is presented.", "8.2.3 Non-Linearly Separable Data", "When the data are non-linearly separable, the original data are changed from its original space to a feature space with higher dimensions. The transformation \b should be capable of distributing the data in a linearly separable way. According to the Cover theorem, two conditions are necessary for this transformation to be successful in the linear separation of the data: the transformation must be non- linear, and the space dimension should be high enough (LORENA and CARVALHO, For example, a group of data with two features and two originally non-linearly separable classes becomes separable when transformed in a 3-D feature space. Equa- tions 8.14 and 8.15 show this transformation and the hyperplane: A visual representation of those data can be seen in Figure 8.4, where the data are non-linearly separable before the transformation. After it, the data becomes separable through a plane. The gray shape helps to visualize the 3-D distribution of the data.", "Figure 8.4: Transformation from the original space to the feature space (based upon LORENA and CARVALHO (2007); RHYS (2020)).", "Mathematically, according to LORENA and CARVALHO (2007), the transfor- mation relies mainly on dot products between the data in feature space. They are calculated using the kernel functions, which receive the original space's two samples and perform the dot product between them in the feature space. This is shown by Equation 8.16:", "Applying the example shown in Equation 8.14, the kernel is determined as given by Equation 8.17:", "Kernel (xi;xj) = (x2", "1i;p", "1j;p", "The most common kernels are Gaussian or RBF (Radial-Basis Function), Sig- moid and Polynomial. The linear kernel is the equivalent to no kernel or to a poly- nomial when the degree is equal to one (LORENA and CARVALHO, 2007; RHYS, 2020). Equations 8.18, 8.19 and 8.20 show the main kernel functions in order of appearance.", "Kernel (xi;xj) = exp(\u0000gammajjxi\u0000xjjj2)(8.18) Kernel (xi;xj) = tanh(gamma (xi:xj) +coef) (8.19) Kernel (xi;xj) = (gamma (xi:xj) +coef)degree(8.20) According to RHYS (2020) and to the scikit-learn library documentation, gamma is a hyperparameter corresponding to the in uence of each data in train- ing as a whole and in the optimal decision surface position. Also, degree is the degree of the polynomial in this kernel and coef is the independent term of the functions. Those hyperparameters, along with C, will be further discussed in the hyperparameter optimization section.", "Figure 8.5 shows the behavior of the decision surface of the main kernels ex- plained in this review. They are linear, Gaussian, sigmoid, and polynomial with degrees 2 and 3.", "Figure 8.5: Kernel functions behavior in classi\fcation with SVM (based upon RHYS", "8.2.4 Multi-class Data", "The two-class separation discussed until this point can be generalized for a dataset with a larger amount of classes. There are two common approaches for this issue: one-versus-all (OVA), or one-versus-rest, and one-versus-one (OVO) (HASTIE et al. , 2009). They are visually displayed in Figure 8.6, respectively, in a dataset with two features and three classes.", "Figure 8.6: Multi-class SVM approaches OVA and OVO (based upon RHYS (2020)). In an OVA approach, the classes are classi\fed to isolate each of them from all the others. In Figure 8.6, this is represented by the dashed lines in the same color of the classes they intend to isolate. In those cases, the SVM number is the same as the number of classes in the dataset (HASTIE et al. , 2009; RHYS, 2020). For example, in a three-class dataset, the classi\fcation will be performed between A and the B plus C, between B and the A plus C, and between C and the A plus B. Those three classi\fcations are made independently.", "When a test is made, that is, the algorithm classi\fes data without label, a \\winner takes all\" approach happens. Each sample is tested on both sides of each hyperplane, and will eventually \ft in a side that isolates one class. That sample will be labeled as a part of this class (RHYS, 2020).", "Conversely, in an OVO approach, the classi\fcation is performed in pairs, cre- ating a SVM for each pair (RHYS, 2020). For example, in a three-class dataset, it will occur between classes A and B, between classes A and C, and between B and C, independently. In Figure 8.6, this is represented by the double dashed lines representing the two colors of the two classes being separated. Usually, even when OVO needs a higher SVM number, this approach tends to be less computing costly (HASTIE et al. , 2009; RHYS, 2020). According to scikit-learn library documentation, the SVM algorithm is implemented in an OVO approach.", "8.3 Case Study", "8.3.1 Hyperparameter Optimization", "Once again, Optuna was applied to perform hyperparameter optimization. The same objective function was de\fned as already described in RF and KNN, maxi- mizing the accuracy scores' mean obtained by 5-fold cross-validation. Scikit-learn library has four kernels implemented: linear, polynomial, Gaussian and sigmoid. The default kernel of the function is the Gaussian one. However, as the four available kernels have di\u000berent hyperparameters, the optimization was made independently. That is, each kernel has its separate objective function. Table 8.1 shows the hyperparameters associated with each kernel.", "Table 8.1: Hyperparameters of each kernel in SVM.", "It can be observed that all of them have the hyperparameter C, and only the linear kernel does not have the gamma . The polynomial kernel is the only one to have thedegree .", "C, as explained in the previous section, is the regularization parameter. It is related to the misclassi\fcations' cost, in uencing in margin width and SVM hard- ness (LORENA and CARVALHO, 2007; RHYS, 2020). According to the library documentation, the regularization strength is proportional to C\u00001, and smaller C values are related to a smooth response surface. Meanwhile, more signi\fcant val- ues indicate a more complex response surface, generally resulting in more accurate classi\fcation. Its default value is 1 and it was searched between 0.1 and 10. Gamma can be de\fned as the kernel coe\u000ecient. As stated previously, it rep- resents the e\u000bect of each sample in the de\fnition of the classi\fcation boundaries. The higher the value, the more each sample a\u000bects the hyperplane. This can lead to over\ftting, as the decision surface may be too dependent on the samples, losing generalization capability (RHYS, 2020). It was searched among the library's two possible options: scale , the default, and auto. They are shown in Equations 8.21 and 8.22, respectively:", "gamma =1 gamma =1 WhereXandp, as before, are the normalized dataset matrix and the number of variables, respectively. Figure 8.7 illustrate the e\u000bects commented above of the hyperparameters Candgamma on decision surface creation.", "Figure 8.7: Hyperparameters Candgamma e\u000bects in the decision boundary de\f- nition (based upon RHYS (2020)).", "It is possible to notice that higher Cvalues lead to narrower margins, with a smaller amount of data between them. Therefore, there is a more signi\fcant penalty due to cases inside the margin. Higher gamma values are related to surfaces' shapes more adherent to the samples (RHYS, 2020).", "The number of trials was di\u000berent among the kernels, as they do not have the same number of hyperparameters. The linear kernel optimization had only 10 trials, as there is only one hyperparameter. The Gaussian and sigmoid kernels had 20 trials, which is the product between the 10 trials for the Chyperparameter and the two availablegamma options.", "The polynomial kernel had 40 trials; once there is also the degree of the polyno- mial to be optimized. The default value of the polynomial function degree is 3, and it was searched among the integer options 2, 3 and 4. The coef hyperparameter was maintained in its default value in polynomial and sigmoid kernels, equal to zero. The results will be discussed in separated subtopics, one for each kernel, to facilitate interpretations.", "Linear Kernel Figures 8.8 and 8.9 illustrate the results of the linear kernel SVM hyperparameter optimization. Once there is only one hyperparameter, the parallel coordinate plot and the slice plot illustrate almost the same information. Therefore, only the \frst one will be displayed, along with the optimization history plot. Figure 8.8: Parallel coordinate plot of linear kernel SVM hyperparameter optimiza- tion.", "Figure 8.9: Optimization history plot of linear kernel SVM hyperparameter opti- mization.", "The \frst of the 10 trials was the best of them all, achieving 87.83% of accuracy. The reduction in clinwas bene\fcial to the classi\fcation results, and the best value Polynomial Kernel Figures 8.10 to 8.12 show the polynomial kernel SVM hyperparameter optimiza- tion outcomes. They represent, as already described in Chapters 6 and 7, the e\u000bect of each hyperparameter and of a combination of hyperparameters in the objective value, as well as the progression of the trials.", "Figure 8.10: Slice plot of polynomial kernel SVM hyperparameter optimization. Figure 8.11: Parallel coordinate plot of polynomial kernel SVM hyperparameter optimization.", "Figure 8.12: Optimization history plot of polynomial kernel SVM hyperparameter optimization.", "Analogously to the linear kernel case, cpoly was considered better for smaller values, with a \fnal value of 0.101. Analyzing the \frst three plots, degreepoly tends to lead to better results when it is equal to 2 or 3. The best accuracy score was 88.07%, achieved when degreepoly was equal to 2. The gammapoly was decided to beauto, as it was generally better than the scale results. Gaussian Kernel The Gaussian kernel SVM hyperparameter optimization results are displayed in Figures 8.13 to 8.15. The plots in the same order as in the previous subtopic are showed. The results of this kernel, along with polynomial and sigmoid, are more comparable, since the same hyperparameters are optimized.", "Figure 8.13: Slice plot of Gaussian kernel SVM hyperparameter optimization. Figure 8.14: Parallel coordinate plot of Gaussian kernel SVM hyperparameter opti- mization.", "Figure 8.15: Optimization history plot of Gaussian kernel SVM hyperparameter optimization.", "Analyzing the slice plot, comparing Figures 8.13 and 8.10, it is possible to notice the same behavior of the Chyperparameter. The smaller is C, the higher is the objective value. When the value is higher than 1, the behavior stabilizes. The optimizedcrbfvalue was 0.102. The same \fgures are also very similar concerning thegamma hyperparameter.", "The parallel coordinate plots, available in Figures 8.11 and 8.14, present a similar result if the degree column is removed from the polynomial kernel analysis. The auto option presented much better results than the scale one. Therefore, gammarbfwas chosen asauto. The highest accuracy achieved was 88.02 Sigmoid Kernel A very similar analysis to the one presented for the Gaussian kernel is made for the sigmoid kernel as follows, once they have the same hyperparameters. Figures 8.16 to 8.18 display the hyperparameter optimization results. Figure 8.16: Slice plot of sigmoid kernel SVM hyperparameter optimization. Figure 8.17: Parallel coordinate plot of sigmoid kernel SVM hyperparameter opti- mization.", "Figure 8.18: Optimization history plot of sigmoid kernel SVM hyperparameter op- timization.", "The highest objective value was 88.08%, when csigwas 0.100 and gammasig wasauto. This was expected, as the previous subtopics showed similar results and similar graphic analysis. The same stabilization e\u000bect when Cis higher than 1 was observed. Figures 8.14 and 8.17 are quite alike, indicating the same behavior of the trials.", "Considering all the studied kernels, any Cvalue between 0.100 and 0.115 should adjust well for any of them. Those were values very close to the lower search bound. As explained at the beginning of this section, Cis related to the width of the margins. Lower values of Cindicate wider margins and smaller penalties. The samegamma \ftted very well for all the kernels that use this hyperparameter. Theauto option is a simpler equation, which depends only of the number of features and not on the dataset variance. Once there are 26 variables in the dataset, gamma value was equal to 0.0385, using Equation 8.22. This indicates that, for all the kernels that apply it, the gamma value was low. Hence, it is less sample-dependent, and over\ftting is less likely to happen.", "Therefore, the same Cvalue andgamma option are adequate for all the kernels. The polynomial kernel's extra hyperparameter, the degree , was better when small and it was chosen to be equal to 2.", "8.3.2 Classi\fcation Results", "The boxplot analysis was used again to understand the reproducibility of the method's training. The four studied kernels boxplots are shown together in Figure Figure 8.19: Boxplots of SVM accuracy training scores.", "The linear and Gaussian kernels showed better results than the other kernels, due to higher accuracy and reproducibility. They had accuracy means of 94.39% and 93.48%, and standard deviations of 0.13% and 0.20%, respectively. Another indicator is that the lowest accuracy of those kernels, 94.20% and 93.19%, are higher than the best value obtained by polynomial and sigmoid ker- nels, which were 91.19% and 88.79%. The accuracy means and standard deviations of those two kernels were 90.92%, 88.08%, 0.25% and 0.46%, respectively. Compared with RF and KNN, the linear and Gaussian kernels are between them, in both accuracy and reproducibility. The sigmoid kernel was the worst result, the least accurate and reproducible. This e\u000bect may be better explained by the learning curve, shown in Figure 8.20.", "Figure 8.20: Learning curves of SVM accuracy training scores. Figure 8.20 con\frms the superiority of the linear and Gaussian kernels, inde- pendently of the number of samples used for training. None of them presented a behavior that would indicate over\ftting. On the contrary, the sigmoid kernel shows an under\ftting behavior, as the accuracy diminished when the number of samples increased. A peak was reached with 10,000 samples, but this was still a worse result than all the others. After the training evaluation, the test was performed. The \frst analysis is the classi\fcation reports obtained, presented in Table 8.2. Table 8.2: Classi\fcation reports of the SVM test data analysis. In the linear and Gaussian kernels, all the precision and recall values were higher than 75%, but most of them were over 90%. A similar behavior to the one observed in RF and KNN can be perceived in all the kernels, as most faulty classes have higher precision than recall, which is the opposite of what happens in the fault-free class.", "Therefore, the fault-free class presents more false positives, leading again to the conclusion of a pre-fault region disturbing the fault detection. Figure 8.21 shows the confusion matrices, useful to continue this analysis.", "Figure 8.21: Confusion matrices of the SVM test data analysis. The confusion matrices show the same behavior among themselves and in com- parison to RF and KNN, with more expressive confusions in samples predicted as fault-free but that are, in reality, part of the faulty classes. In the polynomial and Gaussian kernels, all the confusions between faulty classes were smaller than 1%. In the linear kernel, only the confusion between faults 2 and 5 was a little higher than 1%, around 2%. In the sigmoid kernel, as the results were worse, there were more confusions higher than 1%.", "Considering the test accuracy, the linear and Gaussian kernels can still be consid- ered the best among the SVM analysis, classifying correctly 93.96% and 93.50% of the data, respectively. The polynomial and sigmoid kernels had 90.97% and 88.57% of accuracy.", "So far, the best test result is SVM with linear kernel, followed by SVM with Gaussian kernel and RF. Those three techniques had very similar test accuracies, varying between 93.44% and 93.96%. The next would be the KNN and then SVM with polynomial and sigmoid kernels.", "The full computing time of the SVM method was 2.35h, including data read- ing, hyperparameter optimization, training, testing, and graphic results of the four kernels. As the script is fully integrated, the full computing time for each kernel was not measured. The reduced computing time, including only training and test- ing, was measured for each kernel. The Gaussian and sigmoid kernels took about seconds, respectively.", "Chapter 9 Neural Networks", "9.1 Introduction", "NN are AI techniques based on a representation of the human brain, composed of neurons. The \frst publication about this topic was made almost 80 years ago, and sometime later perceptrons were de\fned as simple processing operators. After a slowdown in the cienti\fc community's interest in NN, they returned to the journals after the backpropagation training algorithm was developed.", "Many di\u000berent NN have been studied in the last years, with various structures, layers, and objectives. Three of them are used in this Chapter: MLP, CNN and MLP are one of the most simple networks, composed of a group of perceptrons organized in layers. CNN were introduced 30 years ago, with the primary purpose of treating image recognition. Those networks are composed of a feature extraction step, followed by the classi\fcation step. The latter is very similar to a MLP. SOM were also developed almost 40 years ago, used for pattern extraction and cluster analysis, considered adequate to work with nonlinear dynamic datasets. It is a non-supervised algorithm, the only one of this kind applied in this work. This Chapter starts with a literature review of general NN concepts, such as activation functions and optimization algorithms for training, followed by a brief description of CNN and SOM. The results are divided into: i) general settings, that explain the algorithms' peculiarities; ii) hyperparameter optimization, with divisions for each type of network; iii) classi\fcation results; and iv) a \fnal variable importance analysis, comparing the one obtained with SOM with other techniques presented in this work.", "9.2 Literature Review", "9.2.1 Basic Concepts", "NN are ML algorithms based on a representation of the brain operation, using computing units known as neurons. Those units receive input data and transform it into output information using parameters, such as weights and bias. The weights represent the synaptic connections in a real brain. A single neuron's operation receiving inputs and using its parameters to produce outputs is called a perceptron (AGGARWAL, 2018; ALOM et al. , 2019).", "In ALOM et al. (2019), a brief timeline of the NN development is exposed. Almost 80 years ago, the \frst work on the topic was published in MCCULLOCH and PITTS (1943), using the neuron nomenclature and, only 15 years after that, the perceptron concept was described in ROSENBLATT (1958). A decade later, a new and discouraging work (MINSKY et al. , 2017) reported the perceptron learning obstacles, which led to a slowdown in NN research. In the '80s, the topic became discussed again after a new learning method was presented in ACKLEY et al. (1985) and deepened in RUMELHART et al. (1986), making NN relevant until nowadays. With its increasing improvements and applications, deeper and more complex NN were developed, called DNN.", "The following subsections will discuss the NN architectures relevant to the present work, with some basic de\fnitions and mathematical approaches.", "9.2.2 Multi-Layer Perceptrons", "The simplest NN is a single perceptron, and a group of perceptrons organized in layers is known as a MLP. They consist of at least one hidden layer between the input and the output layers (AGGARWAL, 2018; ALOM et al. , 2019). A perceptron example is shown in Figure 9.1, with two input data and one output. The inputs are represented by x1andx2, the weights are we1andwe2,bi is the bias, and kis the class output.", "Figure 9.1: Perceptron example (based upon AGGARWAL (2018)). The classkcan be obtained using the sign of the value, de\fning it as \u00001 or +1, in a binary classi\fcation example. This is represented by Equation 9.1, which is generalized in Equation 9.2, as described in AGGARWAL (2018), where the number of features is pandVis the neuron's output. The weights are used to multiply the network's inputs and the neurons' outputs; meanwhile, the bias values are added to the neurons:", "V=sign pX j=1weijxj+bii!", "A MLP example is shown in Figure 9.2, in which Equation 9.2 applies. The input, hidden and output layers are represented in blue, green and pink, respectively. In this example, the NN has two input data, two hidden layers with three neurons each and one output.", "As indicated by the arrows, it is a feedforward NN; therefore, the information travels in a single way, without feedback (AGGARWAL, 2018). Although not rep- resented, each arrow is related to a weight wei, and each neuron iis associated with a biasbii.", "Figure 9.2: MLP example (based upon AGGARWAL (2018)).", "Besides the sign function applied in Equations 9.1 and 9.2, other functions can be used to process the values obtained in each neuron, known as activation functions. This process is summarized in Figure 9.3, where vis the neuron response before the activation function application; meanwhile, Vis the neuron outcome afterwards. Figure 9.3: Activation function application (based upon AGGARWAL (2018)). The most straightforward activation function is linear, shown in Figure 9.3 and by Equation 9.3. Some of the most traditional activations are sigmoid and hyperbolic tangent, given by Equations 9.4 and 9.5 (AGGARWAL, 2018):", "V=pX j=1weijxj+bii=v (9.3) Some more recent approaches are ReLu (Recti\fed Linear Unit) and SeLu (Scaled Exponential Linear Unit) activation functions. They are shown in Equations 9.6 and As described in the Keras library documentation, ReLu returns the same as the linear function if its outcome is positive and zero if not. Meanwhile, SeLu returns 1.0507 times the linear outcome if it is positive or, else, 1.758 times the exponential of the linear result minus one. Figure 9.4 displays the behavior of all the activation functions discussed.", "Figure 9.4: Activation functions behavior (based upon AGGARWAL (2018); HABIB and QURESHI (2020)).", "The linear function is typically applied in the output layer for prediction and regression; meanwhile, the other functions are usually used in the hidden layers. It is also common to apply the softmax activation function in the output layer, mainly in classi\fcation problems with multiple labels (AGGARWAL, 2018). In the case of the present work, although softmax is known to be an adequate choice based on the literature about this topic, the softplus activation function was also applied as an option for the output layer. Softmax and softplus activation functions are shown in Equations 9.8 and 9.9, and sofplus behavior is displayed in Figure 9.5:", "V=evi PK WhereKis the length of v. According to the Keras library, the softmax function transforms real values in probabilities for each category, creating a vector formed by values between 0 and 1, which add up to 1. As described in HAO ZHENG et al. (2015), softplus is a smoother version of the ReLu function, and is considered to be more stable.", "Figure 9.5: Softplus activation function behavior (based upon CHENG et al. (2020)). As in most ML algorithms, the main objective is to perform as minimum mis- classi\fcations as possible, reducing the error and improving the accuracy. This minimization is known as loss function and applies to most NN during training. For categorial targets, when there are more than two classes, the recommended loss function is known as the categorical cross-entropy loss, shown in Equation 9.10. This function is known to assist in convergence and to be used along the softmax activation function in the output layer for discrete outputs, such as classes. The number of neurons in this layer is tipically the same as the number of classes (AG- GARWAL, 2018; GONZ \u0013ALEZ-MU ~NIZ et al. , 2020):", "Where ^v1:::^vkare thekclasses' probabilities as returned by the softmax func- tion, andris the correct one. Equation 9.10 represents the loss for a single sample. To perform the training, commonly, the backpropagation algorithm is applied. It is a supervised training algorithm performed by the Keras library. This is the algorithm developed in RUMELHART et al. (1986) that revived NN as a topic of interest once it solved some of the learning issues faced at the time (ALOM et al. , 2019). Backpropagation is performed in two steps, the forwards and the backwards phases.", "In the forwards phase, the data is inserted into the network by the input layer, passed to the \frst hidden layers until the network's output layer is reached. This layer, \fnally, exits the \fnal results, which are compared with those provided in training, to calculate the error (AGGARWAL, 2018).", "In the backwards phase, the calculated error is used to adjust the hyperparame- ters of the output layer. The error is propagated to the antecedent layer to update its hyperparameters. This operation occurs consecutively until the input layer is reached, which is the last to have its hyperparameters adjusted. Thus, the entire NN had its parameters corrected (AGGARWAL, 2018). The mathematical steps of these algorithms and their adaptations to other NN are discussed in AGGARWAL Regarding other training aspects, the optimizer applied is very relevant. In general, the learning process is based on the minimization of the loss function's gradient. The learning rate is a hyperparameter that multiplies the calculated error and is related to the learning process's speed (AGGARWAL, 2018). To prevent di\u000eculties in convergence, an adaptive learning rate might be applied in training. A constant low learning rate will lead to a slow and long training process. Meanwhile, a large one will lead to a close solution to the optimal one, although it will end up oscillating around it once the distance between the current and the optimal solution is larger than the step (AGGARWAL, 2018). Besides, small learning rates can be stuck in local minima in case the step is too small to leave the local solution's surroundings.", "Some evolutions in the optimization algorithms have been implemented through- out the years. One of them is adaptive learning rates, with inverse or exponential decays. Another development is momentum-based techiniques, which accelerate learning and reduce oscillations once it focuses on consistent directions (AGGAR- WAL, 2018). In the Keras documentation, SGD is an optimization algorithm that applies gradient descent with traditional momentum and was chosen as one of the optimizer options in the present work. Figure 9.6 shows four di\u000berent scenarios for learning rates: constant and large, constant and small, adaptive without momentum and adaptive with momentum.", "Another improvement in the optimization algorithms was made with parameter- speci\fc learning rates, where di\u000berent adaptive learning rates with momentum are applied to speci\fc hyperparameters. These methods keep track of the partial deriva- tives, adapting the learning rate for each of them independently (AGGARWAL, One of the most applied and recommended parameter-speci\fc learning rate al- gorithms is Adam (GONZ \u0013ALEZ-MU ~NIZ et al. , 2020), available in the Keras library. A few years ago, it was introduced in literature in KINGMA and BA (2014), de- scribed as e\u000ecient and adequate to problems with large datasets. The same work presents Adamax, an adaptation of the original Adam algorithm. Another adapted algorithm is Nadam, introduced in DOZAT (2016), using the Nesterov Momentum instead of the traditional momentum. These three optimizers, Adam, Adamax and Nadam, were also applied as options in this work, along with SGD. Aside from the NN topology, activation functions of the layers and optimization Figure 9.6: Learning rate types: constant and adaptive, with and without momen- tum (based upon AGGARWAL (2018)).", "algorithms, two other hyperparameters are relevant: the batch size and the number of epochs. The batch size is the number of samples used to train the NN. The error is obtained after each batch evaluation. The number of epochs can be de\fned as the number of times that the complete dataset will be analyzed during training. Therefore, an epoch ends after all the batches are analyzed (AGGARWAL, 2018). To avoid over\ftting by a larger amount of epochs than necessary, a method known as early stopping can be applied in training, interrupting the iterations pre- maturely. To determine the right moment to intervine in training, cross-validation may be performed, evaluating the training and validation datasets simultaneously. The training is supposed to stop when the NN is best suited for both groups of data at the same time, before the validation loss increases (AGGARWAL, 2018; CHOLLET, 2017). Figure 9.7 illustrates this algorithm.", "Figure 9.7: Early stopping in NN training (based upon ALMEIDA (2002)).", "9.2.3 Convolutional Neural Networks", "CNN are a kind of NN with di\u000berent layers and shapes than MLP. It was \frst introduced about 30 years ago, in FUKUSHIMA (1988), with the original objective of solving recognition problems. They became commonly applied only a few years after, once its training was too computing costly, and the application of gradient- based learning algorithms made CNN easier to use (ALOM et al. , 2019). These NN are composed of two main parts: feature extraction and classi\fca- tion. The \frst part is composed of the convolution and pooling layers, normally alternately. The latter is similar to MLP, composed of fully connected layers, also knowns as dense layers. To connect these two parts of the network, there is usually a atten operation layer before the dense ones (AGGARWAL, 2018; ALOM et al. , The convolutional layers perform the feature learning step, detecting the con- junctions of features. This is made using kernels, also called \flters, of squared dimensions. They are typically smaller than the layer in height and width but have the same depth. During convolution, the \flters are placed in every possible posi- tion in the layer, to guarantee that it was analyzed entirely (AGGARWAL, 2018; GONZ \u0013ALEZ-MU ~NIZ et al. , 2020).", "The pooling layers are applied in small grid regions of the previous layer, di- minishing data's dimensions, aggregating similar features. This procedure reduces the computational cost, making CNN more applicable. One of the most common ways to perform pooling is max-pooling; instead of returning the patches' mean, it chooses the highest value in those patches (AGGARWAL, 2018; ALOM et al. , 2019; GONZ \u0013ALEZ-MU ~NIZ et al. , 2020).", "Figure 9.8 shows an example of a typical CNN, composed of the feature extraction phase with convolution and pooling layers and the classi\fcation phase with the fully connected layers. The input is an image, and there are four possible classes to \ft it in this example.", "Figure 9.8: CNN example (based upon ALOM et al. (2019); GONZ \u0013ALEZ-MU ~NIZ As shown in this example, these networks are well suited for pictures, videos, and other 2D entries. Therefore, an adaptation has been made to allow 1D entries to be analyzed, creating the 1D CNN. Its \frst implementation was described and published about 6 years ago in KIRANYAZ et al. (2015) and it was used by various works that came after, such as biomedical data classi\fcation for diagnosis and fault detection in motors. Speci\fcally in the chemical engineering \feld, SINGH CHADHA et al. (2019) performed a FDD study using the TE benchmark. The one-dimensional version of CNN is less computationally costly once it performs simpler operations (KIRANYAZ et al. , 2019).", "9.2.4 Self-Organizing Maps", "SOM are a NN type developed between the early '80s and the early '90s, as published in KOHONEN (1982) and KOHONEN (1989), both cited in the classic paper entitled \\The self-organizing map\" (KOHONEN, 1990). They are data-driven, useful in pattern extraction and cluster analysis in datasets with multiple variables. Many works have proven them to be functional to treat nonlinear dynamics, time- series analysis and prediction (CLARK et al. , 2020).", "In opposition to all the other algorithms presented in this work so far, this one is an unsupervised learner. That is the reason why it is considered data-driven, as the data are unlabeled (CLARK et al. , 2020).", "The method works organizing the data in a rectangular or hexagonal grid, gener- ating a map. The training intends to map the position of the nodes throughout the dataset. When presented with unknown data, the trained network matches each of them with the best map nodes available, creating clusters of data sharing the same nearest node, besides similar characteristics that justify their grouping (CLARK An example of a SOM is displayed in Figure 9.9, with a rectangular grid. There are two input data in this example, treated with a 5 x 5 map. The network's output is displayed on the right part of the \fgure, with the \fve classes perceived in the dataset.", "Figure 9.9: SOM example (based upon AGGARWAL (2018); CLARK et al. (2020)).", "9.3 Case Study", "9.3.1 General Settings", "Optuna was also applied in the hyperparameter optimization of MLP, CNN and SOM. The objective function was very similar to the one used in RF, KNN and SVM, maximizing the accuracy scores'. However, the obtaining of this value was di\u000berent.", "To perform the 5-fold cross-validation in MLP and CNN, it was necessary to use a wrapper1to integrate the Keras library, which was used to create the models, with the scikit-learn library, that possesses the cross-validation function. This wrapper was also applied to integrate those libraries to build the learning curves, once the function is also a part of scikit-learn. An extra step was necessary in the CNN's case, as even before the hyperparameter optimization, the datasets were reshaped to \ft the network's input layer.", "1Keras models are not tipically compatible with the functions available in scikit-learn. There- fore, Keras has built-in wrappers to make the applications possible. There are two kinds of wrap- pers: one for classi\fers and another for regression models.", "9.3.3 Classi\fcation Results", "The reproducibility of the networks was evaluated again through a boxplot anal- ysis. Figure 9.21 display the boxplot for each of them.", "Figure 9.21: Boxplot of MLP, CNN and SOM accuracy training scores. MLP and CNN results were more accurate and reproducible than the SOM ones. The mean accuracies were 95.97%, 95.15%, and 86.72%, respectively, with standard deviations around 0.25% for the \frst two networks, and 1.4% for the last one. Therefore, MLP presented the best results, but CNN was also very satisfactory. A general comparison of all boxplots in the present work is available in Chapter 10. The learning curves for MLP and CNN are displayed in Figure 9.22, to evaluate if there was over\ftting.", "The learning curves for both networks do not indicate over\ftting and con\frm the quality of the results, which keep improving as more samples are used in training. Since the shades around the curves are more prominent in MLP training, they indicate a higher variation in the results. This be\fts what is observed in the boxplot, with MLP's being slightly wider than CNN's.", "In the case of MLP and CNN, an evaluation of the early stopping behavior was also performed, with model loss plots comparing the training and validation data results. They are displayed in Figure 9.23.", "Figure 9.22: Learning curve of MLP and CNN accuracy training scores. Although both networks are con\fgured to perform until 500 epochs if necessary, they only needed about 50 epochs to complete adequate training. Maybe even fewer epochs were required, once a patience of 20 epochs was applied to guarantee that the networks would not be stuck in local minima during training. Along with the learning curves, these plots con\frm that the networks were not over\ft to the training dataset. Unfortunately, none of these analyses were made regarding SOM, but the network is expected to be under\ftted once it had the worst results among the three.", "Figure 9.23: Model loss of MLP and CNN accuracy training and validation scores. The following evaluations were performed in the test dataset. Table 9.4 shows the classi\fcation report for all the networks, and the confusion matrices are displayed in Figure 9.24, to complete the analysis.", "Table 9.4: Classi\fcation reports of the MLP, CNN and SOM test data analysis. Figure 9.24: Confusion matrices of the MLP, CNN and SOM test data analysis. Once again, the fault-free recall value was higher than precision in all three networks; meanwhile, in about 70% of the cases presented, the precision was higher in faulty classes. This leads to the same conclusion discussed in the three previous Chapters about the existence of a pre-fault region. In MLP and CNN, most precision and recall values were higher than 80%.", "In these two networks, the confusion matrices are similar to those presented in the previous Chapters. There is little confusion between the faulty classes, and most misclassi\fcations are concentrated in samples predicted as fault-free, that are actually of faulty classes. In SOM, that also happens, but there is great confusion between faults 2 and 5. This e\u000bect was only observed in one previous matrix, in the linear SVM, shown in Figure 8.21.", "The test accuracy was similar between MLP and CNN, around 92%; meanwhile, SOM accuracy was smaller, about 88%. This is the worst result so far among all the methods, but this will be further discussed and compared in the next Chapter. SOM's full computing time was much smaller than the others, around 35 min- utes; meanwhile, MLP and CNN took almost 15 hours and a little over 17 hours, respectively. The reduced computing times were 2 minutes and a half, 3 minutes and 10 seconds to MLP, CNN and SOM. A complete comparison of the full and reduced computing times will also be made in the next Chapter.", "9.3.4 Variable Importance", "Considering the weights that associate each neuron to the input features, it is possible to observe which neurons, or parts of the map, are more dependent on cer- tain dataset variables. To simplify the visual interpretation, instead of displaying the 26 feature maps, the most relevant feature map was created. This is a visualiza- tion of the SOM where each neuron is related to its most relevant feature. Figure 9.25 shows this map.", "Figure 9.25: Most relevant feature plane of the SOM.", "To create a VI analysis, the importance of a variable was calculated as the ratio between the number of neurons where that variable is the most important and the total amount of neurons. The optimized map is rectangular and 28 x 28, so there are 784 neurons. Figure 9.26 displays the bar plot of VI in the SOM results. Figure 9.26: VI of the SOM test data analysis.", "Considering Figures 9.25 and 9.26, the most important variable is the sump level of the \frst column, followed by acid gas' pressure and the second column's sump level. Those variables presented total importance of 63.3%.", "Many variables did not appear in the most relevant feature map and are not present in the bar plot. Among the \ffteen variables analyzed in this case, seven of them have importances smaller than 2%, and \fve of them are between 2% and 7%. A general comparison of VI considering the results obtained by PCA and RF will be performed in the next Chapter.", "Chapter 10 General Discussion and Results Comparison", "10.1 Introduction", "This Chapter focuses on discussing and comparing all the methods and results obtained in the present work. The following section performs an analysis on the training reproducibility, training and test accuracies, along with full and reduced computing times. These metrics are evaluated separately and together to decide which techniques performed better and should be used in further studies. The following section compares variable importances obtained by PCA, RF and SOM. The features that are considered important by all three of them are discussed. General considerations about the results compose the last section.", "10.2 Accuracies and Computing Times", "To perform a fair comparison between all the di\u000berent techniques used in the present work, all of the boxplots presented in Chapters 6 to 9 were organized together in Figure 10.1.", "As in the previous Chapters, the smaller the boxplot, the more reproducible is the model. In an accuracy scale, the higher it is, the better, indicating a greater amount of correct classi\fcations. The three worst models, considering both criteria, are SOM and SVM with sigmoid and polynomial kernels. Removing them from the comparison, the models with accuracy greater than 93% remain. Figure 10.2 shows the boxplots for the best models.", "Figure 10.1: Boxplots with accuracy training scores comparing all the methods. Figure 10.2: Boxplots with accuracy training scores comparing the best methods. These models have accuracy between 93% and 96.5%; that is, they are all close to each other and very satisfactory. They are also quite reproducible since the more signi\fcant di\u000berence between the best and the worst accuracy is 0.7% in MLP networks.", "After evaluating the training, the next step is to discuss the classi\fcation of the test data results. This comparison is shown in Figure 10.3, composed of the accuracy values displayed in confusion matrices throughout this work. Figure 10.3: Test accuracies comparison.", "In this analysis, the three models with higher accuracy are among the six best results of the training evaluation, displayed in Figure 10.2. They are RF and SVM with linear and Gaussian kernels, with accuracies greater than 93%. It is relevant to note that RF is better than any other method when the reduced test dataset is analyzed after the VI analysis displayed in Chapter 6, with 95.2% of accuracy. Another evaluation is shown in Figures 10.4 and 10.5, regarding the full and reduced computing time, respectively. As SVM has a script completely integrated between the applied kernels, the full computational time includes all of them. The reduced computational time was measured separately.", "Figure 10.4: Full computing time comparison.", "RF presented one of the shorter full computing times, only after SOM, even including VI evaluation, dataset reduction and the performance of a new analysis, classi\fcation report and confusion matrix. MLP and CNN presented much longer computing times, mainly due to hyperparameter optimization.", "Figure 10.5: Reduced computing time comparison.", "Regarding the reduced computing time, most of the methods needed less than 50 seconds to conclude the scripts, except KNN, MLP and CNN. Longer computing times are unfavorable to real world application of the algorithms. Considering a global analysis of the results above, including training accuracy and reproducibility, test accuracy, and computing times, it can be inferred that the best techniques are RF and SVM with linear and Gaussian kernels. These three techniques were more than 93% accurate in training and testing, in addition to being the most reproducible, with the smallest boxplots.", "The three methods have reduced computing time shorter than 50 seconds. The RF's total computing time is one of the shortest and, although SVM took more than 2 hours, this comprises the four types of kernel tested. Thus, it is likely that the total individual computing time for each kernel would be around 30 minutes, which would make it one of the best in this regard.", "The present work raises the same question asked in FERN \u0013ANDEZ-DELGADO et al. (2014), a work published 7 years ago: \\Do we need hundreds of classi\fers to solve real world classi\fcation problems?\". The authors applied 179 classi\fers from 17 families, including RF, KNN, SVM and NN. Four software or programming languages were used to analyze 121 di\u000berent datasets, so the results would not be biased by the behavior of a speci\fc group of data.", "The main conclusion in FERN \u0013ANDEZ-DELGADO et al. (2014) is that RF al- gorithms normally present the best results among the studied families, followed by SVM with a Gaussian kernel, without a signi\fcant statistical di\u000berence between them. This is concordant with the results obtained in the present work, where both of them are among the three best methods applied to the created dataset. Therefore, in future works using this dataset, it would be advised to focus only on RF and SVM with linear and Gaussian kernels. The possible applications for them will be further discussed in the next Chapter.", "10.3 Variable Importance Comparison", "To create a comparison between the three methods used to analyze the variables importance, Table 10.1 shows the most important variables according to PCA, RF and SOM. The variables are displayed in decreasing order of importance, and the ones present in all three columns are marked in bold.", "Table 10.1: Most important variables in each method.", "In most cases, the importance is similar between the same variable in the two sampling times. That is, the variables usually are accompanied by their own delay. This is expected once they should represent the same type of information in the database.", "In general, the most important variables, considering all three methods, are the mass feed ow of the process (SW1F), the temperature of C1's main feed stream (SW8T) and C2's sump level (C2SL). Considering that most faults were applied in SW1 and the fouling e\u000bect is related to SW8, those variables should be important in classi\fcation. Therefore, this was expected.", "The other relevant variable to all three methods simultaneously, which was not accessed or intentionally disturbed in the simulations, is C2SL. Hence, this is the most important variable concerning process dynamics behavior. There are no vari- ables that do not appear in any of the analysis, each of them is considered important in at least one of the three methods.", "Some variables that may be considered the most important in a method might not even be among the most important ones in another. The di\u000berences between the algorithms might explain this. PCA aggregates multiple variables to create new ones, completely altering the space where the samples are displayed. RF and SOM are completely di\u000berent AI techniques, once the \frst is supervised, and the other is unsupervised. Besides, RF is an algorithm based on decision trees and thresholds; meanwhile, SOM is based on the formation of clusters.", "Considering the results discussed in the previous section, RF is one of the best methods to perform classi\fcation and might also be the most reliable way to decide which are the most important variables.", "10.4 General Considerations", "All of the classi\fcation reports and confusion matrices presented in this work indicate a pre-fault region's existence. This is a problem that originates in creating the database and depends on an operator's experience in a real chemical process or an expert running a simulation once they choose the data labels. Supervised ML algorithms focus on the classes pre-de\fned in training; that is, if a sample, in reality, does not belong to any of those classes, it will be \ftted as the closest one. The closest class concept will depend on how each method performs the classi\fcation, as discussed in the previous Chapters.", "The algorithm may detect a di\u000berent class from the pre-de\fned ones in training. Therefore, the methods can be applied to create this new class. First, in the database labelling, the fault region can be overrated to cover all of the pre-fault region. The data misclassi\fed as fault-free can be separated and receive a new label of pre-fault data.", "After the inclusion of the new label, the algorithms may be trained, so they come to understand this transition phase of the dynamic behavior. ML can help to delimitate the regions of fault-free, pre-fault and fault when even the process operators or simulation experts could not identify when adding the samples' labels. Chapter 11 Conclusions and Future Work", "11.1 Overview", "The present work is a study of an SWTU of two columns using Aspen Plus® V10 and Aspen Plus Dynamics®V10, including the static and dynamic simulations. A dynamic behavior study of the SWTU considering fault-free and faulty scenarios was also performed. The data processing intended to create the most real datasets possible, including noise, delay and an unbalanced proportion of the classes. Various AI algorithms were optimized, applied and analyzed using multiple Python libraries as scikit-learn, Keras and MiniSom to study the process behav- ior. They performed the FDD in the dataset and analyzed which variables are the most important to understand the process.", "11.2 Conclusions", "The static and dynamic simulations of a two-column SWTU were performed successfully, providing the desired separation. Acid gas recovered 85.2% of the H2S, and ammoniacal gas recovered 90.9% of the NH 3in the static simulation. Since 99.7% of the inlet water was recovered in the treated water, there was an insigni\fcant water loss. In the dynamic simulation, H2Srecovery was improved, increasing to 89.1%.", "The fault simulation was successfully performed, representing situations of ood- ing and clogging of the columns. The overheating phenomenon called here \\column boil up\" of the \frst column was simulated, but its representation did not reach the column's operational limit but remained a fault in the process operation. The FDD was satisfactory, with over 87.5% accuracy in all AI techniques. RF and SVM with linear and Gaussian kernels showed the best results, with over 93% of accuracy in train and test, and had the shortest computational times. In addition to the mass feed ow of the process (SW1F) and the temperature of the most important feed stream of the \frst column (SW8T), the bottom level of the second column (C2SL) proved to be the most relevant variable, considering the three methods of evaluation used in this work. After variable reduction, the classi\fcation accuracy reached 95.2% using RF.", "As the AI applied successfully performed the FDD, it reassures an increasing relevance of this work type. It combines the practical knowledge of real process operators and simulation experts with the intrinsic knowledge that the algorithms can extract from the databases. Multiple pieces of information that are not perceived by the operators, such as pattern, can be interpreted by the methods applied. This may come to automate this type of monitoring.", "11.3 Contributions", "The present work had the following innovative academic contributions: Development of a two-column SWTU model, based on industrial dimensions and operational conditions; Proposition and tuning of control loops for the SWTU; Development of dynamic simulations that represent the normal operation of an industrial SWTU, including light disturbances; Development of dynamic simulations that represent faulty scenarios of an in- dustrial SWTU, including: signi\fcant disturbances that generate ooding and overheating, sensor faults in control loops and equipment faults, like fouling; Data processing for FDD studies of isolated and simultaneous faults; De\fnition of fault regions and respective data labeling; Provision of the processed database in Appendix B; FDD study of the two-column SWTU using and comparing several AI tech- niques: RF, KNN, SVM with di\u000berent kernels and various DNN types; Study of hyperparameters' optimization tools for the AI-based FDD algo- rithms.", "Regarding industrial applications, some technological contributions were also made:", "Guide for chemical processes' experts of the adequate AI tools to apply in similar problems; Study of variables' importance for fault investigation in SWTU, using multiple methods, such as PCA, RF and SOM; De\fnition of the importance of pre-fault regions evaluation.", "11.4 Future Work Suggestions", "One topic of interest that may be persued to continue this work is concerning the SWTU modeling. The dynamic simulation can be improved to comprise other fault scenarios. This includes the overheating phenomenon, that was not so well repre- sented in the present work. Besides, the databases labeling can be split in 13 classes instead of 7, considering each of the 6 faults as 2 di\u000berent classes: with and without fouling. Studies on the modi\fed database might enable a deeper understanding of the in uence of fouling in the process dynamic behavior.", "Also, a deeper study of the pre-fault region is recommended. An approach to this issue was discussed in the previous Chapter, which is to perform a two-step classi\fcation. First, the classi\fcation is applied as usual, and then the misclassi\fed data as fault-free can be relabeled as pre-fault data. The training and testing can be done again with the corrected dataset.", "Another option is to apply di\u000berent weights for the multiple classes. If fault-free data are given a higher weight in training, there may be less confusion in this class. Using the same functions of this work, it is possible to input weights to classes in RF and SVM, which are the recommended algorithms to continue the studies. Besides, the classi\fcation can be performed with a combination of AI techniques, using one algorithm to perform fault detection, and another to diagnose the faulty samples. Once RF is one of the best methods applied even after variable reduction, achiev- ing the best accuracy of the present study, it might be interesting to use this same reduced dataset in other algorithms, especially SVM with linear and Gaussian ker- nels. This can be another possible solution to the misclassi\fcation problem, once some features of the original dataset may be enhancing the confusion between the classes, creating a kind of noise.", "All the steps of FDD can be performed in real processes data, once labeled according to the scenarios studied in the present work. In a more bold approach, the dynamic process behavior studies can be magni\fed to understand and include the actions to correct, or even prevent, the studied faults. In the future, this could become an even more automated way to safeguard real processes, reducing the risks and accidents, along with economic loss.", "Bibliography ACKLEY, D. H., HINTON, G. E., SEJNOWSKI, T. J., 1985, \\A learning algo- rithm for boltzmann machines\", Cognitive Science , v. 9, n. 1, pp. 147{169. ADDINGTON, L., FITZ, C., LUNSFORD, K., et al., 2013, \\Sour Water: Where It Comes From and How to Handle It\", .", "AFRNIOMELO, 2020. \\EPV PEQ Aula 5 - Miscel^ anea de T\u0013 ecnicas\". Nov. Available at: <https://www.kaggle.com/afrniomelo/ epv-peq-aula-5-miscel-nea-de-t-cnicas >.", "AGGARWAL, C., 2018, Neural Networks and Deep Learning: A Textbook . AHA, D. W., KIBLER, D., ALBERT, M. K., 1991, \\Instance-based learning algo- rithms\", Machine Learning , v. 6, n. 1 (jan), pp. 37 { 66.", "AKIBA, T., SANO, S., YANASE, T., et al., 2019. \\Optuna: A Next-generation Hyperparameter Optimization Framework\". .", "AL-MALAH, K. I. M., 2017, Aspen Plus: Chemical Engineering Applications . Wi- ALMEIDA, J. S., 2002, \\Predictive non-linear modeling of complex data by arti- \fcial neural networks\", Current Opinion in Biotechnology , v. 13, n. 1, ALOM, M. Z., TAHA, T. M., YAKOPCIC, C., et al., 2019, \\A State-of-the-Art Survey on Deep Learning Theory and Architectures\", Electronics , v. 8, AMAYA, J., DUPUIS, R., INNOCENTI, M. E., et al., 2020, \\Visualizing and Interpreting Unsupervised Solar Wind Classi\fcations\", Frontiers in As- tronomy and Space Sciences , v. 7. ISSN: 2296-987X.", "ARUNTHAVANATHAN, R., KHAN, F., AHMED, S., et al., 2020, \\Fault detection and diagnosis in process system using arti\fcial intelligence-based cognitive technique\", Computers & Chemical Engineering , v. 134, pp. 106697. ISSN: BARROS, D. J. S., 2016, Investigation of the E\u000bect of Process Variables on H2S Removal E\u000eciency in a Two-Stage Sour Water Treatment Unit . Master's Thesis, PPGEQ - UFPR. In Portuguese.", "BASTOS, A. B. F. D., RODRIGUES, G. R., BARBOSA, L. C., et al., 2005, \\Acid Gas Management at Petrobras Re\fneries with a Focus on Reducing Atmo- spheric Emissions\". In: I Congresso Nacional de Engenharia de Petr\u0013 oleo, G\u0013 as Natural e Biocombust\u0013 \u0010veis . In Portuguese.", "BELATO, D. A. S., LIMA, J. R. D., ODDONE, M. R. R., 2002, \\Hydrocraking { A Way to Produce High Quality Low Sulphur Middle Distillates\". In: 17th World Petroleum Congress .", "BELGIU, M., DR \u0015AGUT \u0018 , L., 2016, \\Random forest in remote sensing: A review of applications and future directions\", ISPRS Journal of Photogrammetry and Remote Sensing , v. 114, pp. 24{31.", "BRAZIL, MINISTRY OF THE ENVIRONMENT, N. E. C., 2007. \\Resolution number 382 of December 26, 2006\". O\u000ecial Gazette, n º01 of January 2, section 1.", "BREIMAN, L., FRIEDMAN, J., STONE, C., et al., 1984, Classi\fcation and Re- gression Trees . Taylor & Francis. ISBN: 9780412048418.", "BREIMAN, L., 1996, \\Heuristics of instability and stabilization in model selec- tion\", The Annals of Statistics , v. 24, n. 6, pp. 2350 { 2383. BREIMAN, L., 2001, \\Random Forests\", Machine Learning , v. 45, pp. 5{32. CHENG, Q., LI, H., WU, Q., et al., 2020, \\Parametric Deformable Exponential Linear Units for deep neural networks\", Neural Networks , v. 125, pp. 281{ CHOLLET, F., 2017, Deep Learning with Python . Manning. ISBN: 9781617294433. CLARK, S., SISSON, S., SHARMA, A., 2020, \\Tools for enhancing the applica- tion of self-organizing maps in water resources research and engineering\", Advances in Water Resources , v. 143, pp. 103676. ISSN: 0309-1708. DE SOUZA JR, M. B., ROJAS SOARES, F. D., 2018, \\Machine Learning Appli- cation to Fault Detection and Diagnosis in a Chemical Process\", Blucher Chemical Engineering Proceedings , v. 1, n. 5, pp. 1213{1216. ISSN: 2359- DOWNS, J., VOGEL, E., 1993, \\A plant-wide industrial process control problem\", Computers & Chemical Engineering , v. 17, n. 3, pp. 245{255. ISSN: 0098- 1354. Industrial challenge problems in process control.", "DOZAT, T., 2016, \\Incorporating Nesterov Momentum into Adam\". . EVERITT, B., LANDAU, S., LEESE, M., et al., 2011, Cluster Analysis . Wiley Series in Probability and Statistics. Wiley. ISBN: 9780470978443. FAN, S.-K. S., HSU, C.-Y., TSAI, D.-M., et al., 2020, \\Data-Driven Approach for Fault Detection and Diagnostic in Semiconductor Manufacturing\", IEEE Transactions on Automation Science and Engineering , v. 17, n. 4, FERN \u0013ANDEZ-DELGADO, M., CERNADAS, E., BARRO, S., et al., 2014, \\Do we Need Hundreds of Classi\fers to Solve Real World Classi\fcation Prob- lems?\" Journal of Machine Learning Research , v. 15, n. 90, pp. 3133{3181. FERRERO, C. A., 2009, Algorithm for Time Series Prediction: Prediction Func- tions and Nearest Neighbors Selection Criteria Applied to Environmental Variables in Limnology . Master's Thesis, ICMC - USP. In Portuguese. FISHER ROSEMOUNT SYSTEMS, I., 2010. \\Robust Process Model Identi\fca- tion in Model Based Control Techniques\". Nov.", "FUKUSHIMA, K., 1988, \\Neocognitron: A hierarchical neural network capable of visual pattern recognition\", Neural Networks , v. 1, n. 2, pp. 119{130. GE, X., WANG, B., YANG, X., et al., 2021, \\Fault detection and diagnosis for reactive distillation based on convolutional neural network\", Computers & Chemical Engineering , v. 145, pp. 107172. ISSN: 0098-1354. GONZ \u0013ALEZ-MU ~NIZ, A., D \u0013IAZ, I., CUADRADO, A. A., 2020, \\DCNN for con- dition monitoring and fault detection in rotating machines and its con- tribution to the understanding of machine nature\", Heliyon , v. 6, n. 2, GRANZOTTO, M. H., 2020, Arti\fcial Intelligence Applied to Fault Detection in Chemical Processes . Doctor's Thesis, PPGEQ - UFU. In Portuguese. HABIB, G., QURESHI, S., 2020, \\Optimization and acceleration of convolutional neural networks: A survey\", Journal of King Saud University - Computer and Information Sciences . ISSN: 1319-1578.", "HAO ZHENG, ZHANLEI YANG, WENJU LIU, et al., 2015, \\Improving deep neural networks using softplus units\". In: 2015 International Joint Con- ference on Neural Networks (IJCNN) , pp. 1{4.", "HASTIE, T., TIBSHIRANI, R., FRIEDMAN, J., 2009, The Elements of Statis- tical Learning: Data Mining, Inference, and Prediction, Second Edition . Springer Series in Statistics. Springer New York. ISBN: 9780387848587. HATCHER, N., WEILAND, R., 2012, \\Reliable design of sour water strippers\", Petroleum Technology Quarterly , v. 17, n. 4 (01), pp. 83{91. HATCHER, N. A., JONES, C. E., WEILAND, R. H., 2014, \\Sour water stripping Part 3: WWT technology\", Digital Re\fning , pp. 1{6.", "HIMMELBLAU, D., 1978, Fault Detection and Diagnosis in Chemical and Petro- chemical Processes . Chemical engineering monographs. Elsevier. ISBN: HOTELLING, H., 1933, \\Analysis of a complex of statistical variables into princi- pal components.\" Journal of Educational Psychology , v. 24, pp. 417{441. ISERMANN, R., 2006, Fault-Diagnosis Systems: An Introduction from Fault Detection to Fault Tolerance . Springer Berlin Heidelberg. ISBN: IVANCIUC, O., 2007, \\Applications of Support Vector Machines in Chemistry\". In:Reviews in Computational Chemistry , cap. 6, pp. 291{400, John Wiley JOLLIFFE, I., 2005, \\Principal Component Analysis\". In: Encyclopedia of Statistics in Behavioral Science , American Cancer Society. ISBN: JOLLIFFE, I. T., CADIMA, J., 2016, \\Principal component analysis: a review and recent developments\", Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences , v. 374, n. 2065, KINGMA, D. P., BA, J., 2014. \\Adam: A Method for Stochastic Optimization\". . KIRANYAZ, S., INCE, T., HAMILA, R., et al., 2015, \\Convolutional Neural Net- works for patient-speci\fc ECG classi\fcation\". In: 2015 37th Annual In- ternational Conference of the IEEE Engineering in Medicine and Biology KIRANYAZ, S., AVCI, O., ABDELJABER, O., et al., 2019. \\1D Convolutional Neural Networks and Applications: A Survey\". .", "KNUST, C. M., 2013, Surface Response Analysis for The Design of Sour Water Treatment Plants . Master's Thesis, EPQB - UFRJ. In Portuguese. KOHL, A. L., NIELSEN, R. B., 1997, \\Removal and Use of Ammonia in Gas Puri\fcation\", Gas Puri\fcation , p. 278{329.", "KOHONEN, T., 1982, \\Self-organized formation of topologically correct feature maps\", Biological Cybernetics , v. 43, pp. 59{69.", "KOHONEN, T., 1989, Self-Organization and Associative Memory . Springer Series in Information Sciences. Springer Berlin Heidelberg. ISBN: KOHONEN, T., 1990, \\The self-organizing map\", Proceedings of the IEEE , v. 78, KU, W., STORER, R. H., GEORGAKIS, C., 1995, \\Disturbance detection and isolation by dynamic principal component analysis\", Chemometrics and Intelligent Laboratory Systems , v. 30, n. 1, pp. 179{196.", "LEAVITT, M., ET AL, 2004, Technical Support Document For The 2004 E\u000fuent Guidelines Program Plan . Technical report, Section 7, U.S. Environmental Protection Agency, O\u000ece of Water. EPA-821-R-04-014.", "LECUN, Y., BOTTOU, L., BENGIO, Y., et al., 1998, \\Gradient-based learning applied to document recognition\", Proceedings of the IEEE , v. 86, n. 11, LEE, D., LEE, J.-M., LEE, S.-Y., et al., 2002, \\Dynamic Simulation of the Sour Water Stripping Process and Modi\fed Structure for E\u000bective Pressure Control\", Chemical Engineering Research and Design , v. 80, n. 2, pp. 167{ LEE, S. Y., LEE, J. M., LEE, D., et al., 2004, \\Improvement in Steam Stripping of Sour Water through an Industrial-Scale Simulation\", Korean J. Chem. LI, W., GU, S., ZHANG, X., et al., 2020, \\Transfer learning for process fault diag- nosis: Knowledge transfer from simulation to physical processes\", Com- puters & Chemical Engineering , v. 139, pp. 106904. ISSN: 0098-1354. LORENA, A. C., CARVALHO, A. C. P. L. F. D., 2007, \\An Introduction to Support Vector Machines\", Revista de Inform\u0013 atica Te\u0013 orica e Aplicada , v. 14, n. 2 (dec), pp. 43 { 67. In Portuguese.", "LUYBEN, W., 2007, Chemical Reactor Design and Control . Wiley. ISBN: LUYBEN, W., 2013, Distillation Design and Control Using Aspen Simulation . Wi- MCCULLOCH, W. S., PITTS, W., 1943, \\A logical calculus of the ideas immanent in nervous activity\", The bulletin of mathematical biophysics , v. 5, n. 4, MINSKY, M., PAPERT, S., BOTTOU, L., 2017, Perceptrons: An Introduc- tion to Computational Geometry . The MIT Press. MIT Press. ISBN: MONSHI, M. M. A., POON, J., CHUNG, V., et al., 2021, \\CovidXrayNet: Optimizing data augmentation and CNN hyperparameters for improved COVID-19 detection from CXR\", Computers in Biology and Medicine , MORADO, H. P., 2019, Emission Minimization of Sour Water Striping Units: Surrogate Models for Heat Duty Control . Doctor's Thesis, EPQB - UFRJ. In Portuguese.", "MORAIS, R. L., 2017. \\Use of Random Trees for Sensory Classi\fcation of Cooked Rice\". In Portuguese.", "MORENO-TORRES, J. G., RAEDER, T., ALAIZ-RODR \u0013IGUEZ, R., et al., 2012, \\A unifying view on dataset shift in classi\fcation\", Pattern Recognition , OVADIA, Y., FERTIG, E., REN, J., et al., 2019. \\Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\". . PANYAKAEW, P., PORNPUTTAPONG, N., BHIDAYASIRI, R., 2020, \\Using machine learning-based analytics of daily activities to identify modi\fable risk factors for falling in Parkinson's disease.\" Parkinsonism & related disorders , v. 82, pp. 77{83.", "PARK, Y.-J., FAN, S.-K. S., HSU, C.-Y., 2020, \\A Review on Fault Detection and Process Diagnostics in Industrial Processes\", Processes , v. 8, n. 9. ISSN: PEARSON, K., 1901, \\LIII. On lines and planes of closest \ft to systems of points in space\", The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science , v. 2, n. 11, pp. 559{572.", "PEREIRA PARENTE, A., DE SOUZA JR., M. B., VALDMAN, A., et al., 2019, \\Data Augmentation Applied to Machine Learning-Based Monitoring of a Pulp and Paper Process\", Processes , v. 7, n. 12. ISSN: 2227-9717. doi: POE, W., MOKHATAB, S., 2016, \\Process Control\". In: Modeling, Control, and Optimization of Natural Gas Processing Plants , p. 97{172, Elsevier Sci- QUINLAN, M., 2003, KBR Re\fnery Sulfur Management, Part 11 - Sulfur Com- pound Extraction and Sweetening, Handbook of Petroleum Re\fning Pro- cesses . McGraw-Hill Education (Professional).", "QUINLAN, M., HATI, A., 2010, \\Processing NH3 Acid Gas in Sulphur Recovery QUI~NONERO-CANDELA, J., SUGIYAMA, M., LAWRENCE, N., et al., 2009, Dataset Shift in Machine Learning . Neural information processing series. RAMLI, N. M., 2018, \\Advanced Process Control\". In: El-Shahat, A. (Ed.), Ad- vanced Applications For Arti\fcial Neural Networks , IntechOpen, cap. 8, RHYS, H., 2020, Machine Learning with R, the tidyverse, and mlr . Manning Pub- ROSENBLATT, F., 1958, \\The perceptron: A probabilistic model for information storage and organization in the brain\", Psychological Review , v. 65, n. 6, RUMELHART, D. E., HINTON, G. E., WILLIAMS, R. J., 1986, \\Learning rep- resentations by back-propagating errors\", Nature , v. 323, pp. 533{536. SALVATORE, L., 2007, Sulphur Content Inference in Diesel Streams From Hy- drotreatment Unit Using Knowledge-Based Models . Master's Thesis, TPQB - UFRJ. In Portuguese.", "SARTORI, I., AMARO, C. A., DE SOUZA J \u0013UNIOR, M. B., et al., 2012, \\Fault De- tection, Diagnosis and Correction: A Consistent Proposition of De\fnitions and Terminologies\", Science & Engineering Journal , v. 21, pp. 41{53. In Portuguese.", "SEBORG, D. E., MELLICHAMP, D. A., EDGAR, T. F., et al., 2011, Process Dynamics and Control . John Wiley & Sons.", "SHU, Y., MING, L., CHENG, F., et al., 2016, \\Abnormal situation management: Challenges and opportunities in the big data era\", Computers & Chemical SINGH CHADHA, G., KRISHNAMOORTHY, M., SCHWUNG, A., 2019, \\Time Series based Fault Detection in Industrial Processes using Convolutional Neural Networks\". In: IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society , v. 1, pp. 173{178. doi: 10.1109/ SOUZA JUNIOR, M. B., CAMPOS, M. C. M. M., TUNALA, L. F., 2009, \\Dy- namic Principal Component Analysis Applied to the Monitoring of a Diesel Hydrotreating Unit\". In: Modeling, Control, Simulation and Di- agnosis of Complex Industrial and Energy Systems , cap. 4, pp. 75{96, The International Society of Automation (ISA). ISBN: 9781628705065. SPEAR, M. E., 1952, Charting Statistics . McGraw-Hill.", "SPEISER, J. L., MILLER, M. E., TOOZE, J., et al., 2019, \\A comparison of ran- dom forest variable selection methods for classi\fcation prediction model- ing\", Expert Systems with Applications , v. 134, pp. 93{101. SUTHAHARAN, S., 2015, Machine Learning Models and Algorithms for Big Data Classi\fcation: Thinking With Examples for E\u000bective Learning . Springer. TAUNK, K., DE, S., VERMA, S., et al., 2019, \\A Brief Review of Nearest Neigh- bor Algorithm for Learning and Classi\fcation\". In: 2019 International Conference on Intelligent Computing and Control Systems (ICCS) . IEEE, TUKEY, W., J., 1977, Exploratory Data Analysis . Addison-Wesley. VAIDA, A., CRISTEA, V. M., 2016, \\Development of The Sour Water Plant Dynamic Simulator for Improving Design and Operation\", Studia UBB VAPNIK, V., 1999, The Nature of Statistical Learning Theory . Information Science and Statistics. Springer New York. ISBN: 9780387987804.", "VAPNIK, V. N., CHERVONENKIS, A. Y., 1971, \\On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities\", Theory of Proba- bility and its Applications , v. XVI, n. 2, pp. 264{280.", "VENKATASUBRAMANIAN, V., 2019, \\The promise of arti\fcial intelligence in chemical engineering: Is it here, \fnally?\" AIChE Journal , v. 65, n. 2, VENKATASUBRAMANIAN, V., RENGASWAMY, R., YIN, K., et al., 2003, \\A review of process fault detection and diagnosis Part I: Quantitative model- based methods\", Computers & Chemical Engineering , v. 27, n. 3, pp. 293{ WANG, H., LIU, Y., ZHOU, B., et al., 2020, \\Taxonomy research of arti\fcial intelligence for deterministic solar power forecasting\", Energy Conversion WEILAND, R. H., HATCHER, N. A., 2012a, \\Sour Water Strippers Exposed\". In:Laurence Reid Gas Conditioning Conference , Norman, Oklahoma. WICKHAM, H., STRYJEWSKI, L., 2012, 40 years of boxplots . Technical report, WILSON, G. M., ENG, W. W. Y., 1990, Research Report RR-118 GPSWAT GPS Sour Water Equilibria Correlation and Computer Program . Technical report, Gas Processors Association, Tulsa, Oklahoma.", "ZHAO, Y., LI, T., ZHANG, X., et al., 2019, \\Arti\fcial intelligence-based fault detection and diagnosis methods for building energy systems: Advantages, challenges and the future\", Renewable and Sustainable Energy Reviews , Appendix A Static and Dynamic Simulations Schemes Figures A.1 and A.2 are larger versions of the static and dynamic simulations schemes displayed in sections 4.3 and 4.4, respectively.", "Figure A.1: Static simulation - larger version.", "Figure A.2: Dynamic simulation - larger version.", "Appendix B Developed Databases The standardized databases developed in the present work are available in: https://github.com/nogueira-ju/SWTU_FDD ."]}