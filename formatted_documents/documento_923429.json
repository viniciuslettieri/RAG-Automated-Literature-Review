{"title": "Investigation of modeling and control based on neural networks of a batch crystallization process", "authors": ["Lima, Fernando Arrais Romero Dias", "Souza J√∫nior, Maur√≠cio Bezerra de orient.", "Universidade Federal do Rio de Janeiro. Escola de Qu√≠mica"], "abstract": "A cristaliza√ß√£o √© um processo de separa√ß√£o e purifica√ß√£o muito usado em diferentes √°reas do setor industrial, como a farmac√™utica e a aliment√≠cias. Essa opera√ß√£o unit√°ria tem como objetivo alcan√ßar as distribui√ß√µes de tamanho e formato desejados para o cristal. O controle de temperatura se apresenta como uma parte essencial para alcan√ßar tal objetivo. Este trabalho busca modelar um processo de cristaliza√ß√£o para predizer os momentos da distribui√ß√£o do tamanho de part√≠cula, utilizando redes neuronais como modelo interno em um esquema de controle preditivo. Quatro estruturas diferentes de redes neuronais foram consideradas: a tradicional rede MultilayerPerceptron (MLP), um conjunto de quatro redes MLP em s√©rie e duas redes recorrentes,Echo State Network (ESN) eLong Short-TermMemory (LSTM).Os conjuntos de dados usados para treinamento e teste aplicaram o algoritmo de aprendizagem co-teaching, o qual utiliza dados simulados e experimentais. Os 479 valores experimentais de concentra√ß√£o e n√∫mero, tamanho, √°rea e volume das part√≠culas foram obtidos para v√°rias temperaturas e em dez bateladas diferentes para a cristaliza√ß√£o do sulfato de pot√°ssio (K2SO4). Os 9000 dados simulados foram gerados usando um modelo de balan√ßo populacional validado para o sistema. Primeiro, as quatro estruturas de redes foram treinadas para predizer os valores dos momentos de distribui√ß√£o de tamanho de part√≠cula um passo √† frente, usando a temperatura e os momentos atuais como entradas.Em seguida, o desempenho de predi√ß√£o das redes foi estudado para maiores horizontes de predi√ß√£o. Finalmente, um Controlador Preditivo N√£o-Linear (NMPC), baseado na estrutura de rede neuronal mais eficiente,foi aplicado com sucesso para o processo de cristaliza√ß√£o em batelada a fim de manter a distribui√ß√£o de tamanho das part√≠culas nas trajet√≥rias desejadas por meio da manipula√ß√£o da temperatura de opera√ß√£o. O comportamento do controlador foi estudado para quatro trajet√≥rias de refer√™ncia: constante, primeira ordem, segunda ordem e primeira ordem adapt√°vel. A performance do controlador proposto foi comparada √† de um controlador baseado numa abordagem cl√°ssica, a qual utiliza redes MLP. Consequentemente, o NMPC baseado na ESN foi mais eficiente em alcan√ßar as trajet√≥rias de refer√™ncia. A conclus√£o principal √© que o uso de recorr√™ncias melhorou a predi√ß√£o das redes neuronais, limitada, no entanto, a horizontes pequenos. Essa caracter√≠stica permitiu a implementa√ß√£o bem-sucedida do NMPC.Os resultados fornecem evid√™ncias preliminares para a futura implementa√ß√£o de abordagens de modelagem e controle baseadas em redes neuronais com horizontes curtos de predi√ß√£o para processos de cristaliza√ß√£o industriais.", "bibliography_pages": [96, 106], "keywords": ["Cristaliza√ß√£o", "Redes neuronais"], "urls": ["http://objdig.ufrj.br/61/dissert/923429.pdf"], "pdf_url": "http://objdig.ufrj.br/61/dissert/923429.pdf", "id": "923429", "sentences": ["Abstract Crystallization is a separation and purification process applied in many industrial sectors, such as pharmaceutical and food. This unit operation aims to achieve desired crystal size distribution. Temperature control is a crucial part of reaching this goal. This work seeks to model a crystal lization process to predict the moments of the particle -size distribution with neural networks used as the internal model in the predictive controller. Four different neural networks structures were considered: a classic single Multilayer Perceptron (MLP) network, a set of four MLP networks in series, and two recurrent networks, the Echo State Network (ESN) and the Long Short -Term Memory (LSTM). The dataset used for training and testing consisted of simulated and experimental data, applying the co -teaching learning algorithm. The 479 experimental values of concentration and particle number, length, area, and volume were obtained for several temperatures and ten different batch experiments of potassium sulfate (K 2SO 4) crystallization. The 9000 simulated data were generated using a population balance model for the system. First, the four network structures were trained to predict the moments of the particle size distribution values one step ahead, using the current temperature and moments values as inputs. Then , the network‚Äôs predictive performance was studied for larger prediction horizons. Finally, a Nonlinear Model Predictive Controller (NMPC) based on the most efficient neural network‚Äôs design was successfully applied to the batch crystallization process to maintain crystal size distribution on their desired trajectories by manipulating the operating temperature. The controller behavior was studied for four reference trajectories: constant, 1st order, 2nd order, and adaptive 1st order. The performance of the proposed NMPC was compared to a controller based on a classic approach, which consists of MLP networks. As a result, the ESN -based NMPC was more efficient in achieving the reference trajectories. The main conclusion is that the use of recurrence improved t he quality of the neural networks‚Äô prediction, but this is limited to short horizons in the present study . This feature allowed the successful implementation of the NMPC. The results provide preliminary evidence for the future implementation of prediction horizon recurrent neural net -based approaches for modeling and control of industrial batch crystallization processes.", "Keywords: Crystallization, Neural Networks, Echo State Network, LSTM, NMPC.", "Resumo A cristaliza√ß√£o √© um processo d e separa√ß√£o e purifica√ß√£o muito usado em diferentes √°reas do setor industrial, como a farmac√™utica e a aliment√≠cias. Essa opera√ß√£o unit√°ria tem como objetivo alcan√ßar as distribui√ß √µes de tamanho e formato desejados para o cristal. O controle de temperatura se apresenta como uma parte essencial para alcan√ßar tal objetivo. Este trabalho busca modelar um processo de cristaliza√ß√£o para predizer os momentos da distribui√ß√£o do tamanho de part√≠cula, utilizando redes neuronais como modelo interno em um esquema de c ontrole preditivo. Quatro estruturas diferentes de redes neuronais foram consideradas: a tradicional rede Multilayer Perceptron (MLP) , um conjunto de quatro redes MLP em s√©rie e duas redes recorrentes, Echo State Network (ESN) e Long Short -Term Memory (LST M). Os conjuntos de dados usados para treinamento e teste aplic aram o algoritmo de aprendizagem co-teaching , o qual utiliza dados simulados e experimentais. Os 479 valores experimentais de concentra√ß√£o e n√∫mero, tamanho, √°rea e volume das part√≠culas foram obtidos para v√°rias temperaturas e em dez bateladas diferentes para a cristaliza√ß√£o do sulfato de pot√°ssio (K2SO 4). Os 9000 dados simulados foram gerados usando um modelo de balan√ßo populacional validado para o sistema. Primeiro, as quatro estruturas de re des foram treinadas para predizer os valores dos momentos de distribui√ß√£o de tamanho de part√≠cula um passo √† frente, usando a temperatura e os momentos atuais como entradas. Em seguida, o desempenho de predi√ß√£o das redes foi estudado para maiores horizonte s de predi√ß√£o. Finalmente, um Controlador Preditivo N√£o -Linear (NMPC) , baseado na estrutura de rede neuronal mais eficiente , foi aplicado com sucesso para o processo de cristaliza√ß√£o em batelada a fim de manter a distribui√ß√£o de tamanho das part√≠culas nas trajet√≥rias desejadas por meio da manipula√ß√£o da temperatura de opera√ß√£o. O comportamento do controlador foi estudado para quatro trajet√≥rias de refer√™ncia: constante, primeira ordem, segunda ordem e primeira ordem adapt√°vel. A performance do controlador p roposto foi comparada √† de um controlador baseado numa abordagem cl√°ssica, a qual utiliza redes MLP.", "Consequentemente, o NMPC baseado na ESN foi mais eficiente em alcan√ßar as trajet√≥rias de refer√™ncia. A conclus√£o principal √© que o uso de recorr√™ncias melh orou a predi√ß√£o das redes neuronais, limitada, no entanto, a horizontes pequenos para o presente estudo . Essa caracter√≠stica permitiu a implementa√ß√£o bem -sucedida do NMPC. Os resultados fornecem evid√™ncias preliminares para a futura implementa√ß√£o de abordagens de", "modelagem e controle baseadas em redes neuronais com horizontes de predi√ß√£o para processos de cristaliza√ß√£o industriais.", "Palavras -Chave : Cristaliza√ß √£o, Redes Neuronais , Echo State Network, LSTM, NMPC.", "Contents", "Appendix B ‚Äì Paper published for 32nd European Symposium on Computer Aided Process Engineering (ESCAPE32), June 12 -15, 2022, Toulouse, France. ............... 120 Appendix C ‚Äì Manuscript of an MPC to control the solute concentration in a batch crystallization process submitted to the Process Systems Engineering Brazil 2022", "List of Tables", "List of Figures", "Figure 10: Model predictive control strategy. Adapted from Seborg et al. (2011). ....... 45 Figure 13: Apparatus used to perform the batch crystallization (MORAES, 2019). ..... 53 Figure 16: Simulation of the PB model and comparison to experimental data for a constant Figure 17: Simulation of the PB model and comparison to experimental data for a varying Figure 21: MLP in series structure to predict the moments' values n steps ahead. ........ 69 Figure 23: LSTM inputs and outputs to predict the moments' values n steps ahead. ..... 70 Figure 25: ESN inputs and outputs to predict the moments' values ùëõ steps ahead. ....... 72 Figure 26: R -squared values for the four models and predictions one step forward. ..... 72 Figure 27: R -squared values for the four models and predictions two steps forward. ... 73 Figure 28: R -squared values for the four models and predictions five steps forward. ... 74", "Figure 40: Adaptive first -order reference trajectory results for ESN model. ................. 88 Figure 41: Adaptive first -order reference trajectory results for MLP model. ................ 89 Figure A.1: Moments, concentration and temperature values for the training dataset.", "Dotted points indicate the experimental values, and solid line represents the simulated Figure A.2: Moments, concentration and temperature values for Experiment 1 for the test Figure A. 3: Moments, concentration and temperature values for Experiment 2 for the test Figure A. 4: Moments, concentration and temperature values for Experiment 3 for the test Figure A. 5: Moments, concentration and temperature values for Experiment 4 for the test Figure A. 6: Moments, concentration and temperature values for Experiment 5 for the test Figure A. 7: Moments, concentration and temperature values for Experiment 6 for the test Figure A. 8: Moments, concentration and temperature values for Experiment 7 for the test Figure A. 9: Moments, concentration and temperature values for Experiment 8 for the test Figure A. 10: Moments, concentration and temperature values for Experiment 9 for the", "Figure A.11: Moments, concentration and temperature values for Simulation 1 for the test Figure A.1 2: Moments, concentration and temperature values for Simulation 2 for the test Figure A.1 3: Moments, concentration and temperature values for Simulation 3 for the test", "List of Acronyms and S ymbols", "Acronyms", "ANN Artificial Neural Networks ESN Echo State Network KDP Potassium Dihydrogen Phosphate MPC Model Predictive Control MLP Multilayer Perceptron MSE Mean Squared Error MSP Markov State Process NMPC Nonlinear Model Predictive Control PB Population Balance PSE Process Systems Engineering LSTM Long Short -Term Memory RNN Recurrent Neural Network ùë°ùëéùëõ‚Ñé Hyperbolic Tangent", "Symbols", "ùê¥ Preexponential factor ùëé Growth constant ùëéùëó(ùë°) Weighted sum of recurrent hidden layer ùëè Growth constant ùêµ0 Rate of nucleus formation", "ùêµ Nucleation rate ùêµ(ùë°,ùúÉùëè) Nucleation rate ùëèùëó(ùë°) Weighted sum of output layer from recurrent network ùëèùëê Bias value corresponding to the update term of the cell state ùëèùëì Bias value of the forget gate ùëèùëñ Bias value of the input gate ùëèùëú Bias value of the output gate ùê∂ Concentration of the solute ùê∂‚àó Concentration of the solute in equilibrium ùê∂ùë° Cell state in time ùë° ùê∂ÃÉùë° Term to update the cell state in time ùë° ùê∑(ùë°,ùúÉùëë) Dissolution rate ùê∏ Error Function ùê∏ùëÉ Error Function from each prediction f ( . ) Activation Function ùëìùë° Forgot gate ùëì(ùêø,ùë°) One-dimensional crystal size distribution ùê∫ùëñ Linear growth rate ùê∫ùêº Size independent growth rate ùê∫(ùë°,ùúÉùëî) Growth rate along with the internal coordinate L ‚Ñé(ùëò) State value of the ESN in time ùëò ‚Ñéùëñ(ùë°‚àí1) Past hidden state ‚Ñéùëó(ùë°) Output of the recurrent hidden layer ‚Ñéùë° Hidden state in time ùë°", "ùëñùë° Input gate ùëòùëé Nucleation coefficient ùëòùëè Activation energy for nucleation ùëòùëê Supersaturation (s) exponent for nucleation ùëòùëë ùúá3 exponent for nucleation ùëò1 Activation energy for growth ùëò2 Supersaturation (s) exponent for growth ùêøùëñ Crystal‚Äôs length dimension ùêø Particle‚Äôs size ùêøùëõ Size of the nuclei ùëÄ Control horizon ùëõ‚Ñé Number of neurons in the hidden layer ùëõùëñ Number of neurons in the input layer ùëõùëú Number of neurons in the output layer ùëÅùëù Number of predictions ùëõ(ùë•,ùëü,ùë°) Number density function ùëÅ(ùëü,ùë°) Number of particles per unit volume of physical space ùëÅ1 Inferior p rediction horizon ùëúùë° Output gate ùëÉ Prediction horizon ùëü Vector containing the external positions of the particles ùë†ùëùùëñ,ùëò Information from neuron ùëñ to neuron ùëù in layer ùëò ùë†ùëùùëô Predicted value from the neural network ùëÜ Supersaturation ratio", "ùë°ùëùùëô Target value ùë¢(ùëò) Control action in time ùëò ùë§ùëóùëñ,ùëò Weight value corresponding from neuron ùëñ to neuron ùëó in layer ùëò ùë§ùëê Weight values corresponding to the update term of the cell state ùë§ùëì Weight values of the forget gate ùë§ùëñ Weight values of the input gate ùë§ùëú Weight values of the output gate ùëä Weight values of the ESN ùë• Vector containing the internal coordinates of the particles ùë•ùë° Input vector of the LSTM cell ùë¶ùëó(ùë°) Output of the recurrent network ùë¶(ùëò) Output of the ESN in time ùëò ùë¶ùëü(ùëò) Reference value of the output variable in time ùëò ùë¶ÃÇ(ùëò) Predicted value of the output variable in time ùëò ùúÉùëó,ùëò+1 Bias corresponding to neuron ùëó in layer ùëò+1 ùõºùëùùëó,ùëò Weighted sum from neuron ùëó to neuron ùëù in layer ùëò ùúë Leak rate ùúÉ Contact angle of the solid phase ùúÇ Step Size ùúÄ Moment Term ùúé Relative saturation ùúéùëì Sigmoid activation function ùúáùëñ ùëñùë°‚Ñé moment of the particle size distribution ùúá0‚àó Zero-order moment of the particle size distribution", "ùúáùëñ‚àó Ratio between the ùëñùë°‚Ñéorder and the zero -order moment s of the particle size distribution ùúáùëñ‚àó,ùëÖùëá Reference trajectory of the ùúáùëñ‚àó ùúáùëñùëÖùëá ùëñùë°‚Ñé moment of the particle size distribution reference trajectory ùúáùëñ‚àó(ùë°ùëíùëõùëë) ùúáùëñ‚àó value at the end of the experiment ùúáùëñ0‚àó Initial ùúáùëñ‚àó value ùúáùëñ‚àó,ùëö Measured ùúáùëñ‚àó value ùúèùëóùëñ Weight value corresponding to the output layer of the recurrent network state from neuron ùëñ to neuron ùëó ùõæùëóùëñ Weight value corresponding to the past hidden state from n euro ùëñ to neuron ùëó ùõæ First-order and adaptive first -order reference trajectories constant ùúèùëñ Second -order reference trajectory time constant for the ùëñùë°‚Ñé moment ùõø(ùêø‚àí ùêøùëõ) Initial nuclei crystal size distribution ùõø(ùëó) Term to penalize the outputs ùúÜ(ùëó) Term to penalize the inputs ùõº Volume shape factor ùúåùëê Specific weight of the crystal ùúÉùëî Vector of growth kinetics ùúÉùëè Vector of nucleation kinetics ùúÉùëë Vector of dissolution kinetics Œ©ùëü Domain of external coordinates Œ©ùë• Domain of internal coordinates ‚àÜC Supersaturation", "‚àÜùê∫ùëêùëüùëñùë° Critical Gibbs free energy change ‚àÜùê∫ùëÜ Free energy change for the formation of the nucleus surface ‚àÜùê∫ùëâ Free energy change for the phase transformation ‚àÜùê∫‚Ñéùëúùëö Free energy change for the homogeneou s nucleation ‚àÜùê∫‚Ñéùëíùë° Free energy change for the heterogeneous nucleation ‚àÜùë¢(ùëò) Increment of the control action in time ùëò ‚àáùê∏(ùëäùëò) Error Function Gradient Chapter 1", "Introduction", "1.1. Motivation", "Crystallization is a unit operation widely used in many industries, such as pharmaceutical, fine chemicals, and food, for product separation and purification (ZHANG et al., 2022a). One strategy broadly applied for this process consists of cooling crystalli zation technology. With the growing regulatory constraints on the product quality, it is becoming more important to achiev e the desired crystals‚Äô size distribution while running a crystallization process (NAGY and BRAATZ, 2012) . Therefore, the cooling temp erature profile is a vital part of the operation of the cooling crystallization strategy to reach its goal, making process control an essential part of this process (LIU et Different strategies have been designed to guarantee crystallization pr ocess control, such as PID and model -based controller s (JHA et al., 2017) . In particular, Model Predictive Control (MPC) is a viable option if the process‚Äô important variables can be measured and an accurate process model is available . The MPC strategy applies the process model to predict the output variables in a prediction horizon and then search for the condition of the manipulated variables that guarantees the desired controlled variables values. Thus, modeling the process is an important step to use this strategy. Machine learning algorithms, particularly neural networks, have been widely used in chemical engineering. In the eighties, Hoskins and Himmelblau (1988) published a pioneering wo rk applying neural networks to fault detection and diagnosis of a chemical engineering problem. Twenty years later , Himmelblau himself performed a review (HIMMELBLAU, 2008) presenting over 200 literature references with applications of neural networks in c hemical engineering. In the present century, Venkatasubramanian (2018) showed that artificial neural networks (ANN s) have been used in several industrial sectors. Some of these applications are machine learning for materials science and for catalyst design and discovery . The author cited that big companies, like General Electric and British Petroleum , are using machine learning to monitor oil -well performance in real-time. A more recent study done by Schweidtmann et al. (2021) demonstrated that ANN are more relevant in Process Systems Engineering (PSE) now because a large amount of data is currently available. In terms of architectures, even though feedforward ANNs ‚àí like the ones named Multilayer Perceptron (MLP) ‚àí have become a commonplace, the use of new and efficient paradigms with recurrent signals ‚àí as Echo State Network (ESN) (JAEGER and HAAS, 2004) and Long Short -term Memory (LSTM ) (HOCHREITER and SCHMIDHUBER , 1997; GOODFELLOW et al., 2016) ‚àí is lately receiving a renewed interested to represent dynamic processes (WU et al., 2020a; JORDANOU et al., 2022).", "The last decades have witnessed a growth in the commercial development of in situ real -time sensor technologies , that could provide more online data, and also in terms of computer hardware (NAGY and BRAATZ, 2012). This creates a favorable scenario for the application of neural net -based approaches. Even under these circumstances, there is a lack of studies using ANNs for modeling and control of crystallization process , when compared to other chemical engineering areas . In this area, machine learning approac hes have been used (TANG et al., 2017), but, in terms of ANNs, the use has been restricted to fault detection and diagnosis (SALAMI et al. , 2021), while Population Balance (PB) models seem to be the preferred choice. Neural networks are an efficient approa ch because of their ability to provide models based on process data, capturing the intrinsic nonlinearities. For optimization and process control, ANNs appear as an interesting alternative because their online use typically demands lower computational cost than approaches based on pure phenomenological models (SANT ANNA et al., 2017). Finally, neural networks can also be used synergistically with laws of physics providing hybrid approaches (RAISSI et al., 2019; RAWLINGS et al, 1993).", "Therefore, the first mo tivation of this dissertation is the investigation of the use of neural networks to dynamic model and control for crystallization processes. The second is the use in a batch cooling crystallization process, due to the challenges of the dynamic behavior. Fi nally, as it is very important for the industries, especially pharmaceutical ones, the control of crystal size is aimed .", "With this purpose, data from open -loop experiments of batch crystallization of potassium sulfate (K2SO 4) (MORAES, 2019; MORAES et al., 2019), conducted in the laboratory of the group (LABCADS/EQ/UFRJ), were used here to design the neural network models. The crystallization setup was equipped with image sensing and software that provided online measurements of crystal number and shape. Ad ditionally, a PB model (MORAES, 2019; MORAES et al., 2019), validated with these data, was used in simulations of this investigation as the real process in order to test the proposed control structures.", "1.2. Objectives", "The first objective of this work consists of developing a neural network strategy to predict future moments‚Äô values , as t he moments are related to physical properties of interest such as crystal number, length, area, and volume . In order to investigate the effect of recurrence in the ability of the neural networks to represent the dynamics of the process, feedforward and recurrent neural networks are considered. Therefore, four different neural networks designs are analyzed: a single Multilayer Perceptron ( MLP), a set of four MLP in series, an LSTM, and an Echo State Network (ESN). The second objective of this work consists of developing a neural net -based NMPC to conduct the crystallization batch to the final desired state. The internal model of the contro ller and different reference trajectories are analyzed . Four reference trajectories considering a non -oscillatory path are chosen : a zero -order, a first -order, a second -order, and an adaptive first -order. These reference trajectories are the desired values of crystals' size. As part of the NMPC design, the process‚Äô closed loop behavior with the NMPC based on two different ANN structures (the feedforward MLP and the recurrent ESN) is also compared.", "1.3. Text Outline", "The text is organized as follows:", "‚Ä¢ Chapter 2 contains the bibliographic review divided into three main sections: Section includes a review about crystallization processes ; Section 2.3 presents the model predictive control strategy ; Section 2.4 presents ANN and control applications for the crystallization process, and Section 2.5 explains the innovations of the present study compared to the works shown in the previous section ; ‚Ä¢ Chapter 3 presents the method ology: Section 3.1 contains the experimental setup to develop the crystallization process; Section 3.2 demonstrates the development of the Population Balance model; Section 3.3 explains how the training and test datasets were built; Section 3.4 presents th e proposed model predictive control approach, illustrating the objective function, the constraints, and the reference trajectories , and Section 3.5 shows the approach used to develop the neural networks ; ‚Ä¢ Chapter 4 shows all the results and discussions abou t this project. First, Section 4.1 demonstrates the validation of the Population Balance model used in this work. Then, Section 4.2 presents the results for all four neural networks‚Äô designs proposed, demonstrating their capability to predict the moments‚Äô values in the future. Finally, Section 4.3 discusses the operation of the proposed model predictive controller for the four reference trajectories and compares its operation to a classic approach; ‚Ä¢ Chapter 5 discusses the results and suggests further develo pments. In the sequence, the bibliography is present. Finally, three Appendices are included. Appendix A presents the experimental data (MORAES, 2019; MORAES et al., 2019) used. Appendix B presents a work derived from this dissertation that was accepted for presentation at ESCAPE -32 (2022). Appendix C consists of a work that was submitted to the II -PSE-BR (2022).", "Chapter 2", "Bibliography Review", "This chapter‚Äôs purpose is to make a brief bibliography review of all relevant topics about the subjects studied in this work. First, the subject neural networks is presented, explaining how this technique functions, its uses, and the different structures applied in this work. Then, the topic about the crystallization describes the process, its relev ance in chemical industries, and the Population Balance Model. After that, there is a topic present ing the strategy of model predictive control, and explains how it works. Finally, ANN and control applications for the crystallization process , and the innov ations of the present study compared to other studies are presented.", "2.1. Neural Networks", "Artificial neural networks (ANN s) are a mathematical model that tries to mimic the highly interconnected networks of neurons presented in the human brain. The neural network goal is to understand the correlation between the set of input and output patterns (ABDI-KHANGAH et al ., 2018) . Therefore, the network studies this correlation by first learning from a series of past examples defining sets of input and o utput correspondences for a given system, known as training set. Then, the network is used to predict new outputs applying a new set of input (BAUGHMAN and LIU, 1995) .", "The typical structure of a neural network is shown in Figure 1, which is composed of an input layer, a hidden layer, and an output layer (CHATTOPADHYAY et al., 2019) . First, the information is received in the input layer from an external source, and it is passed to the hidden layer. Then, the hidden layer receives the information and does all the information processing. Finally, the output layer receives the processed information, and sends the results out to an external receptor (GOLHANI et al., 2018) .", "Figure 1: Typical structure of a neural network.", "The information processing done in the hidden layer is a key part of the development of a neural network. Figure 2 illustrates how this action is done in a single neuron. First, the neuron receives a set of information ùë†ùëùùëñ,ùëò from all the neurons of th e previous layer . The neuron sums all th is information multiplied by a weight (ùë§ùëóùëñ,ùëò) corresponding to each neuron of the anterior layer . The result of this weighted sum is summed to an internal activation limit , the bias (ùúÉùëó,ùëò+1). The final result fro m all these sums is pass ed through an activation function (f ( . )), which can be sigmoid or hyperbolic tangent for example, and then, the answer ùë†ùëùùëó,ùëò+1 is achieved (WANG et al., 2016) .", "Figure 2: Information processing done by a neuron.", "There are different kinds of architectures of neural networks, such as Multilayer Perceptron and Recurrent Network. The architectures applied in this work will be presented in the next subtopics.", "2.1.1. Multilayer Perceptr on", "The Multilayer Perceptron (MLP) network is the most applied architecture in the literature because of its simplicity. This kind of network presents feedforward signals, which means that an information that enters the network always goes to the next laye r, never returning to the previous layer (DE SOUZA JR., 1993).", "The typical structure of a MLP network is the same as illustrated in Figure 1, which is composed of an input layer, at least one hidden layer, and an output layer (TATAR et al., 2016) . First, the set of inputs ùë†ùëùùëó,0 enters the network by the input layer as presented by Equation 1, which ùëõùëñ represents the number of neurons in the input layer, and ùëì is the linear activation function (BAUGHMAN and LIU, 1995) .", "The set of outputs ùë†ùëùùëó,1 from the input layer is sent to the hidden layer. In each neuron from the hidden layer, it is realized a weighted sum of all ùë†ùëùùëó,1 to achieve the ùõºùëùùëó,2 value as presented in Equation 2 , which ùëõ‚Ñé is the number of neurons in the hidden layer . The weight values (ùë§ùëóùëñ,1) are corresponding to each connection between neurons , and ùúÉùëó,2 is the bias . Then, the ùë†ùëùùëó,2 calculated according to an activation function and the obtained ùõºùëùùëó,2 as demonstrated by Equation 3 (TAYEBI et al., 2019) .", "After the hidden layer, the ob tained ùë†ùëùùëó,2 values are sent to the output layer. In the output layer, another weighted sum is done, resulting the ùë†ùëùùëó,3 values as presented in Equation 4, which ùëõùëú is the number of neurons in the output layer. Also, ùë§ùëóùëñ,2 are the weight value s and ùúÉùëó,3 is the bias. Then, the ùë†ùëùùëó,3 values are calculated according to the obtained ùõºùëùùëó,3 and an activation function, which is usually linear, as illustrated in Equation 5 (OLAWOYIN and CHEN, 2018) .", "Much has been said about weights and bias es values, but not how to get them , which is the key part of a neural network training. The MLP applies the backpropagation strategy to adjust the weights and biases (ZHANG et al, 2018) . The goal of the training is to minimize the error function between the predicted (ùë†ùëùùëô) and target (ùë°ùëùùëô) values. The error function is presented i n Equation s 6 and 7, which ùëÅùëù is the number of predictions, and ùëõùëú is the number of neurons in the output layer (VISHWAKARMA et al., 2020) .", "ùê∏= ‚àëùê∏ùëùùëÅùëù", "The error calculated in the output layer is backpropagated to the previous layer in order to adjust the weights and biases values from this layer, which consists of the backpropagation strategy mentioned before. Therefore, the first s tep is to adjust the weigh ts and biases between the output and hidden layers and then adjust these values for the input and hidden layers. One strategy applied to do this adjust ment consists of the descending gradient method (DE SOUZA JR., 1993) .", "Rumelhart and McClelland (1986) applied the typical backpropagation method to minimize the objective function in Equation 6 using the descending gradient . Therefore, the search direction was the negative value of the gradient with a constant step size (ùúÇ) as presented in Equation 8. Also, ‚àáùê∏(ùëä) is calculated by Equations 9 to 1 2 (BAUGHMAN and LIU, 1995) .", "ùúïùê∏ ùúïùê∏ ùúïùê∏ ùúïùê∏", "However, the descending gradient with a constant step size used in the typical backpropagation presented some inefficiency because of the difficult convergence by choosing the step size. Therefore, different approaches were studied to solve this problem and one of them consists of adding a moment term as showed in Equation 13 (BAUGHMAN and LIU, 1995) .", "Another approach to improve this training algorithm was proposed by Leonard and Kramer (1990), which consists of combining batching of examples and line search with gradient descent in conjugate directions. This strategy applies a dynamic adjustment of ùúÇ and ùúÄ, avoiding some problems.", "Kingma and Ba (2015) proposed a learning algorithm called ADAM consisting of a third strategy to improve the typical backpropagation. ADAM is a method for stochastic optimization that requires little memory and first-order gradient. Different from the backpropagation, the convergence analysis of ADAM is performed by summing all differences between the current prediction and the best fixed -point parameter. Moreover, this strategy does not apply a fixed -step.", "2.1.2. Recurrent Neural Networks", "Recurrent Neural Networks (RNN) were developed in the 1980s (RUMELHART et al., 1986) in order to represent sequential data (WU et al., 2019). The difference between RNN from the other networks is their structure, which consi sts of a hidden state and a feedback loop of hidden states (REN and NI, 2020). This feedback loop introduces previous information to the current state and uses it to update the current hidden state (SHAHNAZARI, 2020). This kind of structure gives the netwo rk the capability to simulate time series and understand the dynamic behavior (PETSAGKOURAKIS et al., 2020). Figure 3 illustrates the structure of RNN, in which u(t) is the input and x(t) is the output in time t of the hidden units.", "Figure 3: Structure of RNN.", "Equations 1 4 and 15 illustrate the calculation performed in the hidden layer. First, ùëéùíã(ùë°) is calculated by a weighted sum of the inputs ùë•ùëñ(ùë°) and the values of the past hidden state ‚Ñéùëñ(ùë°‚àí1). After that, a nonlinear transformation f is applied to the calculated ùëéùíã(ùë°) and ‚Ñéùëó(ùë°) is obtained. Th is transformation f is an activation function and can be hyperbolic tangent or sigmoid functions , for example (MULDER et al., 2015) .", "The results obtained in the hidden layer are sent to the output layer. The calculation of the output value ùë¶ùíã(ùë°) is done in the same way as in MLP network and is performed by Equations 16 and 17, where ùúèùëñùëó is the weight value and f is an activation function (MULDER et al., 2015) .", "The introduction of dependenc ies in time steps in the neural network causes the use of a different algorithm to calculate the gradient update of the model parameters that minimize the loss function, known as Backpropagation Through Time (BPTT). First, this method calculates the differen ce between the target and the output values and save it for each time step past. The n, the weight gradient updates are calculated as the network is feedback (SUN et al., 2020) . However, RNN trained with this algorithm can present vanish ing or explosion to infinity gradient issues (LILLICRAP and SANTORO, 2019) , introducing a necessity to improve this algorithm and create new structures of RNN, such as Long S hort-Term Memory (LSTM) network and Gated Recurrent Units (GRU ) (REN", "2.1.2.1. Long Short -Term Memory", "Long S hort-Term Memory (LSTM) network is a kind of Recurrent Neural Network very used for forecasting because of its capability of memorizing historical information over a long time (LIU et al., 2021). LSTM solves the problem of gradient vanishing and long -term memory loss, frequent in regular RNN (REN and NI, 2020). Figure 4 illustrates an example of an LSTM cell, which is composed of three gates: forgot gate ( ùëìùë°), input gate (ùëñùë°), and output gate (ùëúùë°). These gat es have the function of controlling the flow of information inside the network (WU et al., 2021b), where ùë•ùë° is the input vector, ùê∂ùë° is the cell state at time t and ‚Ñéùë° is the hidden state at time t.", "Figure 4: LSTM cell . Adapted from Lyu et al. (2020) .", "The first gate is the forget one, which is responsible for choosing the information that has to be deleted from the previous cell state ( ùê∂ùë°‚àí1) to update the cell state (INAPAKURTHI et al., 2021) . Equation 18 demonstrates how the gate signal is calculated, knowing that ùëìùë° can only be 0 or 1, which 0 indicates that a piece of information should be discarded and 1 is the opposite. Furthermore, ùúéùëì is the sigmoid activation function, the terms ùë§ùëì are the wei ghts and ùëèùëì is the bias (MADHU et al., 2021) .", "The input gate is the next step, which is responsible for choosing the useful information for updating the cell state. Equation 19 illustrate s how the gate signal can be obtained, calculated by the sum of the bias (ùëèùëñ), the weighted past prediction and i nputs (ELSHEIKH et al., 2021) . After this, the activation function tanh is applied to the selected information and then the cell state is updated using the data chose n in the input and forgot gates. These last two steps are performed by Equations 20 and 21 (INAPAKURTHI et", "The output gate is the last one and has the function of controlling the information flow through the following hidden state, using the sigmoid function as shown in Equation 22 (ELSHEIKH et al., 2021) . After this, the following hidden stat e is calculated using the determined ùëúùë° and ùê∂ùë°, as presented in Equation 23 (MADHU et al., 2021) .", "While developing an LST M, some other parameters may be considered to achieve the most efficient network, such as the Regularizers terms. The Regularizer L2 is known as weight decay, and it consists of the addition of a term in the objective function equivalent to the square of t he weights multiplied by another weight. This reduces the size of the parameters, avoiding significant variations in the network output. There is another term named Regularizer L1, in which a term equivalent to the absolute of the weights is added to the o bjective function. Another parameter that can be considered consists of the initializers, and they are responsible for defining the way to set the initial random weights of the layers (CHOLLET et al., 2015).", "2.1.2.2. Echo State Network", "The Echo State Network (ESN) is another kind of recurrent neural network proposed by Jaeger et al. in 2004, which applies a different strategy to avoid gradient issues (LI et al., 2012). This network efficiently performs nonlinear dynamic systems modeling as a ‚Äúblack -box‚Äù time series model. It has been applied in several fields, such as speech recognition and network communication (HUANG et al., 2020) and prediction in oil wells (DIAS et al., 2019) . The ESN is composed of an input layer, a dynamical reservoir, and an out put layer, as shown in Figure 4, in which W represents the weight vectors for each layer (DIAS et al., 2019).", "First, the inputs of the network are fed in the input layer and then sent to the reservoir layer. The reservoir layer is the recurrent part, where the outputs of the reservoir and output layers are feedback (MATINO et al., 2019). Furthermore, the weights of the reservoir layer are not trained but randomly fixed, avoiding local minimum problems (YING -CHUN and XIN, 2018). Finally, the results of the reservoir layer are sent to the output layer in order to calculate the expected results.", "Figure 5: Structure of an ESN . Adapted from Patane et al. (2021) .", "Equations 24 and 25 represent the mathematical modeling of reservoir and output layers, respectively (PATANE et al., 2021).", "where, ùëñ,‚Ñé and ùë¶ are the input, state and output vectors at time k; W is the weight vector for each layer as represented in Figure 5; ùëì is the a ctivation function, usually, hyperbolic tangent (JORDANOU et al., 2019) ; ùúë is known as the leak rate, and is the percentage of a current state ùë•(ùëò) that is transferred into the next state ùë•(ùëò+1). The weights from the reservoir layer are randomly determined by a Normal Distribution and rescaled for the resulting system to be stable but still demonstrate rich dynamics. As a result, the only step in training ESN is finding the weight values of the output layer, this is usually done by linear regression (ANTONELO et al., 2017).", "2.1.3. Co-teaching", "Han et al. (2018) proposed a strategy to deal with the training of deep network s using a noisy dataset. The study applied this new idea to enhance the mode l accuracy in image classification problems with noisy data. The proposed strategy was called co - teaching and presented good results dealing with datasets with a large number of classes, heavy noise, and that can be trained from scratch.", "Co-teaching is a different technique to train machine learning models by training two models simultaneously. This strategy takes advantage of noisy -free data, using a dataset composed of noisy -free and noisy data (YANG et al. , 2020) . The co -teaching purpose derives from the observation that neural networks will use a simple pattern to fit training data at the early stage of the training process. As a consequence, noisy data usually has a high loss function value, while noise -free data has a small value (HAN et There are two different types of co -teaching methods as presented in Figure 6: the symmetric and the asymmetric. The symmetric algorithm applies a mixed dataset, containing noisy and noise -free data. First, a mini-batch is chosen from the original dataset at each epoch. Then, each model analyzes its data sequences, and produces a small dataset with all the data that has a low function value. This new dataset is sent to the peer network and this process is repe ated until the end of the training epochs. On the other hand, the asymmetric co -teaching applies a similar training , but one model only uses noise -free data and the other one only uses noisy data. Therefore, model A sends a subset of noise -free data sequences to model B (WU et al., 2021 a).", "Figure 6: Symmetric (1) and asymmetric (2) co -teaching .", "2.2. Crystallization Process", "Crystallization is a chemical process used to generate a solid crystalline product with desired purity, size , and shape from an impure feed solution (SU et al., 2015). A basic application of this technique is in the recovery of salts from their aqueous sol ution in the inorganic bulk chemical industry. Additionally, the crystallization process is used to recover crystalline product s, refine the intermediary, and remove undesired salts in the organic industry (TAVARE, 1995) . As Nagy and Braatz (2012) state cr ystallization is ubiquitously used for separation and purification in many sectors, such as pharmaceuticals, food, and fine chemicals .", "The crystallization occurs through a mass transfer phenomenon, in which the supersaturation acts as the driving force of the crystal formation. The supersaturation (‚àÜùê∂) is calculated by the difference between the concentration of the supersaturated solution (ùê∂) and the saturated concentration (ùê∂‚àó) as presented in Equation 26 (GIULIETTI et al., 2001) .", "Therefore, when ‚àÜùê∂ is higher than 0, the growth of the crystals is observed and the solution is called supersaturated. On the other hand, when ‚àÜùê∂ is less than 0, the dissolution of the crystals is seen, and the solution is called subsaturated. On the other hand, when ‚àÜùê∂ is equal to 0, there is the chemical equilibrium of each phase (MULLIN, There are other important definitions to measure the supersaturation, which consists of the supersaturation ratio (ùëÜ) and the relative saturation (ùúé). These two terms are represented by Equations 27 and 28, respectively (GIULIETTI et al., 2001) .", "ùëÜ=ùê∂", "Supersaturation is a key part to achieve the crystal‚Äôs growth. However, the presence of supersaturation by itself does not directly lead to the formation of crystals. Therefore, to achieve this goal it is needed a metastable solution. In a metastable solution, a small finite change is necessary as an initiator to the formation of crystals (SCHALL and MYERSON, 2019). The metastability of supersaturated solutions is a consequence of nucleation , in which no crystallization occurs unless a nucleus achieves this critical size (MULLIN, 2001).", "The concept of a metastable solution can be seen in Figure 7. As represented in this illustration, three different regions are observed. The first one is delimited by t he equilibrium curve (solid line) and is called the unsaturated zone, where crystallization cannot happen. The metastable region is located between the solid and the dashed lines. As mentioned before, this region is characterized by improvable spontaneous crystallization , but if a crystal w as seeded in a metastable solution, growth would occur on it. The third zone is located above the dashed line and consists of the unstable supersaturated region, where spontaneous crystallization is probable, but not inev itable", "Figure 7: Solubility curve and metastable zone.", "There are several strategies for applying crystallization , such as cooling, evaporation, vacuum (adiabatic cooling), and chemical reaction. The chosen technique usually consists of the one that leaves the highest yield with the lowest energy consumption (COSTA and GIULIETTI, 2012) .", "2.2.1. Crystallization Mechanisms", "The crystallization process can be divided in two steps: nucleation and crystal growth (SU et al., 2022) . The nucleation consists of the generation of the crystals and can occur by two different mechanisms, the primary , which happens with the absence of crysta lline surfaces, and the secondary nucleation , involving the active participation of crystalline surfaces . Also, the primary nucleation can be classified in to two different types, which are homogeneous and heterogeneous nucleation (LEWIS et al., 2015) . In a supersaturated solution, the equilibrium is not observed. This happens because the mean concentration of the solution is constant at a given temperature, but there are local concentration fluctuations, originating ordered microregions or clusters. The cla ssical nucleation theory assumes that clusters are formed in the solution by an addition mechanism that continues until a critical size is reached. Therefore, the new crystalline nucleus is only generated when an initial cluster of solute reaches a minimum , critical size , and this critical size is related to a critical Gibbs free energy change . Equation 29 represents the rate of nucleus formation , where ùê¥ is the preexponential factor and ‚àÜùê∫ùëêùëüùëñùë° is the critical Gibbs free energy change (WENG et al., 2020) .", "For the homogeneous nucleation , it was considered that the free energy change for the formation of the crystalline phase is the sum of the free energy change for the formation of the nucleus surface (‚àÜùê∫ùëÜ) and the free energy change for the phase transformation (‚àÜùê∫ùëâ). The first term con sists of a positive quantity , while the second one consists of a negative quantity (NANEV, 2020) . Figure 8 illustrates the free energy changes and their sums for a spherical nucleus . From this figure , it can be concluded that clusters greater than the crit ical size result in a decrease in free energy and will participate in the nucleation process (YANG et al., 2021) .", "Figure 8: Free energy diagram . Adapted from Myerson and G inde (2019) .", "The heterogeneous nucleation is characterized by the presence of a foreign substance in the supersaturated solution, which reduces the energy required for nucleation. Therefore, the free energy barrier is lower in the heterogeneous system than in the homog eneous system, and the nucleation in a heterogeneous system usually happens at a lower supersaturation (JIA et al., 2016) . Equation s 30 and 31 were developed by Volmer (1939) and demonstrate that the decrease in free energy depended on the contact angle of the solid phase .", "Different from primary nucleation, secondary nucleation only happens in the presence of other crystals in the supersaturated solution. The crystals present in this solution have a catalyzing effect on the nucleation phenomena . Therefore, secondary nucleation occurs at a lower supersaturation than needed for spontaneous nucleation (MYERSON and G INDE, 2019) .", "The other step of the crystallization process consists of crystal growth, which is the grow th of the nuclei by the addition of solute molecules from the supersaturated solution . This phenomenon is described by the change of some dimension (ùêøùëñ) in time, represented by Equation 32. ùê∫ùëñ is known as the linear growth rate (LEWIS et al., 2015) .", "While studying crystal growth, it is important to focus only on a single -phase or plane and analyze the growth of that part. The mechanism for incorporation of a molecule into a crystal face consists first of its adsorption onto the surface . Then, its diffusion along the surface to a step or kink site for incorporation. It is important to have in mind that molecules tend to bond at locations where there are the most neighbors possible because these are the most energetically favorable sites (MYERSON and G INDE, 2019) . Besides nucleation and crystal growth, some other phen omena can occur in crystallization. One of them is breakage, which is usually a consequence of applied tensions between a crystal and the walls of the crystallization vessel , or between different crystals (FASOLI and CONTI, 1973) . Another mechanism is call ed aggregation and consists of multiple crystals stuck together when they collide (BERGLUND, 2019) . This situation can bring some issues to the process, such as purity problems for the final product (BRUNSTEINER et al., 200 5). The p olymorphic transformations are from a crystal shape to another .", "Another important concept that must be presented is seeding , which is commonly used to initiate the crystallization process . Seed crystals are added to a supersaturated solution in the metastable region . The added crystals promote crystal growth within the metastable region . The seeding technique is usually applied to improve the reproducibility of the product across different batches. The initial seed may have some characteristics, such as high purity, with well -defined shapes and faces , and introduced into the solution apart from one another (BARRET et al., 2005).", "Finally, all mechanisms explained in this topic are illustrated by Figure 9.", "Figure 9: Crystallization mechanisms . Adapted from Moraes (2019) .", "2.2.2. Particle Size Distribution", "While studying a crystallization process, particles with different characteristics are obtained. In systems like this, it is important to describe the behavior of the population of the particles and their environment, using statistical density functions to a representative variable, such as number , mass, or volume. Therefore, a large particle‚Äôs population in a crystallization process will display a non -random behavior since the behavior of individual particles will be averaged out (RAMKRISHNA, 2000) . First, some terms must be defined:", "‚Ä¢ ùëü‚â°(ùëü1,ùëü2,ùëü3) is the vector containing the external positions of the particles, namely their position in space; ‚Ä¢ ùë•‚â°(ùë•1,ùë•2,‚Ä¶,ùë•ùëõ) is the vector containing the internal coordinates of the particles, representing n quantities associated with the particle ; ‚Ä¢ Œ©ùëü is the domain of external coordinates; ‚Ä¢ Œ©ùë• is the domain of internal coordinates.", "The particle population is randomly distribut ed along with the two domains. However, the important information is the average behavior and the number density function (ùëõ(ùë•,ùëü,ùë°)) is used to get it. The number density function defines the number of particles in a given subspace of the particle coordinate system . Equation 33 represents the total number of particles in the entire system . Also, the total number of particles per unit volume of physical space is given by Equation 34 (RAMKRISHNA, 2000) .", "For the case of K 2SO 4 crystallization, only one internal variable is considered, which is the particle‚Äôs size (ùêø). Therefore, the ùëñùë°‚Ñé moment of the p article size distribution is defined in order to make use of the number density function ùëõ(ùë•,ùëü,ùë°) as represented by Equation 35 (RANDOLPH, 1971) .", "The subscript ùëñ indicates the moment order. Some relevant information can be acquired while analyzing the moments of different orders. Equation 36 represents ùúá0(ùë°), which consists of the total number of particles per suspension volume . The relation ùúá1(ùë°)ùúá0(ùë°) ‚ÅÑ is demonstr ated by Equation 37 and is the number average particle size . Also, Equation 38 illustrates the relation ùúá2(ùë°)ùúá0(ùë°) ‚ÅÑ , which is the number average particle surface . Finally, the relation ùúá3(ùë°)ùúá0(ùë°) ‚ÅÑ is presented by Equation 39 and consists of the number average particle volume .", "2.2.3. Population Balance Model", "The population balance equation was proposed by Randol ph and Larson (1971 ) and describes the behavior of the size distribution of a crystal‚Äôs population through time as a result of several kinetics processes. The population balance takes into account all the kinetics processes that are responsible for the generation and distribution of crystals in different sizes (COSTA and GIULIETTI , 2012) .", "The one -dimensional population balance equation for a well -mixing batch crystallizer is represented by Equation 40. For this case, some considerations were taken into account, such as the constant volume of the batch system and size -independent growth . Furthermore, this equation doe s not consider the effects of breakage and agglomeration (KANG et al., 2022) .", "Equation 40 presents some terms, such as the growth rate along with the internal coordinate L (ùê∫(ùë°,ùúÉùëî)), the nucleation rate (ùêµ(ùë°,ùúÉùëè)), and the dissolution rate (ùê∑(ùë°,ùúÉùëë)). Also, ùëì(ùêø,ùë°) is the one -dimensional crystal size distribution, ùêøùëõ is the size of the nuclei and ùõø(ùêø‚àí ùêøùëõ) is the initial nuclei crystal size distribution described with the Dirac delta function . Finally, ùúÉ indic ates the vectors of growth, nucleation, and dissolution kinetic parameters respectively for the subscripts ùëî,ùëè and ùëë (KANG et al., Equation 40 is a differential equation and needs some boundary and initial conditions to be solved, which are represented by Equations 41 to 43 (KANG et al., 2022) .", "lim", "The standard method of moments is one strategy used to solve the population balance. The definition of the moment was presented in the previous topic by Equation 35. The standard method of moments has been applied in many articles to solve the Population Balance Model. One example is the work of Moraes et al. (2021 a), which use s the method of moments for a bivariate popul ation balance model and is applied to the control of a crystallization process.", "2.3. Model Predictive Control (MPC)", "The first applications of model predictive control were developed at the end of the 70s by two independent industrial groups: Shell (CUTLER and REMAKER, 1979) and ADERSA (RICHALET et al., 1978). This new approach allowed chemical processes with high complexity to be controlled with more efficiency. This also stimulated researchers to improve this control strategy. Therefore, since 1970 a lot of new control algorithms were developed using an internal model of the process that could predict the system‚Äôs behavior in order to account for this information for the application of the control action (SEBORG et al., 2011) .", "The idea used in the model predictive control approach is represented in Figure 10. In this illustration , ùë¶ is the actual output or actual control variable, ùë¶ÃÇ is the predicted output, ùë¢ is the manipulated input or manipulated variable, ùëÄ is the control horizon and ùëÉ is the prediction horizon. Therefore, at a ùëò sampling instant, the model predictive controller calculates a set of ùëÄ input values, consisting of current input ùë¢(ùëò) and ùëÄ‚àí1 future inputs. This set of inputs is calculated in order to make a set of ùëÉ predicted outputs reach the set point in an optimal way by the optimization of an objective function (SHI and ZHANG, 202 2b).", "Figure 10: Model predictive control strategy . Adapted from Seborg et al. (2011) .", "As exposed before, a set of ùëÄ control actions is calculated at each sampling time, but only the first action is implemented. Then , a new sequence is calculated at the next sampling time, and only the first input action is implemented. This approach is applied at each sampling time, and it is known as the receding horizon strategy (KAMAL and CHOWDHURY, 2022) .", "The structure of a model predict ive controller is illustrated in Figure 1 1. As represented in the picture, first the model is fed with the past inputs and outputs, and the future inputs, which are given by the optimizer. Therefore, the model uses this information to predict future output variables values (ZHOU and DU, 2021) . The predictions are compared to a reference trajectory and, then, the future errors are calculated (MORATO et al., 2020) . The optimizer minimizes the objective function obtained based on the calculated errors and takes into account the process constraints. The optimizer obtains the future control actions, which are u sed by the model, and the first control action is appli ed to the process (ZHANG et al., 2022 b).", "Figure 11: Model predictive control structure.", "Equation 44 represents the objective function, which can be divided into two parts. The first term considers the deviations from forecasts and is calculated by the sum of the square deviations over the prediction horizon mult iplied by Œ¥(j), the term used to penalize the outputs. The second part is referent to the control actions, w hich is calculated by the sum of the square increment of the control actions multiplied by Œª(j), the term used to penalize the inputs. Also, ùëÅ1 is the inferior prediction horizon, ùëÉ is the prediction horizon, ùëÄ is the control horizon (CAMPOS et al., 2013).", "The model predictive control can take into account the constraints of the process as presented by Equations 45 to 47, in which ùë¢ is the control action an d ùë¶ is the output variable. This is one of the advantages of the MPC strategy because the controller will only operate in viable regions for the process (CAMPOS et al., 2013).", "Some different approaches have been proposed to apply model predictive control, using several models to predict the dynamic beha vior of the process, such as a transfer function relating the inputs and outputs. A strategy that has been applied a lot by the literature is the use of neural networks as the model of the controller (SCHWEIDTMANN et al., 2021). This approach consists of a specific type of the MPC, called Nonlinear Model Predictive Control (NMPC). Figure 12 demonstrates the structure of this strategy. The future control actions and the past inputs and outputs are used to feed the neural network. Then, the neural network use s this information to forecast future outputs (WU et al., 2021b). The rest of the controller‚Äôs operation is the same as explained before. One example of the use of neural networks and NMPC combination is exposed in the study of Souza Jr. et al. (1996), app lying this strategy to control a chaotic polymerization reactor.", "Figure 12: Model predictive controller scheme using neural network.", "Several works in the literature review the MPC concepts, presenting more details of this topic. Three examples of these studies are the works of Qin and Badgwell (1997), Qin and Badgwell (2003), and Badgwell and Qin (2019). These studies expose a brief history of MPC, different algorithms of M PC, and its applications in industries.", "2.4. Control and ANN Applications for Crystallization", "Process es This item does not intend to present an extensive overview of control of crystallization processes. These can be found in the literature (RAWLINGS et al., 1993; BRAATZ, 2002; NAGY and BRAATZ, 2012). As the focus of the present work is on the use of neural networks for crystallization, the works chosen to be presented and analyzed here all employ data -based approaches.", "There are some applications in the literature presenting crystallization process control, such as Griffin et al. (2016). In this study, a data -driven model -based approach was developed to control the average size of crystals produced by batch cooling cryst allization. They used a reduced representation of the system state containing two variables, the crystal mass, and the chord count. These variables can be measured online and represent the system's important characteristics. The model formulation took into account Markovian dynamics, in which the system dynamics depend on only the current state and input (supersaturation) to the process. The method of least squares was used to learn the function relating the current system state and input to the next step b y selecting one amongst a pool of candidate functions. These candidate functions are restrained to a set of 6th order polynomials concerning the supersaturation. As soon as the function is learned, it is combined with dynamic programming to obtain optimal control policies for producing crystals of targeted average sizes in a prespecified batch run times. This proposal was tested on a laboratory scale setup for the crystallization of ùëÅùëé 3ùëÜùëÇ4ùëÅùëÇ 3.ùêª2ùëÇ from an aqueous solution containing ùëÅùëé+, ùëÜùëÇ4‚àí and ùëÅùëÇ3‚àí. Grover et al. (2020) adopted a similar approach to Griffin et al. (2016), using it to two different processes: the self -assembly of a colloidal system and the batch crystallization of paracetamol. The crystal mass and chord count were chosen as controlled variables, while supersaturation was manipulated. Furthermore, a Markov State Process (MSP) was formulated and learned from 14 experimental runs, containing 7402 samples. The quality of the learned model was evaluated by a bootstrapping method by retrainin g it on subsets of the complete dataset, with slight variation in the final model parameters. Finally, using the MSP model developed, dynamic programming was used to identify optimal feedback control policies for producing crystals of target mean sizes. Moraes et al. (2021 a) proposed a control strategy to obtain the optimum values of temperature and supersaturation for controlling the crystal mass, size, and shape. A bivariate population balance model was developed for the crystallization of potassium dihyd rogen phosphate (KDP). This model considered the mechanisms of crystal nucleation, growth, dissolution, and disappearance. The proposed model was analyzed in open -loop experiments, efficiently predicting the mean characteristics, lengths, and number of par ticles for the supersaturated and unsaturated zones. Finally, a control scheme was developed for the batch cooling crystallization of KDP, solving a dynamic optimization problem.", "Salami et al. (2021) proposed an image analysis -based approach to live monito ring the crystallization process. A convolutional neural network was trained using a dataset of in situ microscope images of the cephalexin crystallization developed in a laboratory setup. The images were separated into two classes, defined as ‚ÄúGood‚Äù for p ure cephalexin crystals in the slurry and ‚ÄúBad‚Äù for the predominant presence of phenylglycine in the slurry. Therefore, the proposed neural network strategy examin ed the images, detect ed and communicate d undesired crystals in the slurry, and estimate d wher e the undesired product was located for the operator.", "Moraes et al. (2018) developed a model predictive controller for controlling mixed suspension in evaporative crystallization. The model was defined for a potassium sulfate crystallization and is describ ed by the Population Balance Equations with moment equations, solute mass balance, and energy balance. This approach aimed to control the feed solute concentration and the feed flow rate, manipulating the temperature and the steam outflow rate. The MPC was compared to PI controllers, observing better performance for the proposed approach.", "Moraes et al. (2021 b) proposed a control strategy f or supersaturation in the crystallization process. The temperature policy was obtained for an experimental setup of the potassium sulfate crystallization. Three different supersaturation levels were defined for 60 minutes of batches, using seed s with a mean size of 170 ùúám and an initial concentration of 0.1587 ùëîùëêùëö3‚ÅÑ of potassium sulfate. The proposed model for temper ature policies was efficient for the optimum control of the batch cooling crystallization of potassium sulfate.", "Different from the other cases exposed, there are some cases in the literature for modeling the crystallization process using neural networks. N ielsen et al. (2020) proposed a hybrid neural network and population balance modeling approach for particle processes. A soft sensor that predict ed particle phenomena kinetics was developed based on machine learning strategies. The dataset used to train th is application consisted of online sensor data on particle size distribution. The soft sensor was combined to a first -principles population balance. The phenomena considered to model the process were nucleation, growth, shrinkage, agglomeration, and breaka ge. The kinetic rates of these phenomena were estimated using a neural network and integrated into the population balance equations. The proposed approach was tested on a laboratory scale lactose crystallization setup, a laboratory scale flocculation setup , and an industrial scale pharmaceutical crystallization unit.", "Similarly to the Nielsen et al. (2020) study, many other studies in the literature propose hybrid modeling frameworks that apply neural networks combined to the population balance. Some example s are the works developed by Lauret et al. (2001), Galvanauskas et al. (2006), Meng et al. (2019), and Kim et al. (2018). In these cases, the proposed models have focused on a process variable that h as been historically available at a high measurement rate, namely the crystal mass. However, particle size distributions acquired with more sophisticated analytical methods were not considered for the model development.", "2.5. Contributions of the Present Study to the Literature", "The present work proposes a neural networks -based approach for the dynamic modeling of crystallization processes . However, different ly from the exposed cases, the current approach is not of the hybrid type. The neural network is used to directly predict the moments of the particle distribution, instead of predicting kinetic rates that are later incorporated into PB models as in .", "The proposed approach applies innovative strategies. The first one consists of four MLP in series based o n the Population Balance Model. The other case involves the application of recurrent neural networks. This approach has been applied in recent MPC studies (WU et al., 2019) and is a good option to model dynamic processes, such as batch cooling crystallization.", "Another difference between this study consists of the neural networks application for controlling the crystallization process. The neural networks have been use d only to model the crystallization process or fault detection and diagnosis. The model predictive controller has been developed using different approaches, such as Population Balance Models. Therefore, different from the other studies, the present work applies the n eural networks as the internal model of the predictive controller for producing crystals with the desired size. Also, there is a very limited number of applications of ESN to MPC, demonstrating once again the innovative character of the present study .", "Chapter 3", "Methodology", "This chapter‚Äôs purpose is to introduce the methodology used in this study. Therefore, the experimental setup employed to carry the batch crystallization process, and the strategy used to implement the Population Balance to model th is process are presented. The approaches used to prepare the dataset to train and test the neural networks and to apply the model predictive controller are also explained. Furthermore, the approach used to develop the neural networks is shown.", "3.1. K2SO 4 Crystallization Experiments", "The experimental results were obtained by Moraes (2019) and Moraes et al. (2019) . The experimental methodology described here is also based on the same reference s. The batch crystallization experiments were performed i n an apparatus in LABCA DS/UFRJ presented in Figure 13 . The apparatus consists of this set of equipment:", "‚Ä¢ Syrris¬Æ 1 L crystallization vessel ; ‚Ä¢ Orb¬Æ agitator (0 -700 rpm) ; ‚Ä¢ QICPIC Sympatec¬Æ image analyzer integrated with LIXELL accessory for image acquisition in the liquid phase ; ‚Ä¢ Peristaltic pump Masterflex¬Æ L/S, responsible for the recirculation of the sample volume from the analyzer back to the crystallization vessel ; ‚Ä¢ Huber¬Æ Petite Fleur thermostatic bath.", "Figure 13: Apparatus used to perform the batch crystallization (MORAES , 2019) .", "Ten experiments of the batch crystallization of the potassium sulfate were carried out from an aqueous solution. A solution was prepared by the dissolution of the necessary mass to achieve a given supersaturation in 1 L of water and at 40 ¬∞C. Then , the sol ution was heated to 55 ¬∞C in order to dissolve all salt and cooled to the desired initial temperature for the batch. The process conditions are presented below: ‚Ä¢ Initial temperature: 40 ¬∞C for experiments 0 to 6 and 30 ¬∞C for experiments 7 to 9; ‚Ä¢ Batch time : 60 minutes; ‚Ä¢ Agitator rotation: 400 rpm ; ‚Ä¢ Recirculation (sampling) flowrate: 140 cm3/min; ‚Ä¢ Initial supersaturation: 7 x 10-3 g K 2SO 4/cm3; ‚Ä¢ Mass of initial seeds: 0.25 g ; ‚Ä¢ Seed sizes: ‚â§ 75 ùúáùëö ; ‚Ä¢ Supersaturation mode: step cooling .", "The initial seeds were produced in the laboratory by recrystallization of the commercially available salt at high supersaturation conditions. The crystals generated were cleaned with acetone and filtered in a Buchner funnel. Finally, the classification of the crystal size was ma de by sieving, separating them into specific size ranges. In the following, steps of a magnitude of about -0.3 ¬∞C were carried out by the temperature regulator. This was done in order to achieve a desired final batch temperature, which varied across experi ments. The temperature monitoring was done with a thermocouple placed in contact with the solution. The temperature control was performed with the thermostatic bath, which was connected to the vessel‚Äôs jacket. The crystals were monitored online by an exter nal loop. The suspension was continuously sampled from the top of the main flask, pumped to the image analyzer, and then back to the bottom of the flask. Moreover, the QICPIC image analyzer captured a video of the sampled suspension at a frequency of 100 f rames per second. A sampling time of 1 minute was applied to acquire data. Images from all crystals within the sampled solution were analyzed in order to estimate their size.", "The solution concentration was determined offline by gravimetric analysis. Therefore, several samples of 0.5 mL were obtained from the main flask, filtered, and collected in vials. Finally, the liquid samples were weighted before and after they were dried on a stove at 70 ¬∞C with gentle air convection to remove the solv ent.", "3.2. Population Balance Model for K 2SO 4 Crystallization", "As mentioned before, crystallization processes are usually modeled by Population Balance. In this study, a Population Balance developed by Moraes (20 19) was applied to model the process in th e control loop, while the neural network was used as the model of the predictive controller. The Population Balance applied modeled crystal nucleation and crystal growth by rate equations . Therefore, o ther mechanisms, such as agglomeration and breakage, ar e not considered. E nergy and mass balances are also needed to express the evolution of the properties of the dispersed phase as a function of process variables which can be manipulated . The model is composed of a system of Ordinary Differential Equations d escribing the evolution of each of the moments and the concentration with time, ùúáùëñ(ùë°) for i = 0, 1, 2, 3 and C(t), respectively , as presented in Equations 48 to 56.", "ùëëùê∂", "Where :", "‚Ä¢ ùëé: growth constant [‚àí]; ‚Ä¢ ùëè: growth constant [‚àí]; ‚Ä¢ ùêµ: nucleation rate [#/ùëêùëö3/ùëöùëñùëõ ]; ‚Ä¢ ùê∂: concentration of the solute [ùëî/ùêø]; ‚Ä¢ ùê∂‚àó: concentration of the solute in equilibrium [ùëî/ùêø]; ‚Ä¢ ùê∫ùêº: size independent growth rate [ùúáùëö/ùëöùëñùëõ ]; ‚Ä¢ ùëÜ: supersaturation [ùëî/ùêø]; ‚Ä¢ ùëòùëé: nucleation coefficient [ùúáùëö/ùëöùëñùëõ ]; ‚Ä¢ ùëòùëè: activation energy for nucleation [ùêæ]; ‚Ä¢ ùëòùëê: supersaturation (s) exponent for nucleation [‚àí]; ‚Ä¢ ùëòùëë: ùúá3 exponent for nucleation [‚àí]; ‚Ä¢ ùëò1: activation energy for growth [ùêæ]; ‚Ä¢ ùëò2: supersaturation (s) exponent for growth [‚àí]; ‚Ä¢ ùõº: volume shape factor [‚àí]; ‚Ä¢ ùëá: process temperature [ùêæ]; ‚Ä¢ ùúåùëê: specific weight of the crystal [ùëî/ùëêùëö3].", "The parameters estimated for the Population Balance Model by Moraes (20 19) are presented in Table 1.", "Table 1: Parameters for the Population Balance Model.", "The Population Balance model was implemented in Python using the SciPy library (VIRTANEN et al., 2020). Specifically, the function odeint from this library was used to solve the system of Ordinary Differential Equations . This function applies the lsoda from the FORTRAN library odepack . The tolerances used in this integrator were 1.10‚àí10 for atol and 1.10‚àí8 for rtol.", "3.3. Training and Test Datasets Data from ten different batches were available (M ORAES , 2019; MORAES et al., 2019). The training dataset was defined using the co -teaching learning algorithm. Therefore, the set was composed of one experimental batch and also by simulated data. The simulated data was generated by the application of the Popula tion Balance Model presented in topic 3.2 . The chosen experiment was the one with more results with the initial conditions presented below: ‚Ä¢ Temperature: 39.98 ¬∞C;", "Figure 14 presents the temperature behavior for the training dataset over time. As exposed in the figure, there are three different behaviors for the temperature, meaning three different groups of d ata in the training dataset. The first one occurs from the initial moment until 81.23 min and is composed of a mixture of 61 experimental and 2000 simulated data lines (each data line here indicate a line in the dataset with the values of the temperature a nd of the four moments at a given sampling time), representing what happens when the vessel is cooled. The second group appears from 81.23 min until 141.23 min and is composed of 2000 simulated data lines. This second group was designed in order for the ne twork to learn what happens when the temperature is maintained constant. Finally, the last group occurs from 141.23 min until the end of the batch and is composed of 2000 simulated data lines. This last group was designed to enhance the training dataset. More details of this dataset are presented in Appendix A.", "Figure 14: Temperature over time for the training dataset. The test dataset was also composed of simulated and experimental data, but a different approach from the training dataset was used. The nine batches that were not used in the training dataset and three new simulation runs were used to compose the test dataset. Unlike the training dataset, test one did not mix the 418 experimental data lines and the 3000 simul ated data lines for the analysis. Therefore, the nine batches used to compose the test dataset are called Experiment 1 to Experiment 9, and the simulation runs are named Simulation 1 to Simulation 3. Each Experiment and Simulation is studied apart. The 9 b atches depart from different initial conditions, and some of the m present different temperature profiles. Two batches operate at a constant temperature, which are Experiment 2 that is used a temperature of 40 ¬∞C and Experiment 7 that is applied a temperature of 30 ¬∞C. For the other batches, it is applied a varying process temperature. However, the temperature range is not the same for all cases, which is 30 ‚Äì 40 ¬∞C for Experiments 1, 3, 4 and 6, 20 ‚Äì 40 ¬∞C for Experiment 5, 20 ‚Äì 30 ¬∞C for Experimen t 8, and 15 ‚Äì 30 ¬∞C for Experiment 9. More details of the experimental values are presented in Appendix A.", "Each simulation run of the test dataset is composed of results for pre -defined batch temperature profiles and departs from different initial conditio ns according to the experiments. The temperature profiles for each simulation were defined by linear regression according to the experimental values. More details of this dataset are presented in Appendix A. The conditions applied for the three simulations are shown in Table 2.", "Table 2: Conditions used for the three simulation runs. Conditions Simu. 1 Simu. 2 Simu. 3 Initial Concentration", "3.4. Model Predictive Controller Design", "The controlled variables chosen for the optimization problem were ùúá0, ùúá1/ùúá0, ùúá2/ùúá0, and ùúá3/ùúá0, and the manipulated variable is the process temperature. The controlled variables were chosen to reach the desired length of the crystals. Moreover, the temperature was the input variable because it was the only manipulated variable available. The MPC go al is to minimize the process performance index ùêΩ, the objective function of the problem shown in Equation s 57-61. The parameters ùõø and ùõæ are the weights of the outputs and the increment input, respectively; ùúáùëñ(ùëò+ùëó) is the ith predicted output at time k + j and ùúáùëñùëÖùëá(ùëò+ùëó) its reference value. The parameters ùõø and ùõæ were defined by trial-and-error for both approaches. The input incr ement is defined by ‚àÜùë¢(ùëò). P is the prediction horizon, defined by trial-and-error.", "The optimization constraints are given by Equations 62-64, where C is the concentration of the solute, and ùê∂ùëíùëû is the concentration of the solute in equilibrium. Furthermore, the process temperature was restricted to the range of 15 to 40 ¬∞C because it was the range applied experimentally. The constraint defined by Eq uation 62 was defined in order to avoid sudden changes in temperature . Also, the constraints indicated by Equations 63 and 64 were necessary because the Population Balance model implemented only considers crystal nucleation and crystal growth . Therefore, when the ratio in Equation 63 is higher than 1, the growth of the crystals is observed and the solution is called supersaturated. On the other hand, when this is less than 1, the dissolution of the crystals is seen .", "The control loop scheme is illustrated by Figure 1 5. At each sampling time, ùúá0, ùúá1, ùúá2, ùúá3, and concentration are measured. These measurements come from the Population Balance model which represents here the real process. These results of moments are used as the input of the neural network model, which is the internal model for the controller. Then an iterative procedure is carried out in which the network predi cts the moment values several steps ahead in the future for given future temperature values. The neural network outputs are compared to the desired reference trajectory, composing the objective function. The optimizer applies the Successive Quadratic Progr amming algorithm in order to achieve the optimum temperature value. After convergence, the first control action is implemented. This loop was simulated for a 120 minutes crystallization process, using the language Python. The optimization problem was solve d using the function minimize from the SciPy library (VIRTANEN et al., 2020). The tolerance value was chosen as 1.10‚àí8.", "Figure 15: Control loop scheme.", "3.4.1. Reference Trajectories", "3.4.1.1. Zero -Order Trajectory", "The first trajectory studied was a zero -order one defined by Equation 65, where ùúáùëñ(ùë°ùëíùëõùëë) is the moment i value at the end of the experiment and ùúáùëñùëÖùëá(ùë°) is the value of the reference trajectory in time ùë° for a moment i. Therefore, this is a constant trajectory defined using the experimental value at the end of the batch. Particularly, the ùúáùëñ(ùë°ùëíùëõùëë) values were defined as the moments‚Äô values at the end of Experiment 1 from the test dataset.", "3.4.1.2. First -Order Trajectory The first -order trajectory was defined in an iterative way for each ùúáùëñ by Equations 66 and 67. The parameter ùõæ was chosen by trial -and-error and indicates how fast the final value will be reached.", "3.4.1.3. Second -Order Trajectory", "The second -order trajectory was modeled by a critically damped second -order system and is represented by Equation 68. This approach is based on the study of Fonseca et al. (2004). The parameter ùúèùëñ is the time constant and was defined applying least squares to each of the moments ùúáùëñ using the curve_fit function from SciPy library (VIRTANEN", "Equations 69 ‚Äì 72 demonstrate how ùúáùëñ‚àó,ùëÖùëá(ùë°) was calculated for the second -order reference trajectory.", "3.4.1.4. Adaptive First -Order Trajectory", "Different from the other three trajectories, this one is recalculated at each time step. This trajectory is similar to the first -order defined by Equations 66 and 67, but departs from the actual system state at each time step. The adaptive first -order trajectory is defined by Equations 73 and 74. The superscript m denotes the measured value of the variable , and ùõæ is a constant defined by trial -and-error.", "3.5. Neural Networks Design", "The first two approaches proposed were based on MLP networks, which were developed using the sckit -learn library from Python (PEDREGOSA et al., 2011). The ADAM algorithm trained these neural networks. The following network consisted of an LSTM model and was made using the Keras library from Python (CHOLLET et al., 2015). The application of the ADAM algorithm also trained this strategy. Finally, the ESN model was implemented using the library pyESN developed by Kornd√∂rfer (2015). All hyperpar ameters chosen for the neural networks were defined by trial -and-error.", "Chapter 4", "Results and Discussions", "This chapter presents and discusses the results achieved by this study. First, the adopted Population Balance model is simulated and validated using experimental data. Then, the development and results of the four neural network -based models are explained. In the sequence, the developed nonlinear model predictive controller based on the most efficient neural network strategy is presente d, and its behavior is compared to a model predictive controller based on an MLP network, a more conventional approach. Finally, the controller‚Äôs operation is compared for four different reference trajectories.", "4.1. Population Balance Model Simulation", "The assumed Population Balance model by Moraes (20 19) was simulated in Python as exposed in Section 3.2. Figure 1 6 present s the simulation for a constant temperature profile , and Figure 1 7 illustrates the results for a varying temperature profile. The initial conditions for both simulations were based on the initial conditions for the respective experiment. As both pictures show , the proposed model provides a good fit for the experimental values of ùúá0,ùúá1ùúá0‚ÅÑ, ùúá2ùúá0‚ÅÑ ,ùúá3ùúá0‚ÅÑ and concentration. The employed model efficiently predicts the behavior of all these variables for constant and varying temperature. Some differences between the model and the experimental values can be seen. The oscillation in ùúá1ùúá0‚ÅÑ for a constant temperature can be explained by the occurrence of inherent experimental errors. Additionally, also for the constant temperature, the profile for ùúá0 is slightly below the experimental points, but follows the same underdamped growing tendency.", "Figure 16: Simulation of the PB model and comparison to experimental data for a constant temperature profile.", "Figure 17: Simulation of the PB model and comparison to experimental data for a varying temperature profile.", "4.2. Neural Network -Based Model s", "4.2.1. MLP Model", "A model based on MLP network was first designed using ùúá0,ùúá1,ùúá2,ùúá3 and ùëá in time ùêæ as inputs to predict the values of ùúá0,ùúá1,ùúá2 and ùúá3 in time ùêæ+1. The structure of the proposed model is represented by Figure 1 8.", "Figure 18: MLP network design.", "The hyperparameters of the proposed network were analyzed in detail, testing different number of neurons and hidden layers, activation function, batch size, and initial learning rate. The hyperparameters that provided most efficient MLP network after an extensive trial -and-error procedure are presented in Table 3.", "Table 3: MLP model hyperparameters. Number of hidden layers 1 Number of neurons in the hidden layer 400 Activation function Relu Batch size 350 Solver Adam Initial Learning Rate 0.1", "After defining the MLP model, its potential to predict the moments‚Äô values more steps ahead was tested, as demonstrated in Figure 19. The number of steps ahead (ùëõ) studied was 2 and 5.", "Figure 19: MLP structure to predict the moments' values ùëõ steps ahead.", "4.2.2. MLP in Series", "The second proposed model applied a strategy of four MLP networks in series. It was based on the Population Balance model, so it would be expected to have a nature of a hybrid model. The proposed scheme is represented in Figure 20. The first network of the series is inspired in Equation (48); the second one in Equation (49); the third in Equation (50) and the fourth in Equation (51). The expectation is that, based on these inputs, the multilayered network would be able to represent the additi onal complexities of the process.", "Figure 20: MLP network in series design. The first network uses ùúá0 and ùëá in time ùêæ to predict ùúá0 in time ùêæ+1. Then, the predicted ùúá0 is used to feed the second network with ùúá1 and ùëá in time ùêæ+1 to predict ùúá1 in time ùêæ+2. After this, the third MLP network is fed with ùúá2 and ùëá in time ùêæ+2, and the predicted ùúá2 to predict ùúá2 in time ùêæ+3. Finally, the predicted ùúá2 is used as in put for the fourth network with the values of ùúá3 and ùëá in time ùêæ+3 to predict ùúá3 in time Different hyperparameters were tested to achieve the most efficient networks, changing the number of neurons and hidden layers, activation function, batch s ize, and initial learning rate. The hyperparameters that provided the MLP networks with the best results are presented in Table 4 and they were obtained after an extensive trial -and-error procedure. The same neural networks‚Äô structure was adopted for all f our cases.", "Table 4: Four MLP in series hyperparameters. Network First Second Third Fourth Number of hidden layers 1 1 1 1 Number of neurons in the Activation function Relu Relu Relu Relu Batch size 50 50 50 50 Solver Adam Adam Adam Adam Initial Learning Rate 0.1 0.1 0.1 0.1", "After choosing the best hyperparameters for the four networks, their performance to predict the moments‚Äô values more steps ahead was tested, illustrated in Figure 21. The number of steps ahead (ùëõ) studied was 2 and 5. One thing illustrated by the picture is that only the predicted moment value one step ahead is used as input for the following network. The other inputs of the following networks were based on the experimental values.", "Figure 21: MLP in series structure to predict the moments' values n steps ahead.", "4.2.3. LSTM Model", "The third designed model was based on a recurrent neural network, specifically the LSTM network. The inputs used for this model were ùúá0,ùúá1,ùúá2,ùúá3 and ùëá in time ùêæ to predict the values of ùúá0,ùúá1,ùúá2 and ùúá3 in time ùêæ+1. The structure of the proposed model is represented by Figure 22.", "Figure 22: LSTM network design.", "The LSTM hyperparameters were studied, testing different cells and LSTM layers, activation function, recurrent activation function, and batch size. Some parameters had to be considered to achieve the desired performance, such as kernel regularizers, bias initializers, kernel initializers, and recurrent initializers. Also, the initializers were analyzed to achieve a stable performance for the network, and the regularizers were used. Regularizers were used because the R -squared values for test and training sa mples were higher with it and to reach a constant network performance. The output layer used a time distributed layer to apply a layer to every temporal slice of an input. The hyperparameters that provided the best LSTM network are presented in Table 5. Again, trial -and-error was adopted.", "Table 5: LSTM network hyperparameters. Number of LSTM layers 1 Number of cells in the LSTM layer 120 Activation function Tanh Recurrent activation function Sigmoid Batch size 250 Solver Adam Regularizer L1 0.0001 Regularizer L2 0.0001 Bias Initializer Constant (0.5) Kernel Initializer Glorot Uniform (seed equals to 1) Recurrent Initializer Orthogonal (seed equals to 1)", "The LSTM operation to predict the moments‚Äô values more steps ahead was tested, and the inputs and outputs of the network are presented in Figure 23. Different from the other cases, the LSTM network needs ùëõ inputs in the past to predict the outputs ùëõ steps ahead to apply the backpropagation through time algorithm. The number of steps ahead (ùëõ) studied was 2 and 5.", "Figure 23: LSTM inputs and outputs to predict the moments' values n steps ahead. 4.2.4. ESN Model The last model designed was also based on recurrent neural networks, but this time applied ESN network. The inputs used for this model were ùúá0,ùúá1,ùúá2,ùúá3 and ùëá in time ùêæ to predict the values of ùúá0,ùúá1,ùúá2 and ùúá3 in time ùêæ+1. The structure of the proposed model is represented by Figure 2 4.", "Figure 24: ESN network design.", "The hyperparameters of the ESN were analyzed in detail, testing different number of reservoirs, activation function in the output layer and in the output recurrence, and random state rate. The hyperparameters that provided the most efficient ESN network ar e presented in Table 6. Several sets of hyperparameters were tested in a trial -and-error procedure.", "Table 6: ESN hyperparameters. Number of neurons 50 Activation function in output layer Identity Activation function in output recurrence Identity Random State 1 Noise 0.5", "The prediction performance of the ESN was studied to predict the moments‚Äô values 2 and 5 steps ahead, shown in Figure 2 5. Different from the LSTM network, the ESN does not need ùëõ inputs in the past to predict the outputs ùëõ steps ahead because it applies a different training algorithm as explained in section 2.1.2.2.", "Figure 25: ESN inputs and outputs to predict the moments' values ùëõ steps ahead.", "4.2.4. Comparison between the Models", "Figure 26 presents the R -squared values for all four proposed models for predictions one step forward. Therefore, all neural networks design could efficiently predict the moments‚Äô values one step ahead, reaching over 0.9 for almost all cases. Only for the single ML P and Experiment 7 is observed a value below 0.9.", "Figure 26: R-squared values for the four models and predictions one step forward.", "The performance of the proposed neural networks was analyzed for predictions two steps ahead, and these results are shown in Figure 27. The results for this case were a bit worse compared for predictions one step forward. However, the four models could efficiently predict the moments‚Äô values two steps forward, achieving R -squared values over 0.8 fo r all cases.", "Figure 27: R-squared values for the four models and predictions two steps forward.", "Finally, the four neural network paradigms were studied for predictions five steps forward, and the results are illustrated in Figure 28 , which the blue dashed line indicates an R -squared value of 0.75 . The pr ediction horizon was limited to five because the performance of the networks deteriorates at this point. The models based on MLP and LSTM networks did not pres ent good performance for predictions five steps forward, reaching R -squared values below 0.6 for some cases . The MLP in series could efficiently predict the moments‚Äô values, presenting R -squared values over 0.7 for all cases. However, the ESN showed the best performance, reaching R -squared values over 0.75 for all cases. Therefore, the model based on ESN was chosen to be the model of the predictive controller.", "Figure 28: R-squared values for the four models and predi ctions five steps forward.", "4.2.5. Controller‚Äôs Model Design", "The model design presented on the previous topics uses a single network to predict the future moments' values. However, this strategy did not prove to be efficient to be applied as the model of the predictive controller. The controller based on a single network to predict the moments' values could not maintain the controlled variables in the reference trajectories. Therefore, a different approach was used for the controller, applying fou r neural networks developing a controller based on four ESNs and another controller based on four MLP networks. Both strategies use networks with the same inputs and outputs. Tables 7 and 8 show the mean squared error (MSE) values calculated for both appro aches for a zero -order reference trajectory and normalized variables , using a linear normalization. As can be observed, the MSE values were smaller for the controller based on four networks.", "Table 7: MSE for the control approaches for ESN. Approach ùùÅùüé ùùÅùüè/ùùÅùüé ùùÅùüê/ùùÅùüé ùùÅùüë/ùùÅùüé Sum", "Table 8: MSE for the control approaches for MLP. Approach ùùÅùüé ùùÅùüè/ùùÅùüé ùùÅùüê/ùùÅùüé ùùÅùüë/ùùÅùüé Sum", "The first network proposed uses ùëá and ùúá0 in time ùêæ to predict ùúá0 in time ùêæ+1 to ùúá0 in time ùêæ+ùëõ. The hyperparameters for both structures were analyzed in detail, testing different activation functions, number of neurons, initial learning rate, and others. The selected hyperparameters for ESN are presented in Table 9 and for MLP are shown in Table 1 0.", "Table 9: ESN to predict ùúá0 hyperparameters. Number of neurons 50 Activation function in output layer Identity Activation function in output recurrence Identity Random State 1 Noise 0.5", "Table 10: MLP to predict ùúá0 hyperparameters. Number of hidden layers 1 Number of neurons in the hidden layer 50 Activation function Relu Batch size 20 Solver Adam Initial Learning Rate 0.1", "The results for the ESN and the MLP for predictions of the ùúá0 values one, two, five and ten steps ahead are presented in Figure 29 . The proposed model s were efficient for predictions one and two steps forward , achieving values of R -squared higher than 0. 9 for all e xperiments and simulations. However, the result was not the same for predictions five steps forward, reaching values of R-squared values below 0.8 for some cases . The results for ten steps ahead are represented in this case to prove the degradation of the forecast with the increase of the horizon.", "Figure 29: R-squared values for MLP and ESN that predict ùúá0.", "The second strategy analyzed predicts ùúá1 in time ùêæ+1 to ùúá1 in time ùêæ+ùëõ. Therefore, ESN and MLP network use ùúá0, ùúá1 and ùëá in time ùêæ as inputs to do it. Different hyperparameters for both structures were tested, and the selected ones for ESN are presented in Table 11 and for MLP are shown in Table 12.", "Table 11: ESN to predict ùúá1 hyperparameters. Number of neurons 50 Activation function in output layer Identity Activation function in output recurrence Identity Random State 1 Noise 0.5", "Table 12: MLP to predict ùúá1 hyperparameters. Number of hidden layers 1 Number of neurons in the hidden layer 295 Activation function Relu Batch size 50 Solver Adam Initial Learning Rate 0.1", "The R-squared values for MLP and ESN for predictions of the ùúá1 values one, two and five steps ahead are presented in Figure 30 . Both model s efficient ly predicted ùúá1 one and two steps forward , achieving values of R -squared higher than 0. 9 for all experiment s and simulations. For predictions five steps ahead, the result was different, reaching values of R-squared below 0.9 for some cases but all higher than 0.7 2.", "Figure 30: R-squared values for MLP and ESN that predict ùúá1.", "The third structure of neural networks studied has the goal to predict ùúá2 in time ùêæ+1 to ùúá2 in time ùêæ+ùëõ. Therefore, this approach uses ùúá1, ùúá2 and ùëá in time ùêæ as inputs to do it. The hyperparameters for both structures were analyzed in detail, and the selected ones for ESN are presented in Table 13 and for MLP are shown in Table 14.", "Table 13: ESN to predict ùúá2 hyperparameters. Number of neurons 50 Activation function in output layer Identity Activation function in output recurrence Identity Random State 1 Noise 0.5", "Table 14: MLP to predict ùúá2 hyperparameters. Number of hidden layers 1 Number of neurons in the hidden layer 300 Activation function Relu Batch size 50 Solver Adam Initial Learning Rate 0.1", "The results for ESN and MLP for predictions of the ùúá2 values one, two and five steps ahead are presented in Figure 31 . The model s presented success predicting ùúá2 one and two steps forward , reaching values of R -squared higher than 0. 9 for all experiments and simulations. However, the model s were not that efficient for predictions five steps ahead, achieving R-squared values between 0.75 and 0.9 for some cases.", "Figure 31: R-squared values for MLP and ESN that predict ùúá2.", "The last neural network‚Äôs design applied in the control scheme is used to predict ùúá3 in time ùêæ+1 to ùúá3 in time ùêæ+ùëõ with ùúá2, ùúá3 and ùëá in time ùêæ as inputs. The selected hyperparameters for ESN are presented in Table 15 and for MLP are shown in Table 16.", "Table 15: ESN to predict ùúá3 hyperparameters. Number of neurons 50 Activation function in output layer Identity Activation function in output recurrence Identity Random State 1 Noise 0.5", "Table 16: MLP to predict ùúá3 hyperparameters. Number of hidden layers 1 Number of neurons in the hidden layer 300 Activation function Relu Batch size 50 Solver Adam Initial Learning Rate 0.1", "The R-squared values of the ESN and MLP for predictions of the ùúá3 values one, two and five steps ahead are presented i n Figure 32. Both models reached R-squared values higher than 0. 9 for predictions one and two steps forward in all cases. However, they exhibited a worse performance for pr edictions five steps ahead, reaching R -squared values around 0.8 for some cases.", "Figure 32: R-squared values for MLP and ESN that predict ùúá3. 4.3. Model Predictive Controller As represented in Equation 61, the parameter ùõø was tuned as 21 for ùúá0, 2.0 for ùúá2/ùúá0, and 0.5 for the other variables for the controller applying the ESN as model. For the controller using MLP, ùõø was tuned as 31.5 for ùúá0, 3.0 for ùúá2/ùúá0, and 1 .5 for the other variables. Also, the parameter ùõæ was tuned as 0.7 for ùúá0 and 0.1 for the other variables for both controllers. A control horizon of 1 and a prediction horizon of 5 were assumed. The initial conditions used in the control loop were based o n Experiment 1 of the test dataset.", "4.3.1. Zero -Order Reference Trajectory Results", "The control loop was a pplied using a zero -order reference trajectory for the ESN and MLP approaches, and their results are shown in Figures 33 and 34, respectively. Both structures could efficiently achieve the reference trajectory when looking at the curves , but ESN showed a better performance th an MLP. The structures had similar results for the variables ùúá0. However, for the ratio ùúá2ùúá0‚ÅÑ there was a higher offset for the controller based on MLP networks. Moreover , no offset is seen for the ratio ùúá3ùúá0‚ÅÑ using the controller based on ESN, while the controller based on MLP presents an offset for this variable. ESN showed a slightly better result for the ratio ùúá1ùúá0‚ÅÑ because MLP presented a small offset for this case.", "Figure 33: Zero -order reference trajectory results for ESN model.", "Figure 34: Zero -order reference trajectory results for MLP model.", "4.3.2. First -Order Reference Trajectory Results The controller was tested for a first-order reference trajectory . The results for ESN are illustrated in Figure 35 and for MLP are shown in Figure 36. The parameter Œ≥ in Equation 6 7 was defined as 0.9. Both structures presented good results while analyzing the curves , but ESN presented a better behavior once more . The results for this case were similar for the zero -order trajectory with success maintaining the variables in the reference trajectory for ùúá0 and ùúá1ùúá0‚ÅÑ. Furthermore , the controller based on ESN demonstrated a more efficient operation than the one based on MLP for the ratios ùúá2ùúá0‚ÅÑ and ùúá3ùúá0‚ÅÑ, observing the offset problems again for the MLP.", "Figure 35: First-order reference trajectory results for ESN model.", "Figure 36: First-order reference trajectory results for MLP model.", "4.3.3. Second -Order Reference Trajectory Results First, the parameter ùúèùëñ has to be defined for each ùúáùëñ as presented in Section 3.4.1.3. The calculated ùúèùëñ by least squares to each of the moments ùúáùëñ is presented in Figure 37, which ùúèùëñ is equal to 21.1, 20.4, 17.5, and 16.4 for ùëñ equals to 1, 2, 3 and 4, respectively. The experimental values used to do this are from Experiment 1 presented in Appendix A.", "Figure 37: 2nd order reference trajectory defined for each moment.", "The controllers‚Äô results for a second -order reference trajectory are shown in Figure 38 for ESN and in Figure 3 9 for MLP. Different from the other cases, both controllers presented some difficulties to maintain the variables ùúá0 and ùúá1ùúá0‚ÅÑ in the reference trajectory. However, the controller based on ESN left ùúá0 closer to the reference trajectory than the MLP approach. Also, final errors could be seen for the ratios ùúá2ùúá0‚ÅÑ and ùúá3ùúá0‚ÅÑ for both approaches. Therefore, the controller based on MLP network presented a slightly worse performance for this traject ory.", "Figure 38: Second -order reference trajectory results for ESN model.", "Figure 39: Second -order reference trajectory results for MLP model.", "4.3.4. Adaptive First -Order Reference Trajectory Results The results for ESN are shown in Figure 40 and for MLP are illustrated in Figure 41 for an adaptive first -order reference trajectory . The parameter Œ≥ in Equation 74 was defined as 0.6. These two approaches presented a good operation to achieve the reference trajectory when looking at the curves . The controller based on the ESN presented offset values for the variables ùúá0 and ùúá1ùúá0‚ÅÑ, while this is not observed for the other approach. On the other hand, the controller based o n MLP presented an offset for the variable s", "Figure 40: Adaptive first -order reference trajectory results for ESN model.", "Figure 41: Adaptive first -order reference trajectory results for MLP model.", "4.3.5. Control Approaches Comparison", "The MSE was calculated for both controller approaches and all four reference trajectories , considering all normalized variables . These results are shown in Table 17 for ESN and Table 18 for MLP. Therefore, the controller based on ESN presented a better performance because it achieved lower values of MSE for most cases. The controller based on MLP only showed a better performance f or the variable ùúá0 for three trajectories , and for the variable ùúá3/ùúá0 for one trajectory . However, the ESN approach showed a better performance for all four reference trajectories considering the sum of all four variables.", "Table 17: MSE values for the controller based on ESN. ESN Trajectory Zero - Order First - Order Second - Order Adaptive First - Order", "Table 18: MSE values for the controller based on MLP. MLP Trajectory Zero - Order First - Order Second - Order Adaptive First - Order", "The better performance of the ESN is something expected. MLP networks are of the feedforward type and do not consider past states in their formulation. The ESN is a feedback network, presenting a dynamic reservoir layer and recurrences from the outputs to the reservoir layer, efficiently analyzing time series. Therefore, the ESN considers the variables' va lues in different moments. While investigating a batch process, the variable time is a critical part to be considered, evidencing that the ESN is better to design this model than the MLP.", "Another point to be analyzed consists of the temperature variation. As can be observed, the controller based on MLP imposes higher temperature variations, reaching temperature values around 28 ¬∞C. For the controller based on ESN, these temperature variations are lower, reaching a minimum temperature around 30 ¬∞C , affectin g less the energy of the system . Therefore, the best approach imposes minor variations in control actions, ensuring that the reference trajectory is successfully achieved. One positive point about the controller based on MLP networks is how fast it reache s the reference trajectories. While analyzing the controllers‚Äô performance, it is evident that the controller based on MLP networks goes the reference trajectories faster than the ESN approach. This situation happens because the controller based on MLP networks imposes more abrupt temperature changes. Therefore, these temperature changes guarantee that the reference trajectories reach faster but less accurately. Both control strategies reached good concentration results at the end of almost all simulations, except for the second -order reference trajectory. For these cases, it is evident that the solute concentration and the solute concentration in equilibrium ratio are equal to one. This result is a positive point because this ratio equals one in dicates that the maximum quantity of solute is crystallized.", "Chapter 5", "Conclusion", "This project aims to apply neural networks models to the identification and control of a crystallization process. These models were developed using the co-teaching learning approach. Therefore, experimental data from the crystallization of K 2SO 4 and simulated data from the Population Balance Model were used to build the training and test datasets. A particular case of crystallization was studied but this methodology can be applied to other batch crystallization processes where the crystal particles are assumed to possess only one internal variable, their length.", "Four different neural network approaches were studied for the K 2SO 4 crystallization process: a single MLP network, four MLP networks in series, an LSTM network, and an ESN. These neural networks were analyzed to predict the moments‚Äô values one, two, and five steps forward. The prediction performance of the neural networks is worse for higher predict ion horizons. Therefore, the prediction horizon was limited to five because of the prediction degradation.", "All four designs could efficiently predict the moments‚Äô values one, and two steps ahead , achiev ing R-squared values higher than 0.9 for all experiments and simulations. This result evidences the potential of the neural networks, demonstrating that they can model the crystallization process only with data. However, the ESN was the only network that pres ented success for predictions five steps forward because it was the only model that achieved R -squared values higher than 0.75 for all cases. Even with the dynamic behavior of the LSTM network, this strategy was not efficient for predictions five steps forward. Moreover, the models based on MLP networks do not present a dynamic structure and did not present the same performance as the recurren t networks for this reason. The ESN was chosen to be the model of the predictive controller because of its best resu lts.", "Finally, the predictive controller based on ESN was developed, and its operation was compared to a controller based on a classic approach, which consists of an MLP model. However, the proposed strategy of one neural network to predict all moments‚Äô values was not efficient to the controller design. Therefore, a different approach had to be adopted, which consisted of using four different neural networks in the controller, one network to predict each ùúáùëñ value ùëõ steps ahead. Four MLP networks and four ESNs were trained to predict each ùúáùëñ one, two and five steps forward. Using four networks with single outputs was more efficient to control the crystallization process than the network applying multiple outputs.", "The operation of both predictive controllers‚Äô approaches was analyzed for a zero - order, a first -order, an adaptive first -order, and a second -order reference trajectory. Both methods could efficiently reach the reference trajectory for the zero -order, first -order, and adaptive first -order reference trajectories. The choice of the appropriate reference trajectory is crucial for a successful control approach.", "The small control horizon was enough to develop efficient model predictive controllers. However, there was offsets in both cases while analyzing the varia ble ùúá2ùúá0‚ÅÑ but this offset was higher for the controller based on MLP networks. Moreover , no offset is observed for the ratio ùúá3ùúá0‚ÅÑ for the controller based on ESN, while the controller based on MLP presents an offset for this variable . For the second -order reference trajectory, both structures presented operation problems , but the controller based on ESN showed a better behavior once again. This is explained by the controller based on ESN operation left ùúá0 closer to the reference traj ectory than the MLP approach.", "The ESN approach presented lower values of MSE for all reference trajectories considering the four controlled variables. The ESN presents a dynamic structure, considering past states, while the MLP is a feedforward network, n ot considering the input values at different times. These differences between the networks‚Äô systems explain why the ESN was more efficient in controlling the process. Therefore, the proposed predictive controller based on ESN showed to be an efficient strategy for controlling the crystallization process , being efficient to reach the desired crystals‚Äô shape and length, affecting less the system energy.", "As a general concl usion, this investigation showed that ANNs stand as an attractive tool to model the dynamics and to be used in model -based control of a batch cooling crystallization process. The use of a paradigm that uses recursive loops has improved the predictive behav ior. However, the prediction deteriorates when the horizon is increased. So, only short horizons could be used. This did not preclude the use of the ANN in a well -succeeded NMPC strategy.", "5.1. Future Work", "The recurrent networks need a vast dataset to present an efficient predictive ability. Therefore, more experiments in different initial conditions and with a longer duration should be done to improve the training and test datasets. Then, the performances of the recurrent approaches, such as LSTM and E SN, should be studied for larger predictions horizons. The enhanced strategies should be tested for the predictive controller to see if better results are achieved.", "The present study chose the neural networks‚Äô hyperparameters by trial-and-error. Therefore, these networks can be developed using an optimization algorithm to find the best hyperparameters. The same can be said regarding the optimization of the tuning parameters of the NMPC.", "Another option that should be tested is training other neural networks ‚Äô approaches, such as Gated Recurrent Unit (GRU). Therefore, the performance of the new approaches should be tested to predict the moments‚Äô values ùëõ steps forward and their efficiency as the model of the predictive controller. Also, new controllers‚Äô desig ns can be tried to enhance its operation.", "In this study, only one manipulated variable was analyzed to control the process, which is the temperature. Thus, the influence of other variables in the crystallization process should be tested. This change can develop neural network models with better results and achieve better control results since other variables can be manipulated to reach ZHANG, F.; DU, K.; GUO, L.; HUO, Y.; HE , K.; SHAN, B. Progress, problems, and potential of technology for measuring solution concentration in crystallization processes . Measurement , v. 187, p. 1 -14, 2022a.", "ZHANG, Z.; BABAYOMI, O.; DRAGICEVIC, T.; HEYDARI, R.; GARCIA, C.; RODRIGUEZ, J.; KENNEL, R. Advances and opportunities in the model predictive control of microgrids: Part I ‚Äìprimary layer. International Journal of Electrical Power and Energy Systems , v. 134, p. 1 -12, 2022 b. Appendix Appendix A ‚Äì Test and training datasets.", "Figure A.1: Moments, concentration and temperature values for the training dataset. Dotted points indicate the experimental values, and solid line represents the simulated values.", "Figure A.2: Moments, concentration and temperature values for Experiment 1 for the test dataset .", "Figure A.3: Moments, concentration and temperature values for Experiment 2 for the test dataset.", "Figure A.4: Moments, concentration and temperature values for Experiment 3 for the test dataset.", "Figure A.5: Moments, concentration and temperature values for Experiment 4 for the test dataset.", "Figure A.6: Moments, concentration and temperature values for Experiment 5 for the test dataset.", "Figure A.7: Moments, concentration and temperature values for Experiment 6 for the test dataset.", "Figure A.8: Moments, concentration and temperature values for Experiment 7 for the test dataset.", "Figure A.9: Moments, concentration and temperature values for Experiment 8 for the test dataset.", "Figure A.10 : Moments, concentration and temperature values for Experiment 9 for the test dataset.", "Figure A.11 : Moments, concentration and temperature values for Simulation 1 for the test dataset.", "Figure A.12 : Moments, concentration and temperature values for Simulation 2 for the test dataset.", "Figure A.13 : Moments, concentration and temperature values for Simulation 3 for the test dataset.", "Appendix B ‚Äì Paper published for 32nd European Symposium on Computer Aided Process Engineering (ESCAPE32), June 12 -15, 2022, Toulouse, France.", "Appendix C ‚Äì Manuscript of an MPC to control the solute concentration in a batch crystallization process submitted to the Process Systems Engineering Brazil 2022 Congress.", "A RECURRENT NEURAL NETWORKS -BASED APPROACH FOR CONTROLLING THE SOLUTE CONCENTRATION OF A CRYSTALLIZATION PROCESS Fernando Arrais R. D. Lima1,*, Marcellus G. F. de Moraes2, Maur√≠cio B. de Souza Jr.1,2 1 ‚Äì Escola de Qu√≠mica, Universidade Federal do Rio de Janeiro ‚Äì UFRJ ‚Äì RJ, farrais@eq.ufrj. br 2 ‚Äì PEQ/COPPE, Universidade Federal do Rio de Janeiro ‚Äì UFRJ ‚Äì RJ", "Abstract: This work aims to model a crystallization process to predict the solute concentration with neural networks used as the internal model in the predictive controller. Three different neural networks paradigms were considered: a classic single Multilayer Perce ptron (MLP) network, the Echo State Network (ESN), and the Long Short -Term Memory (LSTM). First, the three network structures were trained to predict the solute concentration one step ahead, using the current temperature and concentration values as feed. T hen, the network‚Äôs predictive performance was studied for larger prediction horizons. Finally, a Nonlinear Model Predictive Controller (NMPC) based on the most efficient neural network‚Äôs design was successfully applied to the batch crystallization process to maintain the solute concentration on its desired trajectories by manipulating the operating temperature. The performance of the proposed NMPC was compared to a controller based on MLP networks.", "Keywords: Crystallization, Neural Networks, Echo State Net work, LSTM, NMPC .", "1. Introduction", "Crystallization is a unit operation widely used in many industries, such as pharmaceutical, fine chemicals, and food, for product separation and purification [1]. One strategy broadly applied for this process consists of cooling crystallization technology. Also, it is essential to recover the maximum quantity of solute as possible from the solution. Therefore, the cooling temperature profile is a vital part of the operation of the cooling crystallization strategy to reach its goal, making process control an essential part of this process [2].", "Different strategies were designed to guarantee the process control, such as a PID controller. In particular, Model Predictive Control (MPC) is a viable option if the model‚Äôs variables can be measurable and monitoring, being an interesting choice to contro l the crystallization process [3]. The MPC strategy applies a model to predict the output variables in a prediction horizon and then search for the condition of the manipulated variables that guarantees the desired controlled variables values. Thus, modeli ng the process is an important step to use this strategy.", "Population Balance (PB) models are usually applied for crystallization processes [3]. However, machine learning algorithms, particularly neural networks, have been widely used to model and control p rocesses in chemical engineering. Wu et al. [4] used a recurrent neural network (RNN), specifically the Long-Short Term Memory (LSTM), to model and control a reactor that produces ethylbenzene from ethylene and benzene. Shin et al. [5] trained a neural net work to model a depropanizer and incorporated the developed model in an MPC strategy. Neural networks were applied to model and control many chemical processes, but little was explored for the crystallization process. First, this study aims to use neural n etworks to predict the solute concentration in the future. Therefore, three different neural networks‚Äô designs were analyzed: a single Multilayer Perceptron (MLP), an LSTM, and an Echo State Network (ESN). The prediction behavior of each model was studied for predictions one, two, and five steps forward (i.e., one , two and five sampling time s ahead ). Then, a model predictive controller based on the neural network structure that presents the best behavior was developed. The controller operation was analyzed for four reference trajectories: a zero - order, a first -order, a second -order, and an adaptive first -order. Also, the proposed controller‚Äôs operation was compared to a controller based on a classic approach, using an MLP network.", "2. Experimental", "2.1. Training and Test Datasets", "According to the co -teaching learning algorithm, the dataset consisted of simulated and ten experimental batches of the potassium sulfate crystallization. The training dataset was composed of experiments from one batch and 6000 simulated data based on the chosen experiment. The simulated data was generated by applying the Population Balance Model developed by Moraes [3].", "The test dataset was also composed of simulated and experimental data, but a different approach from the trai ning dataset was used. First, three other simulations were realized for the test samples. Each of the simulation datasets is composed of results for 1000 different temperature values and depart from different initial conditions according to the experiments . Then, the results of each three -simulation run and the remaining nine batches were analyzed separately because the data must be ordered in time for the recurrent networks.", "2.2. Model Predictive Controller Design", "The controlled variable chosen for the optimiz ation problem was the solute concentration and the manipulated variable is the process temperature. The MPC goal is to minimize the process performance index J, the objective function of the problem shown in Eq. 1. The parameters Œ¥(i) and Œ≥ are the weights of the outputs and the increment input, respectively; y_i (k+j) is the ith predicted output at time k + j and „Äñy_i„Äó^r (k+j) its reference value. The parameter Œ¥(i) was defined as 1 and the parameter Œ≥ was defined as 5 for the ESN approach. For the control ler using MLP, Œ¥(i) was defined as 1 and the parameter Œ≥ was defined as 10. The input increment is defined by ‚àÜu(k) as a control horizon of 1 was assumed. P is the prediction horizon, which was chosen as 5 sampling intervals.", "The optimization constraints are given by Eqs . 2-4, where C is the concentration of the solute, and ùê∂ùëíùëû is the concentration of the solute in equilibrium. The constraint defined by Eq. 2 was defined in order to avoid sudden changes in temperature . Also, the constraints indicated by Eqs. 3 and 4 were necessary because the Population Balance model implemented only considers crystal nucleation and crystal growth .", "The control loop scheme is illustrated by Fig. 1. First, the process modeled by the Population Bala nce calculates the ùúá0, ùúá1, ùúá2, ùúá3, and concentration values. The concentration result is used as the input of the neural network model, and the network predict the concentration five steps forward. The neural network outputs are compared to the desired reference trajectory. This result is sent to an optimizer applying the Successive Quadratic Programming algorithm in order to achieve the optimum temperature value. This loop was simulated for a 240 minutes crystallization process, using the language Python. The optimization problem was solved using the function minimize from the SciPy library [6].", "Figure 1: Control loop scheme.", "3. Results and Discussion", "3.1. Neural Networks Models", "First, an MLP network was developed to predict the solute concentration one step ahead using the actual temperature and solute concentration as inputs. Then, its operation was studied for predictions two and five steps forward. This network was composed of a single hidden layer with 50 neurons. Furthermore, it wa s applied the activation function relu, a batch size of 20, the solver Adam, and an initial learning rate of 0.1. Therefore, the MLP could efficiently predict the solute concentration one and two steps ahead, achieving R-squared values higher than 0.9 for all simulations and experiments. However, this result differed for predictions five steps forward because two experiments had R -squared values The second model tested consists of an LSTM network, using the same inputs as the MLP network for pred ictions one step ahead. Then, its prediction performance was studied for two and five steps forward. However, it differed from the other case because for predictions ùëõ steps ahead, it was necessary inputs ùëõ steps back. This network was composed of one LS TM layer with 50 cells. Moreover, it was used hyperbolic tangent as activation function, sigmoid as recurrent activation function, a batch size of 50, and the solver Adam. This model was efficient too for predictions one and two steps ahead. This strategy presented some problems for predictions five steps ahead because it reached R - squared values below 0.5 for two experiments.", "The last model was analyzed based on an ESN, using the same inputs and outputs as the MLP network for predictions one, two, and five steps forward. This model used a single reservoir layer with 50 neurons and identity activation function for the output layer and the output recurrence. Also, this network adds a noise of 0.5 to each neuron. This neural network could efficiently predict t he solute for all cases, achieving R -squared values higher than 0.78 for all cases.", "The R -squared values for predictions five steps ahead for the three networks are presented in Fig. 2. Therefore, the ESN showed a better performance because achieved higher R-squared values as mentioned before.", "Figure 2: Performance of all neural networks for predictions five steps ahead.", "3.2. Model Predictive Controller", "A control loop was developed using the ESN as the internal model of the controller because of its efficiency. Its operation was compared to control applying an MLP network as the internal model. First, the controllers‚Äô operation was studied for a zero -order reference defined by Eq . 5, which ùê∂(ùë°ùëíùëõùëë) is the solute concentration at the end of the exper iment and ùê∂ùëÖùëá(ùë°) is the value of the reference trajectory in time ùë°. Therefore, this is a constant trajectory defined using the experimental value at the end of the batch .", "The controller was tested for a first -order reference trajectory defined in an iterative way for ùê∂ by Eqs . 6 and 7. The parameter ùõæ is constant defined as 0.9 and indicates how fast the final value will be reached.", "The second -order trajectory was modeled by a critically damped second -order system and is represented by Eq . 8. The parameter ùúèùëñ is the time constant and was defined as 18.72 using the least squares of the curve_fit function from SciPy library [6].", "The last reference trajectory studied consists of an adaptive first -order reference trajectory. Different from the other three trajectories, this one is recalculated at each time step. This trajectory is similar to the first -order defined by Eqs . 6 and 7, but departs from the actual system st ate at each time step. The adaptive first -order trajectory is defined by Eqs. 9 and 10. The superscript m denotes the measured value of the variable, and ùõæ is constant defined as 0. 15.", "The results for the zero -order reference trajectory for ESN are shown in F ig. 3, and for MLP are illustrated in Fig . 4. Both structures could efficiently achieve the reference trajectory when looking at the images, but ESN showed a better performance than MLP. Th erefore, the offset observed in the controller based on the MLP network is higher than the ESN one.", "Figure 3: Zero -order reference trajectory results for ESN model.", "Figure 4: Zero -order reference trajectory results for MLP model.", "The controllers‚Äô results for a second -order reference trajectory are shown in Fig . 5 for ESN and in Fig . 6 for MLP. The controllers showed an efficient performance once again, but the controller based on ESN didn‚Äôt present an offset.", "Figure 5: Second -order reference trajectory results for ESN model.", "Figure 6: Second -order reference trajectory results for MLP model.", "The mean squared error (MSE) was calculated for both control strategies and all four reference trajectories. These results are presented in T able 1, demonstrating that the controller based on ESN was more efficient than the one base on MLP. The MSE values were lower for all four trajectories for the ESN approach.", "Table 1: MSE values for the two networks and all four reference trajectories. Neural Network Zero -Order First -Order Second -Order Adaptive First - Order", "4. Conclusion", "In this work, an MPC controller was developed using a neural network approach to predict the solute concentration in a crystallization process . Three different neural networks designs were studied : a single MLP, LSTM and ESN. The dataset used for training and testing applied a co -teaching learning algorithm, using experimental and simulated data. As a result, the ESN presented the best performance to predict the solute concentration , achieving values of R -squared higher than 7 8% for predictions five steps ahead.", "The ESN was chosen to be the model of the controller, and its behavior was compared to a traditional approach, using MLP as the model. The controller based on the ESN produced better results than the one using the MLP, presenting a lower value of MSE . Four different reference trajectories were analyzed: a constant, a 1st order, a 2nd order, and an adaptive 1st order. Therefore, both structures were efficient for a ll cases analyzed.", "Finally, it is suggested to try different approaches of neural network s to model this process, such as Gated Recurrent Unit. Also, this same application could be tested on a continuous crystallization process.", "Acknowledgment This study was financed in part by the Coordena√ß√£o de Aperfei√ßoamento de Pessoal de N√≠vel Superior ‚Äì Brasil (CAPES).", "References", "1. F. Zhang ; K. Du; L. Guo; Y. Huo; K. He; B. Shan. Progress, problems, and potential of technology for measuring solution concentration in crystallization processes. 2. J. Liu.; T. Liu.; J. Chen; H. Yue.; F. Zhang .; F. Sun. Data -driven modeling of product crystal size distribution and optimal input design for batch cooling crystallization processes. Journal of Process Control . 2020, v. 96, p. 1 -14.", "3. M. G. F. Moraes . Modelagem, monitora mento e controle do tamanho e forma de cristais em processos de cristaliza√ß√£o . Doctoral Qualification Exam, 2019. 4. Z. Wu; J. Luo; D. Rincon ; P. D. Christofides . Machine learning -based predictive control using noisy data: evaluating performance and robustness via a large -scale process simulator. Chemical Engineering Research and Design . 2021, v. 168, p. 275 -287. 5. Y. Shin; R. Smith; S. Hwang . Development of model predictive control system using an artificial neural network: A case study with a distilla tion column. Journal of Cleaner Production . 2020, v. 277, p. 1 -14.", "6. P. Virtanen ; R. Gommers ; T. E. Oliphant ; M. Haberland; et al. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods . 2020, v."]}